<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>harvester_e2e_tests.integration.test_vm_functions API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.integration.test_vm_functions</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import re
from time import sleep
from pathlib import Path
from tempfile import NamedTemporaryFile
from datetime import datetime, timedelta

import yaml
import pytest
from paramiko.ssh_exception import ChannelException

pytest_plugins = [
    &#34;harvester_e2e_tests.fixtures.api_client&#34;,
    &#34;harvester_e2e_tests.fixtures.images&#34;,
    &#34;harvester_e2e_tests.fixtures.virtualmachines&#34;
]


@pytest.fixture(scope=&#34;session&#34;)
def virtctl(api_client):
    code, ctx = api_client.vms.download_virtctl()

    with NamedTemporaryFile(&#34;wb&#34;) as f:
        f.write(ctx)
        f.seek(0)
        yield Path(f.name)


@pytest.fixture(scope=&#34;session&#34;)
def kubeconfig_file(api_client):
    kubeconfig = api_client.generate_kubeconfig()
    with NamedTemporaryFile(&#34;w&#34;) as f:
        f.write(kubeconfig)
        f.seek(0)
        yield Path(f.name)


@pytest.fixture(scope=&#34;module&#34;)
def image(api_client, image_opensuse, unique_name, wait_timeout):
    unique_image_id = f&#39;image-{unique_name}&#39;
    code, data = api_client.images.create_by_url(
        unique_image_id, image_opensuse.url, display_name=f&#34;{unique_name}-{image_opensuse.name}&#34;
    )

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(unique_image_id)
        if 100 == data.get(&#39;status&#39;, {}).get(&#39;progress&#39;, 0):
            break
        sleep(3)
    else:
        raise AssertionError(
            &#34;Failed to create Image with error:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    yield dict(id=f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{unique_image_id}&#34;,
               user=image_opensuse.ssh_user)

    code, data = api_client.images.delete(unique_image_id)


@pytest.fixture(scope=&#34;module&#34;)
def unique_vm_name(unique_name):
    return f&#34;vm-{unique_name}&#34;


@pytest.fixture(scope=&#34;class&#34;)
def stopped_vm(api_client, ssh_keypair, wait_timeout, image, unique_vm_name):
    unique_vm_name = f&#34;stopped-{datetime.now().strftime(&#39;%m%S%f&#39;)}-{unique_vm_name}&#34;
    cpu, mem = 1, 2
    pub_key, pri_key = ssh_keypair
    vm_spec = api_client.vms.Spec(cpu, mem)
    vm_spec.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    vm_spec.run_strategy = &#34;Halted&#34;

    userdata = yaml.safe_load(vm_spec.user_data)
    userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
    vm_spec.user_data = yaml.dump(userdata)

    code, data = api_client.vms.create(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;):
            break
        sleep(1)

    yield unique_vm_name, image[&#39;user&#39;]

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)

    api_client.vms.delete(unique_vm_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)

    for vol in vm_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)


@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(name=&#34;minimal_vm&#34;)
def test_minimal_vm(api_client, image, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and other default values
        2. Save
    Exepected Result:
        - VM should created
        - VM should Started
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    vm = api_client.vms.Spec(cpu, mem)
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])

    code, data = api_client.vms.create(unique_vm_name, vm)

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to create Minimal VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )


@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(depends=[&#34;minimal_vm&#34;])
class TestVMOperations:
    &#34;&#34;&#34;
    To cover tests:
    - https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/ # noqa
    &#34;&#34;&#34;

    @pytest.mark.dependency(name=&#34;pause_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_pause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Pause the VM was created
        Exepected Result:
            - VM should change status into `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.pause(unique_vm_name)
        assert 204 == code, &#34;`Pause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if [c for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []) if &#34;Paused&#34; == c[&#39;type&#39;]]:
                conditions = data[&#39;status&#39;][&#39;conditions&#39;]
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to pause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert &#34;Paused&#34; == conditions[-1].get(&#39;type&#39;), conditions
        assert &#34;PausedByUser&#34; == conditions[-1].get(&#39;reason&#39;), conditions

    @pytest.mark.dependency(depends=[&#34;pause_vm&#34;])
    def test_unpause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Unpause the VM was paused
        Exepected Result:
            - VM&#39;s status should not be `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.unpause(unique_vm_name)
        assert 204 == code, &#34;`Unpause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            cond_types = set(c[&#39;type&#39;] for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []))
            if {&#34;AgentConnected&#34;} &amp; cond_types and not {&#34;Paused&#34;} &amp; cond_types:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to unpause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    @pytest.mark.dependency(name=&#34;stop_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_stop(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Stop the VM was created and not stopped
        Exepected Result:
            - VM&#39;s status should be changed to `Stopped`
            - VM&#39;s `RunStrategy` should be changed to `Halted`
        &#39;&#39;&#39;
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        code, data = api_client.vms.get(unique_vm_name)
        assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
        assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]

    @pytest.mark.dependency(name=&#34;start_vm&#34;, depends=[&#34;stop_vm&#34;])
    def test_start(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Start the VM was created and stopped
        Exepected Result:
            - VM should change status into `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.start(unique_vm_name)
        assert 204 == code, &#34;`Start return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
            pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
            if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_restart(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Restart the VM was created
        Exepected Result:
            - VM&#39;s ActivePods should be updated (which means the VM restarted)
            - VM&#39;s status should update to `Running`
            - VM&#39;s qemu-agent should be connected
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

        old_pods = set(data[&#39;status&#39;][&#39;activePods&#39;].items())

        code, data = api_client.vms.restart(unique_vm_name)
        assert 204 == code, &#34;`Restart return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if old_pods.difference(data[&#39;status&#39;].get(&#39;activePods&#39;, old_pods).items()):
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}), activePods is not updated.\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_softreboot(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Softreboot the VM was created
        Exepected Result:
            - VM&#39;s qemu-agent should disconnected (which means the VM rebooting)
            - VM&#39;s qemu-agent should re-connected (which means the VM boot into OS)
            - VM&#39;s status should be changed to `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
        old_agent = data[&#39;status&#39;][&#39;conditions&#39;][-1]
        assert &#34;AgentConnected&#34; == old_agent[&#39;type&#39;], (code, data)

        api_client.vms.softreboot(unique_vm_name)
        # Wait until agent disconnected (leaving OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if &#34;AgentConnected&#34; not in data[&#39;status&#39;][&#39;conditions&#39;][-1][&#39;type&#39;]:
                break
            sleep(5)
        # then wait agent connected again (Entering OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase, conds = data[&#39;status&#39;][&#39;phase&#39;], data[&#39;status&#39;].get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Softreboot VM({unique_vm_name}) with errors:\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        old_t = datetime.strptime(old_agent[&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)
        new_t = datetime.strptime(conds[-1][&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)

        assert new_t &gt; old_t, (
            &#34;Agent&#39;s probe time is not updated.\t&#34;
            f&#34;Before softreboot: {old_t}, After softreboot: {new_t}\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
        )

    def test_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/ # noqa

        Steps:
            1. migrate the VM was created
        Exepected Result:
            - VM&#39;s host Node should be changed to another one
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;] if cur_host != h[&#39;id&#39;])

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            migrating = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if not migrating and new_host == data[&#39;status&#39;][&#39;nodeName&#39;]:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Migrate VM({unique_vm_name}) from {cur_host} to {new_host}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_abort_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/abort-live-migration/

        Steps:
            1. Abort the VM was created and migrating
        Exepected Result:
            - VM should able to perform migrate
            - VM should stay in current host when migrating be aborted.
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;] if cur_host != h[&#39;id&#39;])

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        states = [&#34;Aborting migration&#34;, &#34;Migrating&#34;]
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            m_state = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if m_state == states[-1]:
                states.pop()
                if states:
                    code, err = api_client.vms.abort_migrate(unique_vm_name)
                    assert 204 == code, (code, err)
                else:
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to abort VM({unique_vm_name})&#39;s migration, stuck on {states[-1]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert cur_host == data[&#39;status&#39;][&#39;nodeName&#39;], (
            f&#34;Failed to abort VM({unique_vm_name})&#39;s migration,&#34;
            f&#34;VM been moved to {data[&#39;status&#39;][&#39;nodeName&#39;]} is not the origin host {cur_host}\n&#34;
        )

    def test_delete(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Delete the VM was created
            2. Delete Volumes was belonged to the VM
        Exepected Result:
            - VM should able to be deleted and success
            - Volumes should able to be deleted and success
        &#39;&#39;&#39;

        code, data = api_client.vms.delete(unique_vm_name)
        assert 200 == code, (code, data)

        spec = api_client.vms.Spec.from_dict(data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        fails, check = [], dict()
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            check[vol_name] = api_client.volumes.delete(vol_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            l_check = dict()
            for vol_name, (code, data) in check.items():
                if 200 != code:
                    fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
                else:
                    code, data = api_client.volumes.get(vol_name)
                    if 404 != code:
                        l_check[vol_name] = (code, data)
            check = l_check
            if not check:
                break
            sleep(5)
        else:
            for vol_name, (code, data) in check.items():
                fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

        assert not fails, (
            f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
            &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
        )


@pytest.mark.p0
@pytest.mark.virtualmachines
def test_create_stopped_vm(api_client, stopped_vm, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and runStrategy is `Halted`
        2. Save
    Exepected Result:
        - VM should created
        - VM should Stooped
        - VMI should not exist
    &#34;&#34;&#34;
    unique_vm_name, _ = stopped_vm
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Create a Stopped VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    code, data = api_client.vms.get_status(unique_vm_name)
    assert 404 == code, (code, data)


@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMClone:
    def test_clone_running_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Clone the VM into VM-cloned
            4. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and starting
            - Cloned-VM should becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo {unique_vm_name!r} &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check VM started
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert unique_vm_name in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)

    def test_clone_stopped_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Stop the VM
            4. Clone the VM into VM-cloned
            5. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and stopped
            - Cloned-VM should able to start and becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo &#34;stopped-{unique_vm_name}&#34; &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Stop the VM
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check cloned VM is available and stooped
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if (200 == code
               and &#34;Halted&#34; == data[&#39;spec&#39;].get(&#39;runStrategy&#39;)
               and &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Cloned VM {cloned_name} is not available and stopped&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Check cloned VM started
        api_client.vms.start(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert f&#34;stopped-{unique_vm_name}&#34; in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)


@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMWithVolumes:
    def test_create_with_two_volumes(self, api_client, ssh_keypair, wait_timeout,
                                     host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/ # noqa

        Steps:
            1. Create a VM with 1 CPU 2 Memory and 2 disk volumes
            2. Start the VM
            3. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - 2 disk volumes should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        volumes = [(&#39;disk-1&#39;, 1), (&#39;disk-2&#39;, 2)]
        for name, size in volumes:
            vm_spec.add_volume(name, size)

        # Start VM with 2 additional volumes
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        assert 200 == code, (code, data)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )
        fails = []
        for _, size in volumes:
            if not re.search(f&#34;vd.*{size}G 0 disk&#34;, out):
                fails.append(f&#34;Volume size {size}G not found&#34;)

        assert not fails, (
            f&#34;lsblk output: {out}\n&#34;
            &#34;\n&#34;.join(fails)
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vol_names, vols, claims = [n for n, s in volumes], [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] in vol_names:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for vol_name in claims:
            api_client.volumes.delete(vol_name)

    def test_create_with_existing_volume(self, api_client, ssh_keypair, wait_timeout,
                                         host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/ # noqa

        Steps:
            1. Create a data volume
            2. Create a VM with 1 CPU 2 Memory and the existing data volume
            3. Start the VM
            4. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - Disk volume should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        vol_name, size = &#39;disk-existing&#39;, 3
        vol_spec = api_client.volumes.Spec(size)
        code, data = api_client.volumes.create(f&#34;{unique_vm_name}-{vol_name}&#34;, vol_spec)

        assert 201 == code, (code, data)

        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        vm_spec.add_existing_volume(vol_name, f&#34;{unique_vm_name}-{vol_name}&#34;)

        # Start VM with added existing volume
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        assert f&#34;{size}G 0 disk&#34; in out, (
            f&#34;existing Volume {size}G not found\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vols, claims = [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] == vol_name:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for claim in claims:
            api_client.volumes.delete(claim)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.image"><code class="name flex">
<span>def <span class="ident">image</span></span>(<span>api_client, image_opensuse, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def image(api_client, image_opensuse, unique_name, wait_timeout):
    unique_image_id = f&#39;image-{unique_name}&#39;
    code, data = api_client.images.create_by_url(
        unique_image_id, image_opensuse.url, display_name=f&#34;{unique_name}-{image_opensuse.name}&#34;
    )

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(unique_image_id)
        if 100 == data.get(&#39;status&#39;, {}).get(&#39;progress&#39;, 0):
            break
        sleep(3)
    else:
        raise AssertionError(
            &#34;Failed to create Image with error:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    yield dict(id=f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{unique_image_id}&#34;,
               user=image_opensuse.ssh_user)

    code, data = api_client.images.delete(unique_image_id)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.kubeconfig_file"><code class="name flex">
<span>def <span class="ident">kubeconfig_file</span></span>(<span>api_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;session&#34;)
def kubeconfig_file(api_client):
    kubeconfig = api_client.generate_kubeconfig()
    with NamedTemporaryFile(&#34;w&#34;) as f:
        f.write(kubeconfig)
        f.seek(0)
        yield Path(f.name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.stopped_vm"><code class="name flex">
<span>def <span class="ident">stopped_vm</span></span>(<span>api_client, ssh_keypair, wait_timeout, image, unique_vm_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def stopped_vm(api_client, ssh_keypair, wait_timeout, image, unique_vm_name):
    unique_vm_name = f&#34;stopped-{datetime.now().strftime(&#39;%m%S%f&#39;)}-{unique_vm_name}&#34;
    cpu, mem = 1, 2
    pub_key, pri_key = ssh_keypair
    vm_spec = api_client.vms.Spec(cpu, mem)
    vm_spec.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    vm_spec.run_strategy = &#34;Halted&#34;

    userdata = yaml.safe_load(vm_spec.user_data)
    userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
    vm_spec.user_data = yaml.dump(userdata)

    code, data = api_client.vms.create(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;):
            break
        sleep(1)

    yield unique_vm_name, image[&#39;user&#39;]

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)

    api_client.vms.delete(unique_vm_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)

    for vol in vm_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_create_stopped_vm"><code class="name flex">
<span>def <span class="ident">test_create_stopped_vm</span></span>(<span>api_client, stopped_vm, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/">https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory and runStrategy is <code>Halted</code></li>
<li>Save
Exepected Result:<ul>
<li>VM should created</li>
<li>VM should Stooped</li>
<li>VMI should not exist</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
def test_create_stopped_vm(api_client, stopped_vm, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and runStrategy is `Halted`
        2. Save
    Exepected Result:
        - VM should created
        - VM should Stooped
        - VMI should not exist
    &#34;&#34;&#34;
    unique_vm_name, _ = stopped_vm
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Create a Stopped VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    code, data = api_client.vms.get_status(unique_vm_name)
    assert 404 == code, (code, data)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_minimal_vm"><code class="name flex">
<span>def <span class="ident">test_minimal_vm</span></span>(<span>api_client, image, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/">https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory and other default values</li>
<li>Save
Exepected Result:<ul>
<li>VM should created</li>
<li>VM should Started</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(name=&#34;minimal_vm&#34;)
def test_minimal_vm(api_client, image, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and other default values
        2. Save
    Exepected Result:
        - VM should created
        - VM should Started
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    vm = api_client.vms.Spec(cpu, mem)
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])

    code, data = api_client.vms.create(unique_vm_name, vm)

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to create Minimal VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.unique_vm_name"><code class="name flex">
<span>def <span class="ident">unique_vm_name</span></span>(<span>unique_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def unique_vm_name(unique_name):
    return f&#34;vm-{unique_name}&#34;</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.virtctl"><code class="name flex">
<span>def <span class="ident">virtctl</span></span>(<span>api_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;session&#34;)
def virtctl(api_client):
    code, ctx = api_client.vms.download_virtctl()

    with NamedTemporaryFile(&#34;wb&#34;) as f:
        f.write(ctx)
        f.seek(0)
        yield Path(f.name)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone"><code class="flex name class">
<span>class <span class="ident">TestVMClone</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMClone:
    def test_clone_running_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Clone the VM into VM-cloned
            4. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and starting
            - Cloned-VM should becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo {unique_vm_name!r} &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check VM started
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert unique_vm_name in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)

    def test_clone_stopped_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Stop the VM
            4. Clone the VM into VM-cloned
            5. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and stopped
            - Cloned-VM should able to start and becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo &#34;stopped-{unique_vm_name}&#34; &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Stop the VM
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check cloned VM is available and stooped
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if (200 == code
               and &#34;Halted&#34; == data[&#39;spec&#39;].get(&#39;runStrategy&#39;)
               and &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Cloned VM {cloned_name} is not available and stopped&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Check cloned VM started
        api_client.vms.start(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert f&#34;stopped-{unique_vm_name}&#34; in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_running_vm"><code class="name flex">
<span>def <span class="ident">test_clone_running_vm</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- (legacy) <a href="https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/">https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</a> # noqa
- (new) <a href="https://github.com/harvester/tests/issues/361">https://github.com/harvester/tests/issues/361</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory</li>
<li>Start the VM and write some data</li>
<li>Clone the VM into VM-cloned</li>
<li>Verify VM-Cloned</li>
</ol>
<p>Exepected Result:
- Cloned-VM should be available and starting
- Cloned-VM should becomes <code>Running</code>
- Written data should available in Cloned-VM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_clone_running_vm(self, api_client, ssh_keypair, wait_timeout,
                          host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/ # noqa
    - (new) https://github.com/harvester/tests/issues/361

    Steps:
        1. Create a VM with 1 CPU 2 Memory
        2. Start the VM and write some data
        3. Clone the VM into VM-cloned
        4. Verify VM-Cloned

    Exepected Result:
        - Cloned-VM should be available and starting
        - Cloned-VM should becomes `Running`
        - Written data should available in Cloned-VM
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair
    code, data = api_client.vms.start(unique_vm_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into VM to make some data
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(f&#39;echo {unique_vm_name!r} &gt; ~/vmname&#39;)
            assert not err, (out, err)
            sh.exec_command(&#39;sync&#39;)

    # Clone VM into new VM
    cloned_name = f&#34;cloned-{unique_vm_name}&#34;
    code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
    assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

    # Check VM started
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(cloned_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into new VM to check VM is cloned as old one
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )

            out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
        assert unique_vm_name in out, (
            f&#34;cloud-init writefile failed\n&#34;
            f&#34;Executed stdout: {out}\n&#34;
            f&#34;Executed stderr: {err}&#34;
        )

    # Remove cloned VM and volumes
    code, data = api_client.vms.get(cloned_name)
    cloned_spec = api_client.vms.Spec.from_dict(data)
    api_client.vms.delete(cloned_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(cloned_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    for vol in cloned_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_stopped_vm"><code class="name flex">
<span>def <span class="ident">test_clone_stopped_vm</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- (legacy) <a href="https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/">https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</a> # noqa
- (new) <a href="https://github.com/harvester/tests/issues/361">https://github.com/harvester/tests/issues/361</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory</li>
<li>Start the VM and write some data</li>
<li>Stop the VM</li>
<li>Clone the VM into VM-cloned</li>
<li>Verify VM-Cloned</li>
</ol>
<p>Exepected Result:
- Cloned-VM should be available and stopped
- Cloned-VM should able to start and becomes <code>Running</code>
- Written data should available in Cloned-VM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_clone_stopped_vm(self, api_client, ssh_keypair, wait_timeout,
                          host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/ # noqa
    - (new) https://github.com/harvester/tests/issues/361

    Steps:
        1. Create a VM with 1 CPU 2 Memory
        2. Start the VM and write some data
        3. Stop the VM
        4. Clone the VM into VM-cloned
        5. Verify VM-Cloned

    Exepected Result:
        - Cloned-VM should be available and stopped
        - Cloned-VM should able to start and becomes `Running`
        - Written data should available in Cloned-VM
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair
    code, data = api_client.vms.start(unique_vm_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into VM to make some data
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(f&#39;echo &#34;stopped-{unique_vm_name}&#34; &gt; ~/vmname&#39;)
            assert not err, (out, err)
            sh.exec_command(&#39;sync&#39;)

    # Stop the VM
    code, data = api_client.vms.stop(unique_vm_name)
    assert 204 == code, &#34;`Stop` return unexpected status code&#34;
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    # Clone VM into new VM
    cloned_name = f&#34;cloned-{unique_vm_name}&#34;
    code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
    assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

    # Check cloned VM is available and stooped
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(cloned_name)
        if (200 == code
           and &#34;Halted&#34; == data[&#39;spec&#39;].get(&#39;runStrategy&#39;)
           and &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Cloned VM {cloned_name} is not available and stopped&#34;
            f&#34;Status({code}): {data}&#34;
        )

    # Check cloned VM started
    api_client.vms.start(cloned_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(cloned_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into new VM to check VM is cloned as old one
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )

            out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
        assert f&#34;stopped-{unique_vm_name}&#34; in out, (
            f&#34;cloud-init writefile failed\n&#34;
            f&#34;Executed stdout: {out}\n&#34;
            f&#34;Executed stderr: {err}&#34;
        )

    # Remove cloned VM and volumes
    code, data = api_client.vms.get(cloned_name)
    cloned_spec = api_client.vms.Spec.from_dict(data)
    api_client.vms.delete(cloned_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(cloned_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    for vol in cloned_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations"><code class="flex name class">
<span>class <span class="ident">TestVMOperations</span></span>
</code></dt>
<dd>
<div class="desc"><p>To cover tests:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/">https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</a> # noqa</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(depends=[&#34;minimal_vm&#34;])
class TestVMOperations:
    &#34;&#34;&#34;
    To cover tests:
    - https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/ # noqa
    &#34;&#34;&#34;

    @pytest.mark.dependency(name=&#34;pause_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_pause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Pause the VM was created
        Exepected Result:
            - VM should change status into `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.pause(unique_vm_name)
        assert 204 == code, &#34;`Pause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if [c for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []) if &#34;Paused&#34; == c[&#39;type&#39;]]:
                conditions = data[&#39;status&#39;][&#39;conditions&#39;]
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to pause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert &#34;Paused&#34; == conditions[-1].get(&#39;type&#39;), conditions
        assert &#34;PausedByUser&#34; == conditions[-1].get(&#39;reason&#39;), conditions

    @pytest.mark.dependency(depends=[&#34;pause_vm&#34;])
    def test_unpause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Unpause the VM was paused
        Exepected Result:
            - VM&#39;s status should not be `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.unpause(unique_vm_name)
        assert 204 == code, &#34;`Unpause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            cond_types = set(c[&#39;type&#39;] for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []))
            if {&#34;AgentConnected&#34;} &amp; cond_types and not {&#34;Paused&#34;} &amp; cond_types:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to unpause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    @pytest.mark.dependency(name=&#34;stop_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_stop(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Stop the VM was created and not stopped
        Exepected Result:
            - VM&#39;s status should be changed to `Stopped`
            - VM&#39;s `RunStrategy` should be changed to `Halted`
        &#39;&#39;&#39;
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        code, data = api_client.vms.get(unique_vm_name)
        assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
        assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]

    @pytest.mark.dependency(name=&#34;start_vm&#34;, depends=[&#34;stop_vm&#34;])
    def test_start(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Start the VM was created and stopped
        Exepected Result:
            - VM should change status into `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.start(unique_vm_name)
        assert 204 == code, &#34;`Start return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
            pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
            if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_restart(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Restart the VM was created
        Exepected Result:
            - VM&#39;s ActivePods should be updated (which means the VM restarted)
            - VM&#39;s status should update to `Running`
            - VM&#39;s qemu-agent should be connected
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

        old_pods = set(data[&#39;status&#39;][&#39;activePods&#39;].items())

        code, data = api_client.vms.restart(unique_vm_name)
        assert 204 == code, &#34;`Restart return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if old_pods.difference(data[&#39;status&#39;].get(&#39;activePods&#39;, old_pods).items()):
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}), activePods is not updated.\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_softreboot(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Softreboot the VM was created
        Exepected Result:
            - VM&#39;s qemu-agent should disconnected (which means the VM rebooting)
            - VM&#39;s qemu-agent should re-connected (which means the VM boot into OS)
            - VM&#39;s status should be changed to `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
        old_agent = data[&#39;status&#39;][&#39;conditions&#39;][-1]
        assert &#34;AgentConnected&#34; == old_agent[&#39;type&#39;], (code, data)

        api_client.vms.softreboot(unique_vm_name)
        # Wait until agent disconnected (leaving OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if &#34;AgentConnected&#34; not in data[&#39;status&#39;][&#39;conditions&#39;][-1][&#39;type&#39;]:
                break
            sleep(5)
        # then wait agent connected again (Entering OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase, conds = data[&#39;status&#39;][&#39;phase&#39;], data[&#39;status&#39;].get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Softreboot VM({unique_vm_name}) with errors:\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        old_t = datetime.strptime(old_agent[&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)
        new_t = datetime.strptime(conds[-1][&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)

        assert new_t &gt; old_t, (
            &#34;Agent&#39;s probe time is not updated.\t&#34;
            f&#34;Before softreboot: {old_t}, After softreboot: {new_t}\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
        )

    def test_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/ # noqa

        Steps:
            1. migrate the VM was created
        Exepected Result:
            - VM&#39;s host Node should be changed to another one
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;] if cur_host != h[&#39;id&#39;])

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            migrating = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if not migrating and new_host == data[&#39;status&#39;][&#39;nodeName&#39;]:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Migrate VM({unique_vm_name}) from {cur_host} to {new_host}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_abort_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/abort-live-migration/

        Steps:
            1. Abort the VM was created and migrating
        Exepected Result:
            - VM should able to perform migrate
            - VM should stay in current host when migrating be aborted.
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;] if cur_host != h[&#39;id&#39;])

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        states = [&#34;Aborting migration&#34;, &#34;Migrating&#34;]
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            m_state = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if m_state == states[-1]:
                states.pop()
                if states:
                    code, err = api_client.vms.abort_migrate(unique_vm_name)
                    assert 204 == code, (code, err)
                else:
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to abort VM({unique_vm_name})&#39;s migration, stuck on {states[-1]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert cur_host == data[&#39;status&#39;][&#39;nodeName&#39;], (
            f&#34;Failed to abort VM({unique_vm_name})&#39;s migration,&#34;
            f&#34;VM been moved to {data[&#39;status&#39;][&#39;nodeName&#39;]} is not the origin host {cur_host}\n&#34;
        )

    def test_delete(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Delete the VM was created
            2. Delete Volumes was belonged to the VM
        Exepected Result:
            - VM should able to be deleted and success
            - Volumes should able to be deleted and success
        &#39;&#39;&#39;

        code, data = api_client.vms.delete(unique_vm_name)
        assert 200 == code, (code, data)

        spec = api_client.vms.Spec.from_dict(data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        fails, check = [], dict()
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            check[vol_name] = api_client.volumes.delete(vol_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            l_check = dict()
            for vol_name, (code, data) in check.items():
                if 200 != code:
                    fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
                else:
                    code, data = api_client.volumes.get(vol_name)
                    if 404 != code:
                        l_check[vol_name] = (code, data)
            check = l_check
            if not check:
                break
            sleep(5)
        else:
            for vol_name, (code, data) in check.items():
                fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

        assert not fails, (
            f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
            &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_abort_migrate"><code class="name flex">
<span>def <span class="ident">test_abort_migrate</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/live-migration/abort-live-migration/">https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Abort the VM was created and migrating
Exepected Result:<ul>
<li>VM should able to perform migrate</li>
<li>VM should stay in current host when migrating be aborted.</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_abort_migrate(self, api_client, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/live-migration/abort-live-migration/

    Steps:
        1. Abort the VM was created and migrating
    Exepected Result:
        - VM should able to perform migrate
        - VM should stay in current host when migrating be aborted.
    &#34;&#34;&#34;
    code, host_data = api_client.hosts.get()
    assert 200 == code, (code, host_data)
    code, data = api_client.vms.get_status(unique_vm_name)
    cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
    assert cur_host, (
        f&#34;VMI exists but `nodeName` is empty.\n&#34;
        f&#34;{data}&#34;
    )

    new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;] if cur_host != h[&#39;id&#39;])

    code, data = api_client.vms.migrate(unique_vm_name, new_host)
    assert 204 == code, (code, data)

    states = [&#34;Aborting migration&#34;, &#34;Migrating&#34;]
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        m_state = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
        if m_state == states[-1]:
            states.pop()
            if states:
                code, err = api_client.vms.abort_migrate(unique_vm_name)
                assert 204 == code, (code, err)
            else:
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to abort VM({unique_vm_name})&#39;s migration, stuck on {states[-1]}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    assert cur_host == data[&#39;status&#39;][&#39;nodeName&#39;], (
        f&#34;Failed to abort VM({unique_vm_name})&#39;s migration,&#34;
        f&#34;VM been moved to {data[&#39;status&#39;][&#39;nodeName&#39;]} is not the origin host {cur_host}\n&#34;
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_delete"><code class="name flex">
<span>def <span class="ident">test_delete</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Delete the VM was created</li>
<li>Delete Volumes was belonged to the VM
Exepected Result:<ul>
<li>VM should able to be deleted and success</li>
<li>Volumes should able to be deleted and success</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_delete(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Delete the VM was created
        2. Delete Volumes was belonged to the VM
    Exepected Result:
        - VM should able to be deleted and success
        - Volumes should able to be deleted and success
    &#39;&#39;&#39;

    code, data = api_client.vms.delete(unique_vm_name)
    assert 200 == code, (code, data)

    spec = api_client.vms.Spec.from_dict(data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
        sleep(5)
    else:
        for vol_name, (code, data) in check.items():
            fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

    assert not fails, (
        f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_migrate"><code class="name flex">
<span>def <span class="ident">test_migrate</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/">https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>migrate the VM was created
Exepected Result:<ul>
<li>VM's host Node should be changed to another one</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_migrate(self, api_client, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/ # noqa

    Steps:
        1. migrate the VM was created
    Exepected Result:
        - VM&#39;s host Node should be changed to another one
    &#34;&#34;&#34;
    code, host_data = api_client.hosts.get()
    assert 200 == code, (code, host_data)
    code, data = api_client.vms.get_status(unique_vm_name)
    cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
    assert cur_host, (
        f&#34;VMI exists but `nodeName` is empty.\n&#34;
        f&#34;{data}&#34;
    )

    new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;] if cur_host != h[&#39;id&#39;])

    code, data = api_client.vms.migrate(unique_vm_name, new_host)
    assert 204 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        migrating = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
        if not migrating and new_host == data[&#39;status&#39;][&#39;nodeName&#39;]:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to Migrate VM({unique_vm_name}) from {cur_host} to {new_host}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_pause"><code class="name flex">
<span>def <span class="ident">test_pause</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Pause the VM was created
Exepected Result:<ul>
<li>VM should change status into <code>Paused</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;pause_vm&#34;, depends=[&#34;minimal_vm&#34;])
def test_pause(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Pause the VM was created
    Exepected Result:
        - VM should change status into `Paused`
    &#39;&#39;&#39;
    code, data = api_client.vms.pause(unique_vm_name)
    assert 204 == code, &#34;`Pause` return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if [c for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []) if &#34;Paused&#34; == c[&#39;type&#39;]]:
            conditions = data[&#39;status&#39;][&#39;conditions&#39;]
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to pause VM({unique_vm_name}) with errors:\n&#34;
            f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    assert &#34;Paused&#34; == conditions[-1].get(&#39;type&#39;), conditions
    assert &#34;PausedByUser&#34; == conditions[-1].get(&#39;reason&#39;), conditions</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_restart"><code class="name flex">
<span>def <span class="ident">test_restart</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Restart the VM was created
Exepected Result:<ul>
<li>VM's ActivePods should be updated (which means the VM restarted)</li>
<li>VM's status should update to <code>Running</code></li>
<li>VM's qemu-agent should be connected</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_restart(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Restart the VM was created
    Exepected Result:
        - VM&#39;s ActivePods should be updated (which means the VM restarted)
        - VM&#39;s status should update to `Running`
        - VM&#39;s qemu-agent should be connected
    &#39;&#39;&#39;
    code, data = api_client.vms.get_status(unique_vm_name)
    assert 200 == code, (
        f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
        f&#34;Status({code}): {data}&#34;
    )

    old_pods = set(data[&#39;status&#39;][&#39;activePods&#39;].items())

    code, data = api_client.vms.restart(unique_vm_name)
    assert 204 == code, &#34;`Restart return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if old_pods.difference(data[&#39;status&#39;].get(&#39;activePods&#39;, old_pods).items()):
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to Restart VM({unique_vm_name}), activePods is not updated.\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
        conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
        if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Restart VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_softreboot"><code class="name flex">
<span>def <span class="ident">test_softreboot</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Softreboot the VM was created
Exepected Result:<ul>
<li>VM's qemu-agent should disconnected (which means the VM rebooting)</li>
<li>VM's qemu-agent should re-connected (which means the VM boot into OS)</li>
<li>VM's status should be changed to <code>Running</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_softreboot(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Softreboot the VM was created
    Exepected Result:
        - VM&#39;s qemu-agent should disconnected (which means the VM rebooting)
        - VM&#39;s qemu-agent should re-connected (which means the VM boot into OS)
        - VM&#39;s status should be changed to `Running`
    &#39;&#39;&#39;
    code, data = api_client.vms.get_status(unique_vm_name)
    assert 200 == code, (
        f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
        f&#34;Status({code}): {data}&#34;
    )
    old_agent = data[&#39;status&#39;][&#39;conditions&#39;][-1]
    assert &#34;AgentConnected&#34; == old_agent[&#39;type&#39;], (code, data)

    api_client.vms.softreboot(unique_vm_name)
    # Wait until agent disconnected (leaving OS)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if &#34;AgentConnected&#34; not in data[&#39;status&#39;][&#39;conditions&#39;][-1][&#39;type&#39;]:
            break
        sleep(5)
    # then wait agent connected again (Entering OS)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        phase, conds = data[&#39;status&#39;][&#39;phase&#39;], data[&#39;status&#39;].get(&#39;conditions&#39;, [{}])
        if &#34;Running&#34; == phase and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Softreboot VM({unique_vm_name}) with errors:\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    old_t = datetime.strptime(old_agent[&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)
    new_t = datetime.strptime(conds[-1][&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)

    assert new_t &gt; old_t, (
        &#34;Agent&#39;s probe time is not updated.\t&#34;
        f&#34;Before softreboot: {old_t}, After softreboot: {new_t}\n&#34;
        f&#34;Last API Status({code}): {data}&#34;
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_start"><code class="name flex">
<span>def <span class="ident">test_start</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Start the VM was created and stopped
Exepected Result:<ul>
<li>VM should change status into <code>Running</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;start_vm&#34;, depends=[&#34;stop_vm&#34;])
def test_start(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Start the VM was created and stopped
    Exepected Result:
        - VM should change status into `Running`
    &#39;&#39;&#39;
    code, data = api_client.vms.start(unique_vm_name)
    assert 204 == code, &#34;`Start return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
        pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
        if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
        conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
        if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_stop"><code class="name flex">
<span>def <span class="ident">test_stop</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Stop the VM was created and not stopped
Exepected Result:<ul>
<li>VM's status should be changed to <code>Stopped</code></li>
<li>VM's <code>RunStrategy</code> should be changed to <code>Halted</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;stop_vm&#34;, depends=[&#34;minimal_vm&#34;])
def test_stop(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Stop the VM was created and not stopped
    Exepected Result:
        - VM&#39;s status should be changed to `Stopped`
        - VM&#39;s `RunStrategy` should be changed to `Halted`
    &#39;&#39;&#39;
    code, data = api_client.vms.stop(unique_vm_name)
    assert 204 == code, &#34;`Stop` return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    code, data = api_client.vms.get(unique_vm_name)
    assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
    assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_unpause"><code class="name flex">
<span>def <span class="ident">test_unpause</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Unpause the VM was paused
Exepected Result:<ul>
<li>VM's status should not be <code>Paused</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;pause_vm&#34;])
def test_unpause(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Unpause the VM was paused
    Exepected Result:
        - VM&#39;s status should not be `Paused`
    &#39;&#39;&#39;
    code, data = api_client.vms.unpause(unique_vm_name)
    assert 204 == code, &#34;`Unpause` return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        cond_types = set(c[&#39;type&#39;] for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []))
        if {&#34;AgentConnected&#34;} &amp; cond_types and not {&#34;Paused&#34;} &amp; cond_types:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to unpause VM({unique_vm_name}) with errors:\n&#34;
            f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes"><code class="flex name class">
<span>class <span class="ident">TestVMWithVolumes</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMWithVolumes:
    def test_create_with_two_volumes(self, api_client, ssh_keypair, wait_timeout,
                                     host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/ # noqa

        Steps:
            1. Create a VM with 1 CPU 2 Memory and 2 disk volumes
            2. Start the VM
            3. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - 2 disk volumes should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        volumes = [(&#39;disk-1&#39;, 1), (&#39;disk-2&#39;, 2)]
        for name, size in volumes:
            vm_spec.add_volume(name, size)

        # Start VM with 2 additional volumes
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        assert 200 == code, (code, data)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )
        fails = []
        for _, size in volumes:
            if not re.search(f&#34;vd.*{size}G 0 disk&#34;, out):
                fails.append(f&#34;Volume size {size}G not found&#34;)

        assert not fails, (
            f&#34;lsblk output: {out}\n&#34;
            &#34;\n&#34;.join(fails)
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vol_names, vols, claims = [n for n, s in volumes], [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] in vol_names:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for vol_name in claims:
            api_client.volumes.delete(vol_name)

    def test_create_with_existing_volume(self, api_client, ssh_keypair, wait_timeout,
                                         host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/ # noqa

        Steps:
            1. Create a data volume
            2. Create a VM with 1 CPU 2 Memory and the existing data volume
            3. Start the VM
            4. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - Disk volume should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        vol_name, size = &#39;disk-existing&#39;, 3
        vol_spec = api_client.volumes.Spec(size)
        code, data = api_client.volumes.create(f&#34;{unique_vm_name}-{vol_name}&#34;, vol_spec)

        assert 201 == code, (code, data)

        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        vm_spec.add_existing_volume(vol_name, f&#34;{unique_vm_name}-{vol_name}&#34;)

        # Start VM with added existing volume
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        assert f&#34;{size}G 0 disk&#34; in out, (
            f&#34;existing Volume {size}G not found\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vols, claims = [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] == vol_name:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for claim in claims:
            api_client.volumes.delete(claim)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_existing_volume"><code class="name flex">
<span>def <span class="ident">test_create_with_existing_volume</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a data volume</li>
<li>Create a VM with 1 CPU 2 Memory and the existing data volume</li>
<li>Start the VM</li>
<li>Verify the VM</li>
</ol>
<p>Exepected Result:
- VM should able to start and becomes <code>Running</code>
- Disk volume should be available in the VM
- Disk size in VM should be the same as its volume configured</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_create_with_existing_volume(self, api_client, ssh_keypair, wait_timeout,
                                     host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/ # noqa

    Steps:
        1. Create a data volume
        2. Create a VM with 1 CPU 2 Memory and the existing data volume
        3. Start the VM
        4. Verify the VM

    Exepected Result:
        - VM should able to start and becomes `Running`
        - Disk volume should be available in the VM
        - Disk size in VM should be the same as its volume configured
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair

    vol_name, size = &#39;disk-existing&#39;, 3
    vol_spec = api_client.volumes.Spec(size)
    code, data = api_client.volumes.create(f&#34;{unique_vm_name}-{vol_name}&#34;, vol_spec)

    assert 201 == code, (code, data)

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;RerunOnFailure&#34;
    vm_spec.add_existing_volume(vol_name, f&#34;{unique_vm_name}-{vol_name}&#34;)

    # Start VM with added existing volume
    code, data = api_client.vms.update(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    # Log into VM to verify added volumes
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(&#34;lsblk -r&#34;)
            assert not err, (out, err)

    assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
        f&#34;Added Volumes amount is not correct.\n&#34;
        f&#34;lsblk output: {out}&#34;
    )

    assert f&#34;{size}G 0 disk&#34; in out, (
        f&#34;existing Volume {size}G not found\n&#34;
        f&#34;lsblk output: {out}&#34;
    )

    # Tear down: Stop VM and remove added volumes
    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;Halted&#34;
    vols, claims = [], []
    for vd in vm_spec.volumes:
        if vd[&#39;disk&#39;][&#39;name&#39;] == vol_name:
            claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
        else:
            vols.append(vd)
    else:
        vm_spec.volumes = vols

    api_client.vms.update(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)

    for claim in claims:
        api_client.volumes.delete(claim)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_two_volumes"><code class="name flex">
<span>def <span class="ident">test_create_with_two_volumes</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory and 2 disk volumes</li>
<li>Start the VM</li>
<li>Verify the VM</li>
</ol>
<p>Exepected Result:
- VM should able to start and becomes <code>Running</code>
- 2 disk volumes should be available in the VM
- Disk size in VM should be the same as its volume configured</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_create_with_two_volumes(self, api_client, ssh_keypair, wait_timeout,
                                 host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and 2 disk volumes
        2. Start the VM
        3. Verify the VM

    Exepected Result:
        - VM should able to start and becomes `Running`
        - 2 disk volumes should be available in the VM
        - Disk size in VM should be the same as its volume configured
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair
    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;RerunOnFailure&#34;
    volumes = [(&#39;disk-1&#39;, 1), (&#39;disk-2&#39;, 2)]
    for name, size in volumes:
        vm_spec.add_volume(name, size)

    # Start VM with 2 additional volumes
    code, data = api_client.vms.update(unique_vm_name, vm_spec)
    assert 200 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    # Log into VM to verify added volumes
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(&#34;lsblk -r&#34;)
            assert not err, (out, err)

    assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
        f&#34;Added Volumes amount is not correct.\n&#34;
        f&#34;lsblk output: {out}&#34;
    )
    fails = []
    for _, size in volumes:
        if not re.search(f&#34;vd.*{size}G 0 disk&#34;, out):
            fails.append(f&#34;Volume size {size}G not found&#34;)

    assert not fails, (
        f&#34;lsblk output: {out}\n&#34;
        &#34;\n&#34;.join(fails)
    )

    # Tear down: Stop VM and remove added volumes
    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;Halted&#34;
    vol_names, vols, claims = [n for n, s in volumes], [], []
    for vd in vm_spec.volumes:
        if vd[&#39;disk&#39;][&#39;name&#39;] in vol_names:
            claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
        else:
            vols.append(vd)
    else:
        vm_spec.volumes = vols

    api_client.vms.update(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)

    for vol_name in claims:
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.integration" href="index.html">harvester_e2e_tests.integration</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.image" href="#harvester_e2e_tests.integration.test_vm_functions.image">image</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.kubeconfig_file" href="#harvester_e2e_tests.integration.test_vm_functions.kubeconfig_file">kubeconfig_file</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.stopped_vm" href="#harvester_e2e_tests.integration.test_vm_functions.stopped_vm">stopped_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_create_stopped_vm" href="#harvester_e2e_tests.integration.test_vm_functions.test_create_stopped_vm">test_create_stopped_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_minimal_vm" href="#harvester_e2e_tests.integration.test_vm_functions.test_minimal_vm">test_minimal_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.unique_vm_name" href="#harvester_e2e_tests.integration.test_vm_functions.unique_vm_name">unique_vm_name</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.virtctl" href="#harvester_e2e_tests.integration.test_vm_functions.virtctl">virtctl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone">TestVMClone</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_running_vm" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_running_vm">test_clone_running_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_stopped_vm" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_stopped_vm">test_clone_stopped_vm</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations">TestVMOperations</a></code></h4>
<ul class="two-column">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_abort_migrate" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_abort_migrate">test_abort_migrate</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_delete" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_delete">test_delete</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_migrate" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_migrate">test_migrate</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_pause" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_pause">test_pause</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_restart" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_restart">test_restart</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_softreboot" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_softreboot">test_softreboot</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_start" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_start">test_start</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_stop" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_stop">test_stop</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_unpause" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_unpause">test_unpause</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes">TestVMWithVolumes</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_existing_volume" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_existing_volume">test_create_with_existing_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_two_volumes" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_two_volumes">test_create_with_two_volumes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>