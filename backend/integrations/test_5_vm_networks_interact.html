<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>harvester_e2e_tests.integrations.test_5_vm_networks_interact API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.integrations.test_5_vm_networks_interact</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import shlex
import subprocess
from time import sleep
from operator import add
from functools import reduce
from datetime import datetime, timedelta

import pytest
import paramiko


pytest_plugins = [
    &#34;harvester_e2e_tests.fixtures.api_client&#34;,
    &#34;harvester_e2e_tests.fixtures.networks&#34;,
    &#34;harvester_e2e_tests.fixtures.virtualmachines&#34;
]

tcp = &#34;sudo sed -i &#39;s/AllowTcpForwarding no/AllowTcpForwarding yes/g&#39; /etc/ssh/sshd_config&#34;
restore_tcp = &#34;sudo sed -i &#39;s/AllowTcpForwarding yes/AllowTcpForwarding no/g&#39; /etc/ssh/sshd_config&#34;
restart_ssh = &#34;sudo systemctl restart sshd.service&#34;

vm_credential = {&#34;user&#34;: &#34;opensuse&#34;, &#34;password&#34;: &#34;123456&#34;}
node_user = &#34;rancher&#34;

cloud_user_data = \
    &#34;&#34;&#34;
password: {password}\nchpasswd: {{ expire: False }}\nssh_pwauth: True
&#34;&#34;&#34;

cloud_network_data = \
    &#34;&#34;&#34;
network:
  version: 1
  config:
    - type: physical
      name: eth0
      subnets:
        - type: dhcp
    - type: physical
      name: eth1
      subnets:
        - type: dhcp
&#34;&#34;&#34;


@pytest.fixture(scope=&#34;session&#34;)
def client():
    client = paramiko.client.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    yield client
    client.close()


@pytest.fixture(scope=&#39;module&#39;)
def cluster_network(vlan_nic, api_client, unique_name):
    code, data = api_client.clusternetworks.get_config()
    assert 200 == code, (code, data)

    node_key = &#39;network.harvesterhci.io/matched-nodes&#39;
    cnet_nodes = dict()  # cluster_network: items
    for cfg in data[&#39;items&#39;]:
        if vlan_nic in cfg[&#39;spec&#39;][&#39;uplink&#39;][&#39;nics&#39;]:
            nodes = json.loads(cfg[&#39;metadata&#39;][&#39;annotations&#39;][node_key])
            cnet_nodes.setdefault(cfg[&#39;spec&#39;][&#39;clusterNetwork&#39;], []).extend(nodes)

    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    all_nodes = set(n[&#39;id&#39;] for n in data[&#39;data&#39;])
    try:
        # vlad_nic configured on specific cluster network, reuse it
        yield next(cnet for cnet, nodes in cnet_nodes.items() if all_nodes == set(nodes))
        return None
    except StopIteration:
        configured_nodes = reduce(add, cnet_nodes.values(), [])
        if any(n in configured_nodes for n in all_nodes):
            raise AssertionError(
                &#34;Not all nodes&#39; VLAN NIC {vlan_nic} are available.\n&#34;
                f&#34;VLAN NIC configured nodes: {configured_nodes}\n&#34;
                f&#34;All nodes: {all_nodes}\n&#34;
            )

    # Create cluster network
    cnet = f&#34;cnet-{datetime.strptime(unique_name, &#39;%Hh%Mm%Ss%f-%m-%d&#39;).strftime(&#39;%H%M%S&#39;)}&#34;
    created = []
    code, data = api_client.clusternetworks.create(cnet)
    assert 201 == code, (code, data)
    while all_nodes:
        node = all_nodes.pop()
        code, data = api_client.clusternetworks.create_config(node, cnet, vlan_nic, hostname=node)
        assert 201 == code, (
            f&#34;Failed to create cluster config for {node}\n&#34;
            f&#34;Created: {created}\t Remaining: {all_nodes}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        created.append(node)

    yield cnet

    # Teardown
    deleted = {name: api_client.clusternetworks.delete_config(name) for name in created}
    failed = [(name, code, data) for name, (code, data) in deleted.items() if 200 != code]
    if failed:
        fmt = &#34;Unable to delete VLAN Config {} with error ({}): {}&#34;
        raise AssertionError(
            &#34;\n&#34;.join(fmt.format(name, code, data) for (name, code, data) in failed)
        )

    code, data = api_client.clusternetworks.delete(cnet)
    assert 200 == code, (code, data)


@pytest.fixture(scope=&#34;module&#34;)
def vm_network(api_client, unique_name, wait_timeout, cluster_network, vlan_id):
    code, data = api_client.networks.create(
        unique_name, vlan_id, cluster_network=cluster_network
    )
    assert 201 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(unique_name)
        annotations = data[&#39;metadata&#39;].get(&#39;annotations&#39;, {})
        if 200 == code and annotations.get(&#39;network.harvesterhci.io/route&#39;):
            route = json.loads(annotations[&#39;network.harvesterhci.io/route&#39;])
            if route[&#39;cidr&#39;]:
                break
        sleep(3)
    else:
        raise AssertionError(
            &#34;VM network created but route info not available\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    yield dict(name=unique_name, cidr=route[&#39;cidr&#39;], namespace=data[&#39;metadata&#39;][&#39;namespace&#39;])

    code, data = api_client.networks.delete(unique_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(unique_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to remote VM network {unique_name} after {wait_timeout}s\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )


def create_image_url(api_client, display_name, image_url, wait_timeout):
    code, data = api_client.images.create_by_url(display_name, image_url)

    assert 201 == code, (code, data)
    image_spec = data.get(&#39;spec&#39;)

    assert display_name == image_spec.get(&#39;displayName&#39;)
    assert &#34;download&#34; == image_spec.get(&#39;sourceType&#39;)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.images.get(display_name)
        image_status = data.get(&#39;status&#39;, {})

        assert 200 == code, (code, data)
        if image_status.get(&#39;progress&#39;) == 100:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to download image {display_name} with {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )


def check_vm_running(api_client, unique_name, wait_timeout):
    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name)
        vm_fields = data[&#39;metadata&#39;][&#39;fields&#39;]

        assert 200 == code, (code, data)
        if vm_fields[2] == &#39;Running&#39;:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to create VM {unique_name} in Running status, exceed given timeout\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )


def check_vm_ip_exists(api_client, unique_name, wait_timeout):
    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )


def delete_vm(api_client, unique_name, wait_timeout):
    api_client.vms.delete(unique_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        if code == 404:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to delete VM {unique_name}, exceed given timeout\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )


@pytest.mark.p0
@pytest.mark.networks
class TestBackendNetwork:

    @pytest.mark.p0
    @pytest.mark.networks
    def test_mgmt_network_connection(
        self, api_client, request, client, image_opensuse, unique_name, wait_timeout,
        host_shell, vm_shell_from_host
    ):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/validate-network-management-network/


        Steps:
        1. Create a new VM
        2. Make sure that the network is set to the management network with masquerade as the type
        3. Wait until the VM boot in running state
        4. Check can ping VM with management network from Harvester node
        5. Check can SSH to VM with management network from Harvester node
        6. Check can&#39;t SSH to VM with management network from external host
        &#34;&#34;&#34;
        vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)
        vm_user, vm_passwd = vm_credential[&#39;user&#39;], vm_credential[&#39;password&#39;]

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        # Update AllowTcpForwarding for ssh jumpstart

        spec = api_client.vms.Spec(1, 2)

        spec.user_data += cloud_user_data.format(password=vm_passwd)

        unique_name = unique_name + &#34;-mgmt&#34;
        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        mgmt_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Ping management ip address from Harvester node
        with host_shell.login(vip) as sh:
            stdout, stderr = sh.exec_command(f&#34;ping -c 50 {mgmt_ip}&#34;)

        assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # SSH to management ip address and execute command from Harvester node
        with vm_shell_from_host(vip, mgmt_ip, vm_user, vm_passwd) as sh:
            stdout, stderr = sh.exec_command(&#34;ls&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # Check should not SSH to management ip address from external host
        command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

        with pytest.raises(subprocess.CalledProcessError) as ex:
            subprocess.check_output(command, stderr=subprocess.STDOUT,
                                    shell=False, encoding=&#34;utf-8&#34;)

        # OpenSSH returns the return code of the program that was executed on
        # the remote, unless there was an error for SSH itself, in which case
        # it returns 255
        assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                            &#34; to VM on management interface&#34;
                                            f&#34; {mgmt_ip} from external host&#34;)

        # cleanup VM
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    @pytest.mark.dependency(name=&#34;vlan_network_connection&#34;)
    def test_vlan_network_connection(self, api_client, request, client, unique_name,
                                     image_opensuse, vm_network, wait_timeout):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/validate-network-external-vlan/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM and set the external vlan network to it
        3. Check can ping external VLAN IP from external host
        4. Check can SSH to VM from external IP from external host
        &#34;&#34;&#34;
        unique_name = unique_name + &#34;-vlan&#34;

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2, mgmt_network=False)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
            f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
        )

        vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Ping vlan ip address from external host
        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;50&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # SSH to vlan ip address and execute command from external host
        _stdout, _stderr = self.ssh_client(
            client, vlan_ip, vm_credential[&#34;user&#34;], vm_credential[&#34;password&#34;], &#39;ls&#39;, wait_timeout)

        stdout = _stdout.read().decode(&#39;ascii&#39;).strip(&#34;\n&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM external vlan IP {vlan_ip}&#34;
            f&#34;on vlan interface from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    @pytest.mark.dependency(name=&#34;reboot_vlan_connection&#34;,
                            depends=[&#34;vlan_network_connection&#34;])
    def test_reboot_vlan_connection(self, api_client, request, unique_name,
                                    image_opensuse, vm_network, wait_timeout):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM and add the external vlan network
        3. Check can ping external VLAN IP
        4. Reboot VM
        5. Ping VM during reboot
        6. Check can&#39;t ping VM during reboot
        7. Check the VM should reboot
        8. Ping VM after reboot
        9. Check can ping VM
        &#34;&#34;&#34;
        unique_name = unique_name + &#34;-reboot-vlan&#34;

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2, mgmt_network=False)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
            f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
        )

        vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Check can ping vlan ip

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # Restart VM
        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to reboot vm with error: {code}, {data}&#34;)

        # Check VM start in Starting state
        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
            vm_fields = data[&#39;metadata&#39;][&#39;fields&#39;]

            if vm_fields[2] == &#39;Starting&#39;:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to restart VM {unique_name} in Starting status, exceed given timeout\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Check can&#39;t ping vlan ip during reboot

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

        process = subprocess.run(command, stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE, universal_newlines=True)

        result = process.stdout

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &lt; 0, (
            f&#34;Failed: since can ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host during reboot&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
            f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
        )

        vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Ping vlan ip address

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    def test_mgmt_to_vlan_connection(self, api_client, request, client, unique_name,
                                     image_opensuse, vm_network, wait_timeout):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM
        3. Make sure that the network is set to the management network with masquerade as the type
        4. Wait until the VM boot in running state
        5. Edit VM and change management network to external VLAN with bridge type
        6. Check VM should save and reboot
        7. Check can ping the VM from an external network host
        8. Check can ssh to the VM from an external network host
        &#34;&#34;&#34;

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])
        unique_name = unique_name + &#34;-mgmt-vlan&#34;
        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)
        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # get data from running VM and transfer to spec
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        spec = spec.from_dict(data)

        # Switch to vlan network
        spec.mgmt_network = False

        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        # Update VM spec
        code, data = api_client.vms.update(unique_name, spec)
        assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        # Determine by vlan network Name
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        ip_addresses = []

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
            if &#39;interfaces&#39; in data[&#39;status&#39;]:
                interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
                ip_addresses = []
                for interface in interfaces_data:
                    # Check the ipAddress in digital format
                    if (
                        &#39;ipAddress&#39; in interface and
                        interface[&#39;ipAddress&#39;].replace(&#39;.&#39;, &#39;&#39;).isdigit()
                    ):
                        ip_addresses.append(interface[&#39;ipAddress&#39;])

                if len(ip_addresses) &gt; 0:
                    if &#39;nic-1&#39; in interface[&#39;name&#39;]:
                        break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to get VM {unique_name} IP address, exceed given timeout\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Ping vlan ip address
        vlan_ip = ip_addresses[0]

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;50&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # SSH to vlan ip address and execute command
        _stdout, _stderr = self.ssh_client(
            client, vlan_ip, vm_credential[&#34;user&#34;], vm_credential[&#34;password&#34;], &#39;ls&#39;, wait_timeout)

        stdout = _stdout.read().decode(&#39;ascii&#39;).strip(&#34;\n&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM external vlan IP {vlan_ip}&#34;
            f&#34;on vlan interface from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    def test_vlan_to_mgmt_connection(
        self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout,
        host_shell, vm_shell_from_host
    ):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM
        3. Make sure that the network is set to the vlan network with bridge as the type
        4. Wait until the VM boot in running state
        5. Edit VM and change from external VLAN to management network
        6. Check VM should save and reboot
        7. Check can ping VM with management network from Harvester node
        8. Check can SSH to VM with management network from Harvester node
        9. Check can&#39;t SSH to VM with management network from external host
        &#34;&#34;&#34;

        vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)
        vm_user, vm_passwd = vm_credential[&#39;user&#39;], vm_credential[&#39;password&#39;]

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2, mgmt_network=False)
        spec.user_data += cloud_user_data.format(password=vm_passwd)
        unique_name = unique_name + &#34;-vlan-mgmt&#34;

        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)
        spec.add_network(&#34;default&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # get data from running VM and transfer to spec
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        spec = spec.from_dict(data)

        spec.networks = []
        spec.mgmt_network = True

        code, data = api_client.vms.update(unique_name, spec)
        assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm status: {code}, {data}&#34;)

            if &#39;interfaces&#39; in data[&#39;status&#39;]:
                interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
                ip_addresses = []
                if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:

                    if &#39;default&#39; in interfaces_data[0][&#39;name&#39;]:
                        if &#39;domain, guest-agent&#39; not in interfaces_data[0][&#39;infoSource&#39;]:
                            ip_addresses.append(interfaces_data[0][&#39;ipAddress&#39;])
                            break
                sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Check can ping management ip address from Harvester node
        mgmt_ip = ip_addresses[0]

        with host_shell.login(vip) as sh:
            stdout, stderr = sh.exec_command(f&#34;ping -c 50 {mgmt_ip}&#34;)

        assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # Check can ssh to host and execute command from Harvester node
        with vm_shell_from_host(vip, mgmt_ip, vm_user, vm_passwd) as sh:
            stdout, stderr = sh.exec_command(&#34;ls&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # Check should not SSH to management ip address from external host
        command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

        with pytest.raises(subprocess.CalledProcessError) as ex:
            subprocess.check_output(command, stderr=subprocess.STDOUT,
                                    shell=False, encoding=&#34;utf-8&#34;)

        # OpenSSH returns the return code of the program that was executed on
        # the remote, unless there was an error for SSH itself, in which case
        # it returns 255
        assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                            &#34; to VM on management interface&#34;
                                            f&#34; {mgmt_ip} from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    def test_delete_vlan_from_multiple(
        self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout,
        host_shell
    ):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/delete-vlan-network-form/

        Steps:
        1. Create an external VLAN network
        2. Make sure that the network is set to the management network with masquerade as the type
        3. Add another external VLAN management
        4. Create VM
        5. Wait until the VM boot in running state
        6. Delete the external VLAN from VM
        7. Check can ping the VM on the management network
        8. Check can&#39;t SSH to VM with management network from external host
        &#34;&#34;&#34;

        vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

        # Add network data to trigger DHCP on multiple NICs
        spec.network_data += cloud_network_data

        unique_name = unique_name + &#34;-delete-vlan&#34;

        # Add image
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        # Add external vlan network
        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        # Create VM
        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check have 2 NICs and wait until all ip address exists
        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
            if len(data[&#39;status&#39;][&#39;interfaces&#39;]) == 2:
                if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:
                    if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]:
                        break
                sleep(5)

        else:
            raise AssertionError(
                f&#34;Failed to get multiple IPs on VM: {unique_name}, exceed the given timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # get data from running VM and transfer to spec
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        spec = spec.from_dict(data)

        spec.networks = []
        spec.mgmt_network = True

        code, data = api_client.vms.update(unique_name, spec)
        assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        ip_addresses = []

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm status: {code}, {data}&#34;)
            if &#39;interfaces&#39; in data[&#39;status&#39;]:
                interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
                ip_addresses = []

                interfaces = data[&#39;status&#39;][&#39;interfaces&#39;]

                if len(interfaces) == 1 and &#39;ipAddress&#39; in interfaces[0]:
                    ip_addresses.append(interfaces_data[0][&#39;ipAddress&#39;])

                    if &#39;default&#39; in interfaces_data[0][&#39;name&#39;]:
                        break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Ping management ip address
        mgmt_ip = ip_addresses[0]

        ping_command = &#34;ping -c 50 {0}&#34;.format(mgmt_ip)

        with host_shell.login(vip) as sh:
            stdout, stderr = sh.exec_command(ping_command)

        assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node: {code}, {data}&#34;)

        # #Check should not SSH to management ip address from external host
        command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

        with pytest.raises(subprocess.CalledProcessError) as ex:
            subprocess.check_output(command, stderr=subprocess.STDOUT,
                                    shell=False, encoding=&#34;utf-8&#34;)

        # OpenSSH returns the return code of the program that was executed on
        # the remote, unless there was an error for SSH itself, in which case
        # it returns 255
        assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                            &#34; to VM on management interface&#34;
                                            f&#34; {mgmt_ip} from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    def ssh_client(self, client, dest_ip, username, password, command, timeout,
                   allow_agent=False, look_for_keys=False):
        client.connect(dest_ip, username=username, password=password,
                       allow_agent=allow_agent, look_for_keys=look_for_keys,
                       timeout=timeout)

        split_command = shlex.split(command)
        _stdin, _stdout, _stderr = client.exec_command(&#39; &#39;.join(
            shlex.quote(part) for part in split_command), get_pty=True)
        return _stdout, _stderr

    def ssh_jumpstart(self, client, dest_ip, client_ip, client_user, client_password,
                      dest_user, dest_password, command, allow_agent=False, look_for_keys=False):
        client.connect(client_ip, username=client_user, password=client_password,
                       allow_agent=allow_agent, look_for_keys=look_for_keys)

        client_transport = client.get_transport()
        dest_addr = (dest_ip, 22)
        client_addr = (client_ip, 22)
        client_channel = client_transport.open_channel(&#34;direct-tcpip&#34;, dest_addr, client_addr)

        jumpstart = paramiko.SSHClient()
        jumpstart.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        jumpstart.connect(dest_ip, username=dest_user, password=dest_password, sock=client_channel)

        split_command = shlex.split(command)
        _stdin, _stdout, _stderr = jumpstart.exec_command(&#39; &#39;.join(
            shlex.quote(part) for part in split_command))
        return _stdout, _stderr</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.check_vm_ip_exists"><code class="name flex">
<span>def <span class="ident">check_vm_ip_exists</span></span>(<span>api_client, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_vm_ip_exists(api_client, unique_name, wait_timeout):
    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.check_vm_running"><code class="name flex">
<span>def <span class="ident">check_vm_running</span></span>(<span>api_client, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_vm_running(api_client, unique_name, wait_timeout):
    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name)
        vm_fields = data[&#39;metadata&#39;][&#39;fields&#39;]

        assert 200 == code, (code, data)
        if vm_fields[2] == &#39;Running&#39;:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to create VM {unique_name} in Running status, exceed given timeout\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.client"><code class="name flex">
<span>def <span class="ident">client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;session&#34;)
def client():
    client = paramiko.client.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    yield client
    client.close()</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.cluster_network"><code class="name flex">
<span>def <span class="ident">cluster_network</span></span>(<span>vlan_nic, api_client, unique_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#39;module&#39;)
def cluster_network(vlan_nic, api_client, unique_name):
    code, data = api_client.clusternetworks.get_config()
    assert 200 == code, (code, data)

    node_key = &#39;network.harvesterhci.io/matched-nodes&#39;
    cnet_nodes = dict()  # cluster_network: items
    for cfg in data[&#39;items&#39;]:
        if vlan_nic in cfg[&#39;spec&#39;][&#39;uplink&#39;][&#39;nics&#39;]:
            nodes = json.loads(cfg[&#39;metadata&#39;][&#39;annotations&#39;][node_key])
            cnet_nodes.setdefault(cfg[&#39;spec&#39;][&#39;clusterNetwork&#39;], []).extend(nodes)

    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    all_nodes = set(n[&#39;id&#39;] for n in data[&#39;data&#39;])
    try:
        # vlad_nic configured on specific cluster network, reuse it
        yield next(cnet for cnet, nodes in cnet_nodes.items() if all_nodes == set(nodes))
        return None
    except StopIteration:
        configured_nodes = reduce(add, cnet_nodes.values(), [])
        if any(n in configured_nodes for n in all_nodes):
            raise AssertionError(
                &#34;Not all nodes&#39; VLAN NIC {vlan_nic} are available.\n&#34;
                f&#34;VLAN NIC configured nodes: {configured_nodes}\n&#34;
                f&#34;All nodes: {all_nodes}\n&#34;
            )

    # Create cluster network
    cnet = f&#34;cnet-{datetime.strptime(unique_name, &#39;%Hh%Mm%Ss%f-%m-%d&#39;).strftime(&#39;%H%M%S&#39;)}&#34;
    created = []
    code, data = api_client.clusternetworks.create(cnet)
    assert 201 == code, (code, data)
    while all_nodes:
        node = all_nodes.pop()
        code, data = api_client.clusternetworks.create_config(node, cnet, vlan_nic, hostname=node)
        assert 201 == code, (
            f&#34;Failed to create cluster config for {node}\n&#34;
            f&#34;Created: {created}\t Remaining: {all_nodes}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        created.append(node)

    yield cnet

    # Teardown
    deleted = {name: api_client.clusternetworks.delete_config(name) for name in created}
    failed = [(name, code, data) for name, (code, data) in deleted.items() if 200 != code]
    if failed:
        fmt = &#34;Unable to delete VLAN Config {} with error ({}): {}&#34;
        raise AssertionError(
            &#34;\n&#34;.join(fmt.format(name, code, data) for (name, code, data) in failed)
        )

    code, data = api_client.clusternetworks.delete(cnet)
    assert 200 == code, (code, data)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.create_image_url"><code class="name flex">
<span>def <span class="ident">create_image_url</span></span>(<span>api_client, display_name, image_url, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_image_url(api_client, display_name, image_url, wait_timeout):
    code, data = api_client.images.create_by_url(display_name, image_url)

    assert 201 == code, (code, data)
    image_spec = data.get(&#39;spec&#39;)

    assert display_name == image_spec.get(&#39;displayName&#39;)
    assert &#34;download&#34; == image_spec.get(&#39;sourceType&#39;)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.images.get(display_name)
        image_status = data.get(&#39;status&#39;, {})

        assert 200 == code, (code, data)
        if image_status.get(&#39;progress&#39;) == 100:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to download image {display_name} with {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.delete_vm"><code class="name flex">
<span>def <span class="ident">delete_vm</span></span>(<span>api_client, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_vm(api_client, unique_name, wait_timeout):
    api_client.vms.delete(unique_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        if code == 404:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to delete VM {unique_name}, exceed given timeout\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.vm_network"><code class="name flex">
<span>def <span class="ident">vm_network</span></span>(<span>api_client, unique_name, wait_timeout, cluster_network, vlan_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def vm_network(api_client, unique_name, wait_timeout, cluster_network, vlan_id):
    code, data = api_client.networks.create(
        unique_name, vlan_id, cluster_network=cluster_network
    )
    assert 201 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(unique_name)
        annotations = data[&#39;metadata&#39;].get(&#39;annotations&#39;, {})
        if 200 == code and annotations.get(&#39;network.harvesterhci.io/route&#39;):
            route = json.loads(annotations[&#39;network.harvesterhci.io/route&#39;])
            if route[&#39;cidr&#39;]:
                break
        sleep(3)
    else:
        raise AssertionError(
            &#34;VM network created but route info not available\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    yield dict(name=unique_name, cidr=route[&#39;cidr&#39;], namespace=data[&#39;metadata&#39;][&#39;namespace&#39;])

    code, data = api_client.networks.delete(unique_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(unique_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to remote VM network {unique_name} after {wait_timeout}s\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork"><code class="flex name class">
<span>class <span class="ident">TestBackendNetwork</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
class TestBackendNetwork:

    @pytest.mark.p0
    @pytest.mark.networks
    def test_mgmt_network_connection(
        self, api_client, request, client, image_opensuse, unique_name, wait_timeout,
        host_shell, vm_shell_from_host
    ):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/validate-network-management-network/


        Steps:
        1. Create a new VM
        2. Make sure that the network is set to the management network with masquerade as the type
        3. Wait until the VM boot in running state
        4. Check can ping VM with management network from Harvester node
        5. Check can SSH to VM with management network from Harvester node
        6. Check can&#39;t SSH to VM with management network from external host
        &#34;&#34;&#34;
        vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)
        vm_user, vm_passwd = vm_credential[&#39;user&#39;], vm_credential[&#39;password&#39;]

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        # Update AllowTcpForwarding for ssh jumpstart

        spec = api_client.vms.Spec(1, 2)

        spec.user_data += cloud_user_data.format(password=vm_passwd)

        unique_name = unique_name + &#34;-mgmt&#34;
        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        mgmt_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Ping management ip address from Harvester node
        with host_shell.login(vip) as sh:
            stdout, stderr = sh.exec_command(f&#34;ping -c 50 {mgmt_ip}&#34;)

        assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # SSH to management ip address and execute command from Harvester node
        with vm_shell_from_host(vip, mgmt_ip, vm_user, vm_passwd) as sh:
            stdout, stderr = sh.exec_command(&#34;ls&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # Check should not SSH to management ip address from external host
        command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

        with pytest.raises(subprocess.CalledProcessError) as ex:
            subprocess.check_output(command, stderr=subprocess.STDOUT,
                                    shell=False, encoding=&#34;utf-8&#34;)

        # OpenSSH returns the return code of the program that was executed on
        # the remote, unless there was an error for SSH itself, in which case
        # it returns 255
        assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                            &#34; to VM on management interface&#34;
                                            f&#34; {mgmt_ip} from external host&#34;)

        # cleanup VM
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    @pytest.mark.dependency(name=&#34;vlan_network_connection&#34;)
    def test_vlan_network_connection(self, api_client, request, client, unique_name,
                                     image_opensuse, vm_network, wait_timeout):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/validate-network-external-vlan/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM and set the external vlan network to it
        3. Check can ping external VLAN IP from external host
        4. Check can SSH to VM from external IP from external host
        &#34;&#34;&#34;
        unique_name = unique_name + &#34;-vlan&#34;

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2, mgmt_network=False)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
            f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
        )

        vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Ping vlan ip address from external host
        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;50&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # SSH to vlan ip address and execute command from external host
        _stdout, _stderr = self.ssh_client(
            client, vlan_ip, vm_credential[&#34;user&#34;], vm_credential[&#34;password&#34;], &#39;ls&#39;, wait_timeout)

        stdout = _stdout.read().decode(&#39;ascii&#39;).strip(&#34;\n&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM external vlan IP {vlan_ip}&#34;
            f&#34;on vlan interface from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    @pytest.mark.dependency(name=&#34;reboot_vlan_connection&#34;,
                            depends=[&#34;vlan_network_connection&#34;])
    def test_reboot_vlan_connection(self, api_client, request, unique_name,
                                    image_opensuse, vm_network, wait_timeout):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM and add the external vlan network
        3. Check can ping external VLAN IP
        4. Reboot VM
        5. Ping VM during reboot
        6. Check can&#39;t ping VM during reboot
        7. Check the VM should reboot
        8. Ping VM after reboot
        9. Check can ping VM
        &#34;&#34;&#34;
        unique_name = unique_name + &#34;-reboot-vlan&#34;

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2, mgmt_network=False)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
            f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
        )

        vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Check can ping vlan ip

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # Restart VM
        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to reboot vm with error: {code}, {data}&#34;)

        # Check VM start in Starting state
        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
            vm_fields = data[&#39;metadata&#39;][&#39;fields&#39;]

            if vm_fields[2] == &#39;Starting&#39;:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to restart VM {unique_name} in Starting status, exceed given timeout\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Check can&#39;t ping vlan ip during reboot

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

        process = subprocess.run(command, stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE, universal_newlines=True)

        result = process.stdout

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &lt; 0, (
            f&#34;Failed: since can ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host during reboot&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
            f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
        )

        vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

        # Ping vlan ip address

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    def test_mgmt_to_vlan_connection(self, api_client, request, client, unique_name,
                                     image_opensuse, vm_network, wait_timeout):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM
        3. Make sure that the network is set to the management network with masquerade as the type
        4. Wait until the VM boot in running state
        5. Edit VM and change management network to external VLAN with bridge type
        6. Check VM should save and reboot
        7. Check can ping the VM from an external network host
        8. Check can ssh to the VM from an external network host
        &#34;&#34;&#34;

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])
        unique_name = unique_name + &#34;-mgmt-vlan&#34;
        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)
        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # get data from running VM and transfer to spec
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        spec = spec.from_dict(data)

        # Switch to vlan network
        spec.mgmt_network = False

        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        # Update VM spec
        code, data = api_client.vms.update(unique_name, spec)
        assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # Get VM interface ipAddresses
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

        interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

        assert 1 == len(interfaces_data), (
            f&#34;Failed: get more than one interface: {interfaces_data}&#34;
        )

        # Determine by vlan network Name
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        ip_addresses = []

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
            if &#39;interfaces&#39; in data[&#39;status&#39;]:
                interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
                ip_addresses = []
                for interface in interfaces_data:
                    # Check the ipAddress in digital format
                    if (
                        &#39;ipAddress&#39; in interface and
                        interface[&#39;ipAddress&#39;].replace(&#39;.&#39;, &#39;&#39;).isdigit()
                    ):
                        ip_addresses.append(interface[&#39;ipAddress&#39;])

                if len(ip_addresses) &gt; 0:
                    if &#39;nic-1&#39; in interface[&#39;name&#39;]:
                        break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to get VM {unique_name} IP address, exceed given timeout\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Ping vlan ip address
        vlan_ip = ip_addresses[0]

        command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;50&#39;, vlan_ip]

        result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

        assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
            f&#34;on vlan interface from external host&#34;)

        # SSH to vlan ip address and execute command
        _stdout, _stderr = self.ssh_client(
            client, vlan_ip, vm_credential[&#34;user&#34;], vm_credential[&#34;password&#34;], &#39;ls&#39;, wait_timeout)

        stdout = _stdout.read().decode(&#39;ascii&#39;).strip(&#34;\n&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM external vlan IP {vlan_ip}&#34;
            f&#34;on vlan interface from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    def test_vlan_to_mgmt_connection(
        self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout,
        host_shell, vm_shell_from_host
    ):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/


        Steps:
        1. Create an external VLAN network
        2. Create a new VM
        3. Make sure that the network is set to the vlan network with bridge as the type
        4. Wait until the VM boot in running state
        5. Edit VM and change from external VLAN to management network
        6. Check VM should save and reboot
        7. Check can ping VM with management network from Harvester node
        8. Check can SSH to VM with management network from Harvester node
        9. Check can&#39;t SSH to VM with management network from external host
        &#34;&#34;&#34;

        vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)
        vm_user, vm_passwd = vm_credential[&#39;user&#39;], vm_credential[&#39;password&#39;]

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2, mgmt_network=False)
        spec.user_data += cloud_user_data.format(password=vm_passwd)
        unique_name = unique_name + &#34;-vlan-mgmt&#34;

        # Create VM
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)
        spec.add_network(&#34;default&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        # get data from running VM and transfer to spec
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        spec = spec.from_dict(data)

        spec.networks = []
        spec.mgmt_network = True

        code, data = api_client.vms.update(unique_name, spec)
        assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm status: {code}, {data}&#34;)

            if &#39;interfaces&#39; in data[&#39;status&#39;]:
                interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
                ip_addresses = []
                if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:

                    if &#39;default&#39; in interfaces_data[0][&#39;name&#39;]:
                        if &#39;domain, guest-agent&#39; not in interfaces_data[0][&#39;infoSource&#39;]:
                            ip_addresses.append(interfaces_data[0][&#39;ipAddress&#39;])
                            break
                sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Check can ping management ip address from Harvester node
        mgmt_ip = ip_addresses[0]

        with host_shell.login(vip) as sh:
            stdout, stderr = sh.exec_command(f&#34;ping -c 50 {mgmt_ip}&#34;)

        assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # Check can ssh to host and execute command from Harvester node
        with vm_shell_from_host(vip, mgmt_ip, vm_user, vm_passwd) as sh:
            stdout, stderr = sh.exec_command(&#34;ls&#34;)

        assert stdout.find(&#34;bin&#34;) == 0, (
            f&#34;Failed to ssh to VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node&#34;)

        # Check should not SSH to management ip address from external host
        command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

        with pytest.raises(subprocess.CalledProcessError) as ex:
            subprocess.check_output(command, stderr=subprocess.STDOUT,
                                    shell=False, encoding=&#34;utf-8&#34;)

        # OpenSSH returns the return code of the program that was executed on
        # the remote, unless there was an error for SSH itself, in which case
        # it returns 255
        assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                            &#34; to VM on management interface&#34;
                                            f&#34; {mgmt_ip} from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    @pytest.mark.p0
    @pytest.mark.networks
    def test_delete_vlan_from_multiple(
        self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout,
        host_shell
    ):
        &#34;&#34;&#34;
        Manual test plan reference:
        https://harvester.github.io/tests/manual/network/delete-vlan-network-form/

        Steps:
        1. Create an external VLAN network
        2. Make sure that the network is set to the management network with masquerade as the type
        3. Add another external VLAN management
        4. Create VM
        5. Wait until the VM boot in running state
        6. Delete the external VLAN from VM
        7. Check can ping the VM on the management network
        8. Check can&#39;t SSH to VM with management network from external host
        &#34;&#34;&#34;

        vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)

        # Check image exists
        code, data = api_client.images.get(image_opensuse.name)

        if code == 404:
            create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

        spec = api_client.vms.Spec(1, 2)
        spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

        # Add network data to trigger DHCP on multiple NICs
        spec.network_data += cloud_network_data

        unique_name = unique_name + &#34;-delete-vlan&#34;

        # Add image
        spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

        # Add external vlan network
        spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

        # Create VM
        code, data = api_client.vms.create(unique_name, spec)
        assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check have 2 NICs and wait until all ip address exists
        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
            if len(data[&#39;status&#39;][&#39;interfaces&#39;]) == 2:
                if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:
                    if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]:
                        break
                sleep(5)

        else:
            raise AssertionError(
                f&#34;Failed to get multiple IPs on VM: {unique_name}, exceed the given timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # get data from running VM and transfer to spec
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        spec = spec.from_dict(data)

        spec.networks = []
        spec.mgmt_network = True

        code, data = api_client.vms.update(unique_name, spec)
        assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

        code, data = api_client.vms.restart(unique_name)
        assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

        # Check VM start in running state
        check_vm_running(api_client, unique_name, wait_timeout)

        # Check until VM ip address exists
        check_vm_ip_exists(api_client, unique_name, wait_timeout)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        ip_addresses = []

        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_name)
            assert 200 == code, (f&#34;Failed to get specific vm status: {code}, {data}&#34;)
            if &#39;interfaces&#39; in data[&#39;status&#39;]:
                interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
                ip_addresses = []

                interfaces = data[&#39;status&#39;][&#39;interfaces&#39;]

                if len(interfaces) == 1 and &#39;ipAddress&#39; in interfaces[0]:
                    ip_addresses.append(interfaces_data[0][&#39;ipAddress&#39;])

                    if &#39;default&#39; in interfaces_data[0][&#39;name&#39;]:
                        break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Ping management ip address
        mgmt_ip = ip_addresses[0]

        ping_command = &#34;ping -c 50 {0}&#34;.format(mgmt_ip)

        with host_shell.login(vip) as sh:
            stdout, stderr = sh.exec_command(ping_command)

        assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
            f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
            f&#34;on management interface from Harvester node: {code}, {data}&#34;)

        # #Check should not SSH to management ip address from external host
        command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

        with pytest.raises(subprocess.CalledProcessError) as ex:
            subprocess.check_output(command, stderr=subprocess.STDOUT,
                                    shell=False, encoding=&#34;utf-8&#34;)

        # OpenSSH returns the return code of the program that was executed on
        # the remote, unless there was an error for SSH itself, in which case
        # it returns 255
        assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                            &#34; to VM on management interface&#34;
                                            f&#34; {mgmt_ip} from external host&#34;)

        # cleanup vm
        delete_vm(api_client, unique_name, wait_timeout)

    def ssh_client(self, client, dest_ip, username, password, command, timeout,
                   allow_agent=False, look_for_keys=False):
        client.connect(dest_ip, username=username, password=password,
                       allow_agent=allow_agent, look_for_keys=look_for_keys,
                       timeout=timeout)

        split_command = shlex.split(command)
        _stdin, _stdout, _stderr = client.exec_command(&#39; &#39;.join(
            shlex.quote(part) for part in split_command), get_pty=True)
        return _stdout, _stderr

    def ssh_jumpstart(self, client, dest_ip, client_ip, client_user, client_password,
                      dest_user, dest_password, command, allow_agent=False, look_for_keys=False):
        client.connect(client_ip, username=client_user, password=client_password,
                       allow_agent=allow_agent, look_for_keys=look_for_keys)

        client_transport = client.get_transport()
        dest_addr = (dest_ip, 22)
        client_addr = (client_ip, 22)
        client_channel = client_transport.open_channel(&#34;direct-tcpip&#34;, dest_addr, client_addr)

        jumpstart = paramiko.SSHClient()
        jumpstart.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        jumpstart.connect(dest_ip, username=dest_user, password=dest_password, sock=client_channel)

        split_command = shlex.split(command)
        _stdin, _stdout, _stderr = jumpstart.exec_command(&#39; &#39;.join(
            shlex.quote(part) for part in split_command))
        return _stdout, _stderr</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.ssh_client"><code class="name flex">
<span>def <span class="ident">ssh_client</span></span>(<span>self, client, dest_ip, username, password, command, timeout, allow_agent=False, look_for_keys=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ssh_client(self, client, dest_ip, username, password, command, timeout,
               allow_agent=False, look_for_keys=False):
    client.connect(dest_ip, username=username, password=password,
                   allow_agent=allow_agent, look_for_keys=look_for_keys,
                   timeout=timeout)

    split_command = shlex.split(command)
    _stdin, _stdout, _stderr = client.exec_command(&#39; &#39;.join(
        shlex.quote(part) for part in split_command), get_pty=True)
    return _stdout, _stderr</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.ssh_jumpstart"><code class="name flex">
<span>def <span class="ident">ssh_jumpstart</span></span>(<span>self, client, dest_ip, client_ip, client_user, client_password, dest_user, dest_password, command, allow_agent=False, look_for_keys=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ssh_jumpstart(self, client, dest_ip, client_ip, client_user, client_password,
                  dest_user, dest_password, command, allow_agent=False, look_for_keys=False):
    client.connect(client_ip, username=client_user, password=client_password,
                   allow_agent=allow_agent, look_for_keys=look_for_keys)

    client_transport = client.get_transport()
    dest_addr = (dest_ip, 22)
    client_addr = (client_ip, 22)
    client_channel = client_transport.open_channel(&#34;direct-tcpip&#34;, dest_addr, client_addr)

    jumpstart = paramiko.SSHClient()
    jumpstart.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    jumpstart.connect(dest_ip, username=dest_user, password=dest_password, sock=client_channel)

    split_command = shlex.split(command)
    _stdin, _stdout, _stderr = jumpstart.exec_command(&#39; &#39;.join(
        shlex.quote(part) for part in split_command))
    return _stdout, _stderr</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_delete_vlan_from_multiple"><code class="name flex">
<span>def <span class="ident">test_delete_vlan_from_multiple</span></span>(<span>self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout, host_shell)</span>
</code></dt>
<dd>
<div class="desc"><p>Manual test plan reference:
<a href="https://harvester.github.io/tests/manual/network/delete-vlan-network-form/">https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</a></p>
<p>Steps:
1. Create an external VLAN network
2. Make sure that the network is set to the management network with masquerade as the type
3. Add another external VLAN management
4. Create VM
5. Wait until the VM boot in running state
6. Delete the external VLAN from VM
7. Check can ping the VM on the management network
8. Check can't SSH to VM with management network from external host</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
def test_delete_vlan_from_multiple(
    self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout,
    host_shell
):
    &#34;&#34;&#34;
    Manual test plan reference:
    https://harvester.github.io/tests/manual/network/delete-vlan-network-form/

    Steps:
    1. Create an external VLAN network
    2. Make sure that the network is set to the management network with masquerade as the type
    3. Add another external VLAN management
    4. Create VM
    5. Wait until the VM boot in running state
    6. Delete the external VLAN from VM
    7. Check can ping the VM on the management network
    8. Check can&#39;t SSH to VM with management network from external host
    &#34;&#34;&#34;

    vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)

    # Check image exists
    code, data = api_client.images.get(image_opensuse.name)

    if code == 404:
        create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

    spec = api_client.vms.Spec(1, 2)
    spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

    # Add network data to trigger DHCP on multiple NICs
    spec.network_data += cloud_network_data

    unique_name = unique_name + &#34;-delete-vlan&#34;

    # Add image
    spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

    # Add external vlan network
    spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

    # Create VM
    code, data = api_client.vms.create(unique_name, spec)
    assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check have 2 NICs and wait until all ip address exists
    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        if len(data[&#39;status&#39;][&#39;interfaces&#39;]) == 2:
            if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:
                if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]:
                    break
            sleep(5)

    else:
        raise AssertionError(
            f&#34;Failed to get multiple IPs on VM: {unique_name}, exceed the given timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    # get data from running VM and transfer to spec
    code, data = api_client.vms.get(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
    spec = spec.from_dict(data)

    spec.networks = []
    spec.mgmt_network = True

    code, data = api_client.vms.update(unique_name, spec)
    assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

    code, data = api_client.vms.restart(unique_name)
    assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    ip_addresses = []

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm status: {code}, {data}&#34;)
        if &#39;interfaces&#39; in data[&#39;status&#39;]:
            interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
            ip_addresses = []

            interfaces = data[&#39;status&#39;][&#39;interfaces&#39;]

            if len(interfaces) == 1 and &#39;ipAddress&#39; in interfaces[0]:
                ip_addresses.append(interfaces_data[0][&#39;ipAddress&#39;])

                if &#39;default&#39; in interfaces_data[0][&#39;name&#39;]:
                    break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    # Ping management ip address
    mgmt_ip = ip_addresses[0]

    ping_command = &#34;ping -c 50 {0}&#34;.format(mgmt_ip)

    with host_shell.login(vip) as sh:
        stdout, stderr = sh.exec_command(ping_command)

    assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
        f&#34;on management interface from Harvester node: {code}, {data}&#34;)

    # #Check should not SSH to management ip address from external host
    command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

    with pytest.raises(subprocess.CalledProcessError) as ex:
        subprocess.check_output(command, stderr=subprocess.STDOUT,
                                shell=False, encoding=&#34;utf-8&#34;)

    # OpenSSH returns the return code of the program that was executed on
    # the remote, unless there was an error for SSH itself, in which case
    # it returns 255
    assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                        &#34; to VM on management interface&#34;
                                        f&#34; {mgmt_ip} from external host&#34;)

    # cleanup vm
    delete_vm(api_client, unique_name, wait_timeout)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_mgmt_network_connection"><code class="name flex">
<span>def <span class="ident">test_mgmt_network_connection</span></span>(<span>self, api_client, request, client, image_opensuse, unique_name, wait_timeout, host_shell, vm_shell_from_host)</span>
</code></dt>
<dd>
<div class="desc"><p>Manual test plan reference:
<a href="https://harvester.github.io/tests/manual/network/validate-network-management-network/">https://harvester.github.io/tests/manual/network/validate-network-management-network/</a></p>
<p>Steps:
1. Create a new VM
2. Make sure that the network is set to the management network with masquerade as the type
3. Wait until the VM boot in running state
4. Check can ping VM with management network from Harvester node
5. Check can SSH to VM with management network from Harvester node
6. Check can't SSH to VM with management network from external host</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
def test_mgmt_network_connection(
    self, api_client, request, client, image_opensuse, unique_name, wait_timeout,
    host_shell, vm_shell_from_host
):
    &#34;&#34;&#34;
    Manual test plan reference:
    https://harvester.github.io/tests/manual/network/validate-network-management-network/


    Steps:
    1. Create a new VM
    2. Make sure that the network is set to the management network with masquerade as the type
    3. Wait until the VM boot in running state
    4. Check can ping VM with management network from Harvester node
    5. Check can SSH to VM with management network from Harvester node
    6. Check can&#39;t SSH to VM with management network from external host
    &#34;&#34;&#34;
    vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)
    vm_user, vm_passwd = vm_credential[&#39;user&#39;], vm_credential[&#39;password&#39;]

    # Check image exists
    code, data = api_client.images.get(image_opensuse.name)

    if code == 404:
        create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

    # Update AllowTcpForwarding for ssh jumpstart

    spec = api_client.vms.Spec(1, 2)

    spec.user_data += cloud_user_data.format(password=vm_passwd)

    unique_name = unique_name + &#34;-mgmt&#34;
    # Create VM
    spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

    code, data = api_client.vms.create(unique_name, spec)
    assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # Get VM interface ipAddresses
    code, data = api_client.vms.get_status(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

    interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

    assert 1 == len(interfaces_data), (
        f&#34;Failed: get more than one interface: {interfaces_data}&#34;
    )

    mgmt_ip = interfaces_data[0][&#39;ipAddress&#39;]

    # Ping management ip address from Harvester node
    with host_shell.login(vip) as sh:
        stdout, stderr = sh.exec_command(f&#34;ping -c 50 {mgmt_ip}&#34;)

    assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
        f&#34;on management interface from Harvester node&#34;)

    # SSH to management ip address and execute command from Harvester node
    with vm_shell_from_host(vip, mgmt_ip, vm_user, vm_passwd) as sh:
        stdout, stderr = sh.exec_command(&#34;ls&#34;)

    assert stdout.find(&#34;bin&#34;) == 0, (
        f&#34;Failed to ssh to VM management IP {mgmt_ip} &#34;
        f&#34;on management interface from Harvester node&#34;)

    # Check should not SSH to management ip address from external host
    command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

    with pytest.raises(subprocess.CalledProcessError) as ex:
        subprocess.check_output(command, stderr=subprocess.STDOUT,
                                shell=False, encoding=&#34;utf-8&#34;)

    # OpenSSH returns the return code of the program that was executed on
    # the remote, unless there was an error for SSH itself, in which case
    # it returns 255
    assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                        &#34; to VM on management interface&#34;
                                        f&#34; {mgmt_ip} from external host&#34;)

    # cleanup VM
    delete_vm(api_client, unique_name, wait_timeout)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_mgmt_to_vlan_connection"><code class="name flex">
<span>def <span class="ident">test_mgmt_to_vlan_connection</span></span>(<span>self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Manual test plan reference:
<a href="https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/">https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</a></p>
<p>Steps:
1. Create an external VLAN network
2. Create a new VM
3. Make sure that the network is set to the management network with masquerade as the type
4. Wait until the VM boot in running state
5. Edit VM and change management network to external VLAN with bridge type
6. Check VM should save and reboot
7. Check can ping the VM from an external network host
8. Check can ssh to the VM from an external network host</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
def test_mgmt_to_vlan_connection(self, api_client, request, client, unique_name,
                                 image_opensuse, vm_network, wait_timeout):
    &#34;&#34;&#34;
    Manual test plan reference:
    https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/


    Steps:
    1. Create an external VLAN network
    2. Create a new VM
    3. Make sure that the network is set to the management network with masquerade as the type
    4. Wait until the VM boot in running state
    5. Edit VM and change management network to external VLAN with bridge type
    6. Check VM should save and reboot
    7. Check can ping the VM from an external network host
    8. Check can ssh to the VM from an external network host
    &#34;&#34;&#34;

    # Check image exists
    code, data = api_client.images.get(image_opensuse.name)

    if code == 404:
        create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

    spec = api_client.vms.Spec(1, 2)
    spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])
    unique_name = unique_name + &#34;-mgmt-vlan&#34;
    # Create VM
    spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)
    code, data = api_client.vms.create(unique_name, spec)
    assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # get data from running VM and transfer to spec
    code, data = api_client.vms.get(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

    spec = spec.from_dict(data)

    # Switch to vlan network
    spec.mgmt_network = False

    spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

    # Update VM spec
    code, data = api_client.vms.update(unique_name, spec)
    assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

    code, data = api_client.vms.restart(unique_name)
    assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # Get VM interface ipAddresses
    code, data = api_client.vms.get_status(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

    interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

    assert 1 == len(interfaces_data), (
        f&#34;Failed: get more than one interface: {interfaces_data}&#34;
    )

    # Determine by vlan network Name
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    ip_addresses = []

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        if &#39;interfaces&#39; in data[&#39;status&#39;]:
            interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
            ip_addresses = []
            for interface in interfaces_data:
                # Check the ipAddress in digital format
                if (
                    &#39;ipAddress&#39; in interface and
                    interface[&#39;ipAddress&#39;].replace(&#39;.&#39;, &#39;&#39;).isdigit()
                ):
                    ip_addresses.append(interface[&#39;ipAddress&#39;])

            if len(ip_addresses) &gt; 0:
                if &#39;nic-1&#39; in interface[&#39;name&#39;]:
                    break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to get VM {unique_name} IP address, exceed given timeout\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    # Ping vlan ip address
    vlan_ip = ip_addresses[0]

    command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;50&#39;, vlan_ip]

    result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

    assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
        f&#34;on vlan interface from external host&#34;)

    # SSH to vlan ip address and execute command
    _stdout, _stderr = self.ssh_client(
        client, vlan_ip, vm_credential[&#34;user&#34;], vm_credential[&#34;password&#34;], &#39;ls&#39;, wait_timeout)

    stdout = _stdout.read().decode(&#39;ascii&#39;).strip(&#34;\n&#34;)

    assert stdout.find(&#34;bin&#34;) == 0, (
        f&#34;Failed to ssh to VM external vlan IP {vlan_ip}&#34;
        f&#34;on vlan interface from external host&#34;)

    # cleanup vm
    delete_vm(api_client, unique_name, wait_timeout)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_reboot_vlan_connection"><code class="name flex">
<span>def <span class="ident">test_reboot_vlan_connection</span></span>(<span>self, api_client, request, unique_name, image_opensuse, vm_network, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Manual test plan reference:
<a href="https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/">https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</a></p>
<p>Steps:
1. Create an external VLAN network
2. Create a new VM and add the external vlan network
3. Check can ping external VLAN IP
4. Reboot VM
5. Ping VM during reboot
6. Check can't ping VM during reboot
7. Check the VM should reboot
8. Ping VM after reboot
9. Check can ping VM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
@pytest.mark.dependency(name=&#34;reboot_vlan_connection&#34;,
                        depends=[&#34;vlan_network_connection&#34;])
def test_reboot_vlan_connection(self, api_client, request, unique_name,
                                image_opensuse, vm_network, wait_timeout):
    &#34;&#34;&#34;
    Manual test plan reference:
    https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/


    Steps:
    1. Create an external VLAN network
    2. Create a new VM and add the external vlan network
    3. Check can ping external VLAN IP
    4. Reboot VM
    5. Ping VM during reboot
    6. Check can&#39;t ping VM during reboot
    7. Check the VM should reboot
    8. Ping VM after reboot
    9. Check can ping VM
    &#34;&#34;&#34;
    unique_name = unique_name + &#34;-reboot-vlan&#34;

    # Check image exists
    code, data = api_client.images.get(image_opensuse.name)

    if code == 404:
        create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

    spec = api_client.vms.Spec(1, 2, mgmt_network=False)
    spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

    # Create VM
    spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

    spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

    code, data = api_client.vms.create(unique_name, spec)
    assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # Get VM interface ipAddresses
    code, data = api_client.vms.get_status(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

    interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

    assert 1 == len(interfaces_data), (
        f&#34;Failed: get more than one interface: {interfaces_data}&#34;
    )

    assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
        f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
    )

    vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

    # Check can ping vlan ip

    command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

    result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

    assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
        f&#34;on vlan interface from external host&#34;)

    # Restart VM
    code, data = api_client.vms.restart(unique_name)
    assert 204 == code, (f&#34;Failed to reboot vm with error: {code}, {data}&#34;)

    # Check VM start in Starting state
    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
        vm_fields = data[&#39;metadata&#39;][&#39;fields&#39;]

        if vm_fields[2] == &#39;Starting&#39;:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to restart VM {unique_name} in Starting status, exceed given timeout\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    # Check can&#39;t ping vlan ip during reboot

    command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

    process = subprocess.run(command, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE, universal_newlines=True)

    result = process.stdout

    assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &lt; 0, (
        f&#34;Failed: since can ping VM external vlan IP {vlan_ip} &#34;
        f&#34;on vlan interface from external host during reboot&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # Get VM interface ipAddresses
    code, data = api_client.vms.get_status(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

    interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

    assert 1 == len(interfaces_data), (
        f&#34;Failed: get more than one interface: {interfaces_data}&#34;
    )

    assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
        f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
    )

    vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

    # Ping vlan ip address

    command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;10&#39;, vlan_ip]

    result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

    assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
        f&#34;on vlan interface from external host&#34;)

    # cleanup vm
    delete_vm(api_client, unique_name, wait_timeout)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_vlan_network_connection"><code class="name flex">
<span>def <span class="ident">test_vlan_network_connection</span></span>(<span>self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Manual test plan reference:
<a href="https://harvester.github.io/tests/manual/network/validate-network-external-vlan/">https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</a></p>
<p>Steps:
1. Create an external VLAN network
2. Create a new VM and set the external vlan network to it
3. Check can ping external VLAN IP from external host
4. Check can SSH to VM from external IP from external host</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
@pytest.mark.dependency(name=&#34;vlan_network_connection&#34;)
def test_vlan_network_connection(self, api_client, request, client, unique_name,
                                 image_opensuse, vm_network, wait_timeout):
    &#34;&#34;&#34;
    Manual test plan reference:
    https://harvester.github.io/tests/manual/network/validate-network-external-vlan/


    Steps:
    1. Create an external VLAN network
    2. Create a new VM and set the external vlan network to it
    3. Check can ping external VLAN IP from external host
    4. Check can SSH to VM from external IP from external host
    &#34;&#34;&#34;
    unique_name = unique_name + &#34;-vlan&#34;

    # Check image exists
    code, data = api_client.images.get(image_opensuse.name)

    if code == 404:
        create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

    spec = api_client.vms.Spec(1, 2, mgmt_network=False)
    spec.user_data += cloud_user_data.format(password=vm_credential[&#34;password&#34;])

    # Create VM
    spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)

    spec.add_network(&#34;nic-1&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

    code, data = api_client.vms.create(unique_name, spec)
    assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # Get VM interface ipAddresses
    code, data = api_client.vms.get_status(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)

    interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]

    assert 1 == len(interfaces_data), (
        f&#34;Failed: get more than one interface: {interfaces_data}&#34;
    )

    assert &#34;nic-1&#34; == interfaces_data[0][&#39;name&#39;], (
        f&#34;Failed: Network name did not match to added vlan: {interfaces_data}&#34;
    )

    vlan_ip = interfaces_data[0][&#39;ipAddress&#39;]

    # Ping vlan ip address from external host
    command = [&#39;/usr/bin/ping&#39;, &#39;-c&#39;, &#39;50&#39;, vlan_ip]

    result = subprocess.check_output(command, shell=False, encoding=&#34;utf-8&#34;)

    assert result.find(f&#34;64 bytes from {vlan_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM external vlan IP {vlan_ip} &#34;
        f&#34;on vlan interface from external host&#34;)

    # SSH to vlan ip address and execute command from external host
    _stdout, _stderr = self.ssh_client(
        client, vlan_ip, vm_credential[&#34;user&#34;], vm_credential[&#34;password&#34;], &#39;ls&#39;, wait_timeout)

    stdout = _stdout.read().decode(&#39;ascii&#39;).strip(&#34;\n&#34;)

    assert stdout.find(&#34;bin&#34;) == 0, (
        f&#34;Failed to ssh to VM external vlan IP {vlan_ip}&#34;
        f&#34;on vlan interface from external host&#34;)

    # cleanup vm
    delete_vm(api_client, unique_name, wait_timeout)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_vlan_to_mgmt_connection"><code class="name flex">
<span>def <span class="ident">test_vlan_to_mgmt_connection</span></span>(<span>self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout, host_shell, vm_shell_from_host)</span>
</code></dt>
<dd>
<div class="desc"><p>Manual test plan reference:
<a href="https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/">https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</a></p>
<p>Steps:
1. Create an external VLAN network
2. Create a new VM
3. Make sure that the network is set to the vlan network with bridge as the type
4. Wait until the VM boot in running state
5. Edit VM and change from external VLAN to management network
6. Check VM should save and reboot
7. Check can ping VM with management network from Harvester node
8. Check can SSH to VM with management network from Harvester node
9. Check can't SSH to VM with management network from external host</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.networks
def test_vlan_to_mgmt_connection(
    self, api_client, request, client, unique_name, image_opensuse, vm_network, wait_timeout,
    host_shell, vm_shell_from_host
):
    &#34;&#34;&#34;
    Manual test plan reference:
    https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/


    Steps:
    1. Create an external VLAN network
    2. Create a new VM
    3. Make sure that the network is set to the vlan network with bridge as the type
    4. Wait until the VM boot in running state
    5. Edit VM and change from external VLAN to management network
    6. Check VM should save and reboot
    7. Check can ping VM with management network from Harvester node
    8. Check can SSH to VM with management network from Harvester node
    9. Check can&#39;t SSH to VM with management network from external host
    &#34;&#34;&#34;

    vip = request.config.getoption(&#39;--endpoint&#39;).strip(&#39;https://&#39;)
    vm_user, vm_passwd = vm_credential[&#39;user&#39;], vm_credential[&#39;password&#39;]

    # Check image exists
    code, data = api_client.images.get(image_opensuse.name)

    if code == 404:
        create_image_url(api_client, image_opensuse.name, image_opensuse.url, wait_timeout)

    spec = api_client.vms.Spec(1, 2, mgmt_network=False)
    spec.user_data += cloud_user_data.format(password=vm_passwd)
    unique_name = unique_name + &#34;-vlan-mgmt&#34;

    # Create VM
    spec.add_image(image_opensuse.name, &#34;default/&#34; + image_opensuse.name)
    spec.add_network(&#34;default&#34;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)

    code, data = api_client.vms.create(unique_name, spec)
    assert 201 == code, (f&#34;Failed to create vm with error: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    # get data from running VM and transfer to spec
    code, data = api_client.vms.get(unique_name)
    assert 200 == code, (f&#34;Failed to get specific vm content: {code}, {data}&#34;)
    spec = spec.from_dict(data)

    spec.networks = []
    spec.mgmt_network = True

    code, data = api_client.vms.update(unique_name, spec)
    assert 200 == code, (f&#34;Failed to update specific vm with spec: {code}, {data}&#34;)

    code, data = api_client.vms.restart(unique_name)
    assert 204 == code, (f&#34;Failed to restart specific vm: {code}, {data}&#34;)

    # Check VM start in running state
    check_vm_running(api_client, unique_name, wait_timeout)

    # Check until VM ip address exists
    check_vm_ip_exists(api_client, unique_name, wait_timeout)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name)
        assert 200 == code, (f&#34;Failed to get specific vm status: {code}, {data}&#34;)

        if &#39;interfaces&#39; in data[&#39;status&#39;]:
            interfaces_data = data[&#39;status&#39;][&#39;interfaces&#39;]
            ip_addresses = []
            if &#39;ipAddress&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][0]:

                if &#39;default&#39; in interfaces_data[0][&#39;name&#39;]:
                    if &#39;domain, guest-agent&#39; not in interfaces_data[0][&#39;infoSource&#39;]:
                        ip_addresses.append(interfaces_data[0][&#39;ipAddress&#39;])
                        break
            sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to get VM {unique_name} IP address, exceed the given timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    # Check can ping management ip address from Harvester node
    mgmt_ip = ip_addresses[0]

    with host_shell.login(vip) as sh:
        stdout, stderr = sh.exec_command(f&#34;ping -c 50 {mgmt_ip}&#34;)

    assert stdout.find(f&#34;64 bytes from {mgmt_ip}&#34;) &gt; 0, (
        f&#34;Failed to ping VM management IP {mgmt_ip} &#34;
        f&#34;on management interface from Harvester node&#34;)

    # Check can ssh to host and execute command from Harvester node
    with vm_shell_from_host(vip, mgmt_ip, vm_user, vm_passwd) as sh:
        stdout, stderr = sh.exec_command(&#34;ls&#34;)

    assert stdout.find(&#34;bin&#34;) == 0, (
        f&#34;Failed to ssh to VM management IP {mgmt_ip} &#34;
        f&#34;on management interface from Harvester node&#34;)

    # Check should not SSH to management ip address from external host
    command = [&#39;/usr/bin/ssh&#39;, &#39;-o&#39;, &#39;ConnectTimeout=5&#39;, mgmt_ip]

    with pytest.raises(subprocess.CalledProcessError) as ex:
        subprocess.check_output(command, stderr=subprocess.STDOUT,
                                shell=False, encoding=&#34;utf-8&#34;)

    # OpenSSH returns the return code of the program that was executed on
    # the remote, unless there was an error for SSH itself, in which case
    # it returns 255
    assert ex.value.returncode == 255, (&#34;Failed: should not be able to SSH&#34;
                                        &#34; to VM on management interface&#34;
                                        f&#34; {mgmt_ip} from external host&#34;)

    # cleanup vm
    delete_vm(api_client, unique_name, wait_timeout)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.integrations" href="index.html">harvester_e2e_tests.integrations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.check_vm_ip_exists" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.check_vm_ip_exists">check_vm_ip_exists</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.check_vm_running" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.check_vm_running">check_vm_running</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.client" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.client">client</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.cluster_network" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.cluster_network">cluster_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.create_image_url" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.create_image_url">create_image_url</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.delete_vm" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.delete_vm">delete_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.vm_network" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.vm_network">vm_network</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork">TestBackendNetwork</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.pytestmark" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.ssh_client" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.ssh_client">ssh_client</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.ssh_jumpstart" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.ssh_jumpstart">ssh_jumpstart</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_delete_vlan_from_multiple" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_delete_vlan_from_multiple">test_delete_vlan_from_multiple</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_mgmt_network_connection" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_mgmt_network_connection">test_mgmt_network_connection</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_mgmt_to_vlan_connection" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_mgmt_to_vlan_connection">test_mgmt_to_vlan_connection</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_reboot_vlan_connection" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_reboot_vlan_connection">test_reboot_vlan_connection</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_vlan_network_connection" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_vlan_network_connection">test_vlan_network_connection</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_vlan_to_mgmt_connection" href="#harvester_e2e_tests.integrations.test_5_vm_networks_interact.TestBackendNetwork.test_vlan_to_mgmt_connection">test_vlan_to_mgmt_connection</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>