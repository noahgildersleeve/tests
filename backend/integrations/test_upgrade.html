<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>harvester_e2e_tests.integrations.test_upgrade API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.integrations.test_upgrade</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.cluster_network"><code class="name flex">
<span>def <span class="ident">cluster_network</span></span>(<span>vlan_nic, api_client, unique_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.cluster_state"><code class="name flex">
<span>def <span class="ident">cluster_state</span></span>(<span>request, unique_name, api_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.config_backup_target"><code class="name flex">
<span>def <span class="ident">config_backup_target</span></span>(<span>request, api_client, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.config_storageclass"><code class="name flex">
<span>def <span class="ident">config_storageclass</span></span>(<span>request, api_client, unique_name, cluster_state)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.harvester_crds"><code class="name flex">
<span>def <span class="ident">harvester_crds</span></span>(<span>api_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.image"><code class="name flex">
<span>def <span class="ident">image</span></span>(<span>api_client, image_ubuntu, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.interceptor"><code class="name flex">
<span>def <span class="ident">interceptor</span></span>(<span>api_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.stopped_vm"><code class="name flex">
<span>def <span class="ident">stopped_vm</span></span>(<span>request, api_client, ssh_keypair, wait_timeout, unique_name, image)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.upgrade_target"><code class="name flex">
<span>def <span class="ident">upgrade_target</span></span>(<span>request, unique_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.vm_network"><code class="name flex">
<span>def <span class="ident">vm_network</span></span>(<span>api_client, unique_name, wait_timeout, cluster_network, vlan_id, cluster_state)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade"><code class="flex name class">
<span>class <span class="ident">TestAnyNodesUpgrade</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.upgrade
@pytest.mark.any_nodes
class TestAnyNodesUpgrade:
    @pytest.mark.dependency(name=&#34;preq_setup_logging&#34;)
    def test_preq_setup_logging(self, api_client):
        # TODO: enable addon if &gt; v1.2.0
        return

    @pytest.mark.dependency(name=&#34;preq_setup_vmnetwork&#34;)
    def test_preq_setup_vmnetwork(self, vm_network):
        &#39;&#39;&#39; Be used to trigger the fixture to setup VM network &#39;&#39;&#39;

    @pytest.mark.dependency(name=&#34;preq_setup_storageclass&#34;)
    def test_preq_setup_storageclass(self, config_storageclass):
        &#34;&#34;&#34; Be used to trigger the fixture to setup storageclass&#34;&#34;&#34;

    @pytest.mark.dependency(name=&#34;preq_setup_vms&#34;)
    def test_preq_setup_vms(
        self, api_client, ssh_keypair, unique_name, vm_checker, vm_shell, vm_network, image,
        config_storageclass, config_backup_target, wait_timeout, cluster_state
    ):
        # create new storage class, make it default
        # create 3 VMs:
        # - having the new storage class
        # - the VM that have some data written, take backup
        # - the VM restored from the backup
        pub_key, pri_key = ssh_keypair
        old_sc, new_sc = config_storageclass
        unique_vm_name = f&#34;ug-vm-{unique_name}&#34;

        cpu, mem, size = 1, 2, 5
        vm_spec = api_client.vms.Spec(cpu, mem, mgmt_network=False)
        vm_spec.add_image(&#39;disk-0&#39;, image[&#39;id&#39;], size=size)
        vm_spec.add_network(&#39;nic-1&#39;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)
        userdata = yaml.safe_load(vm_spec.user_data)
        userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
        vm_spec.user_data = yaml.dump(userdata)

        code, data = api_client.vms.create(unique_vm_name, vm_spec)
        assert 201 == code, (code, data)
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(unique_vm_name, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;nic-1&#39;)
        # write data into VM
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(vm_ip, image[&#39;user&#39;], pkey=pri_key) as sh:
                    cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                    assert cloud_inited and not err, (out, err)
                    out, err = sh.exec_command(
                        &#34;dd if=/dev/urandom of=./generate_file bs=1M count=1024; sync&#34;
                    )
                    assert not out, (out, err)
                    vm1_md5, err = sh.exec_command(
                        &#34;md5sum ./generate_file &gt; ./generate_file.md5; cat ./generate_file.md5&#34;
                    )
                    assert not err, (vm1_md5, err)
                    break
            except (SSHException, NoValidConnectionsError, TimeoutError):
                sleep(5)
        else:
            raise AssertionError(&#34;Timed out while writing data into VM&#34;)

        # Take backup then check it&#39;s ready
        code, data = api_client.vms.backup(unique_vm_name, unique_vm_name)
        assert 204 == code, (code, data)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, backup = api_client.backups.get(unique_vm_name)
            if 200 == code and backup.get(&#39;status&#39;, {}).get(&#39;readyToUse&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#39;Timed-out waiting for the backup \&#39;{unique_vm_name}\&#39; to be ready.&#39;
            )
        # restore into new VM
        restored_vm_name = f&#34;r-{unique_vm_name}&#34;
        spec = api_client.backups.RestoreSpec.for_new(restored_vm_name)
        code, data = api_client.backups.restore(unique_vm_name, spec)
        assert 201 == code, (code, data)
        # Check restore VM is created
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(restored_vm_name)
            if 200 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;restored VM {restored_vm_name} is not created&#34;
            )
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(restored_vm_name, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM({restored_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        # Check data consistency
        r_vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                       if iface[&#39;name&#39;] == &#39;nic-1&#39;)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(r_vm_ip, image[&#39;user&#39;], pkey=pri_key) as sh:
                    cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                    assert cloud_inited and not err, (out, err)
                    out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                    assert not err, (out, err)
                    vm2_md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                    assert not err, (vm2_md5, err)
                    assert vm1_md5 == vm2_md5
                    out, err = sh.exec_command(
                        f&#34;ping -c1 {vm_ip} &gt; /dev/null &amp;&amp; echo -n success || echo -n fail&#34;
                    )
                    assert &#34;success&#34; == out and not err
                    break
            except (SSHException, NoValidConnectionsError, TimeoutError):
                sleep(5)
        else:
            raise AssertionError(&#34;Unable to login to restored VM to check data consistency&#34;)

        # Create VM having additional volume with new storage class
        vm_spec.add_volume(&#34;vol-1&#34;, 5, storage_cls=new_sc[&#39;metadata&#39;][&#39;name&#39;])
        code, data = api_client.vms.create(f&#34;sc-{unique_vm_name}&#34;, vm_spec)
        assert 201 == code, (code, data)
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(f&#34;sc-{unique_vm_name}&#34;, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM(sc-{unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

        # store into cluster&#39;s state
        names = [unique_vm_name, f&#34;r-{unique_vm_name}&#34;, f&#34;sc-{unique_vm_name}&#34;]
        cluster_state.vms = dict(md5=vm1_md5, names=names, ssh_user=image[&#39;user&#39;], pkey=pri_key)

    @pytest.mark.dependency(name=&#34;any_nodes_upgrade&#34;)
    def test_perform_upgrade(
        self, api_client, unique_name, upgrade_target, upgrade_timeout, interceptor
    ):
        &#34;&#34;&#34;
        - perform upgrade
        - check all nodes upgraded
        &#34;&#34;&#34;
        # Check nodes counts
        code, data = api_client.hosts.get()
        assert code == 200, (code, data)
        nodes = len(data[&#39;data&#39;])

        # create Upgrade version and start
        skip_version_check = {&#34;harvesterhci.io/skip-version-check&#34;: True}  # for test purpose
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;
        code, data = api_client.versions.create(version, url, checksum)
        assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
        code, data = api_client.upgrades.create(version, annotations=skip_version_check)
        assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;
        upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

        # Check upgrade status
        # TODO: check every upgrade stages
        endtime = datetime.now() + timedelta(seconds=upgrade_timeout * nodes)
        while endtime &gt; datetime.now():
            code, data = api_client.upgrades.get(upgrade_name)
            if 200 != code:
                continue
            interceptor.check(data)
            conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
            state = data.get(&#39;metadata&#39;, {}).get(&#39;labels&#39;, {}).get(&#39;harvesterhci.io/upgradeState&#39;)
            if &#34;Succeeded&#34; == state and &#34;True&#34; == conds.get(&#39;Completed&#39;, {}).get(&#39;status&#39;):
                break
            if any(&#34;False&#34; == c[&#39;status&#39;] for c in conds.values()):
                raise AssertionError(f&#34;Upgrade failed with conditions: {conds.values()}&#34;)
            sleep(30)
        else:
            raise AssertionError(
                f&#34;Upgrade timed out with conditions: {conds.values()}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_logging&#34;])
    def test_verify_logging_pods(self, api_client):
        &#34;&#34;&#34; Verify logging pods and logs
        Criteria: https://github.com/harvester/tests/issues/535
        &#34;&#34;&#34;

        code, pods = api_client.get_pods(namespace=&#34;cattle-logging-system&#34;)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, &#34;No logging pods found&#34;

        fails = []
        for pod in pods[&#39;data&#39;]:
            # Verify pod is running or completed
            phase = pod[&#34;status&#34;][&#34;phase&#34;]
            if phase not in (&#34;Running&#34;, &#34;Succeeded&#34;):
                fails.append((pod[&#39;metadata&#39;][&#39;name&#39;], phase))
        else:
            assert not fails, (
                &#34;\n&#34;.join(f&#34;Pod({n})&#39;s phase({p}) is not expected.&#34; for n, p in fails)
            )

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_audit_log(self, api_client, host_shell, wait_timeout):
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        label_main = &#34;node-role.kubernetes.io/control-plane&#34;
        masters = [n for n in data[&#39;data&#39;] if n[&#39;metadata&#39;][&#39;labels&#39;].get(label_main) == &#34;true&#34;]
        assert len(masters) &gt; 0, &#34;No master nodes found&#34;

        script = (&#34;sudo tail /var/lib/rancher/rke2/server/logs/audit.log | awk &#39;END{print}&#39; &#34;
                  &#34;| jq .requestReceivedTimestamp &#34;
                  &#34;| xargs -I {} date -d \&#34;{}\&#34; +%s&#34;)

        node_ips = [n[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION] for n in masters]
        cmp = dict()
        done = set()
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            for ip in done.symmetric_difference(node_ips):
                try:
                    with host_shell.login(ip) as shell:
                        out, err = shell.exec_command(script)
                        timestamp = int(out)
                        if not err and ip not in cmp:
                            cmp[ip] = timestamp
                            continue
                        if not err and cmp[ip] &lt; timestamp:
                            done.add(ip)
                except (SSHException, NoValidConnectionsError, TimeoutError):
                    continue

            if not done.symmetric_difference(node_ips):
                break
            sleep(5)
        else:
            raise AssertionError(
                &#34;\n&#34;.join(&#34;Node {ip} audit log is not updated.&#34; for ip in set(node_ips) ^ done)
            )

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vmnetwork&#34;])
    def test_verify_network(self, api_client, cluster_state):
        &#34;&#34;&#34; Verify cluster and VLAN networks
        - cluster network `mgmt` should exists
        - Created VLAN should exists
        &#34;&#34;&#34;

        code, cnets = api_client.clusternetworks.get()
        assert code == 200, (
            &#34;Failed to get Networks: %d, %s&#34; % (code, cnets))

        assert len(cnets[&#34;items&#34;]) &gt; 0, (&#34;No Networks found&#34;)

        assert any(n[&#39;metadata&#39;][&#39;name&#39;] == &#34;mgmt&#34; for n in cnets[&#39;items&#39;]), (
            &#34;Cluster network mgmt not found&#34;)

        code, vnets = api_client.networks.get()
        assert code == 200, (f&#34;Failed to get VLANs: {code}, {vnets}&#34; % (code, vnets))
        assert len(vnets[&#34;items&#34;]) &gt; 0, (&#34;No VLANs found&#34;)

        used_vlan = cluster_state.network[&#39;metadata&#39;][&#39;name&#39;]
        assert any(used_vlan == n[&#39;metadata&#39;][&#39;name&#39;] for n in vnets[&#39;items&#39;]), (
            f&#34;VLAN {used_vlan} not found&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vms&#34;])
    def test_verify_vms(self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout):
        &#34;&#34;&#34; Verify VMs&#39; state and data
        Criteria:
        - VMs should keep in running state
        - data in VMs should not lost
        &#34;&#34;&#34;

        code, vmis = api_client.vms.get_status()
        assert code == 200 and len(vmis[&#39;data&#39;]), (code, vmis)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            fails, ips = list(), dict()
            for name in cluster_state.vms[&#39;names&#39;]:
                code, data = api_client.vms.get_status(name)
                try:
                    assert 200 == code
                    assert &#34;Running&#34; == data[&#39;status&#39;][&#39;phase&#39;]
                    assert data[&#39;status&#39;][&#39;nodeName&#39;]
                    ips[name] = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                                     if iface[&#39;name&#39;] == &#39;nic-1&#39;)
                except (AssertionError, TypeError, StopIteration, KeyError) as ex:
                    fails.append((name, (ex, code, data)))
            if not fails:
                break
        else:
            raise AssertionError(&#34;\n&#34;.join(
                f&#34;VM {name} is not in expected state.\nException: {ex}\nAPI Status({code}): {data}&#34;
                for (name, (ex, code, data)) in fails)
            )

        pri_key, ssh_user = cluster_state.vms[&#39;pkey&#39;], cluster_state.vms[&#39;ssh_user&#39;]
        for name in cluster_state.vms[&#39;names&#39;][:-1]:
            vm_ip = ips[name]
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    with vm_shell.login(vm_ip, ssh_user, pkey=pri_key) as sh:
                        out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                        assert not err, (out, err)
                        md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                        assert not err, (md5, err)
                        assert md5 == cluster_state.vms[&#39;md5&#39;]
                        break
                except (SSHException, NoValidConnectionsError, TimeoutError):
                    sleep(5)
            else:
                fails.append(f&#34;Data in VM({name}, {vm_ip}) is inconsistent.&#34;)

        assert not fails, &#34;\n&#34;.join(fails)

        # Teardown: remove all VMs
        for name in cluster_state.vms[&#39;names&#39;]:
            code, data = api_client.vms.get(name)
            spec = api_client.vms.Spec.from_dict(data)
            _ = vm_checker.wait_deleted(name)
            for vol in spec.volumes:
                vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
                api_client.volumes.delete(vol_name)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vms&#34;])
    def test_verify_restore_vm(
        self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout
    ):
        &#34;&#34;&#34; Verify VM restored from the backup
        Criteria:
        - VM should able to start
        - data in VM should not lost
        &#34;&#34;&#34;

        backup_name = cluster_state.vms[&#39;names&#39;][0]
        restored_vm_name = f&#34;new-r-{backup_name}&#34;

        # Restore VM from backup and check networking is good
        restore_spec = api_client.backups.RestoreSpec.for_new(restored_vm_name)
        code, data = api_client.backups.restore(backup_name, restore_spec)
        assert code == 201, &#34;Unable to restore backup {backup_name} after upgrade&#34;
        # Check restore VM is created
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(restored_vm_name)
            if 200 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;restored VM {restored_vm_name} is not created&#34;
            )
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(restored_vm_name, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM({restored_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

        # Check data in restored VM is consistent
        pri_key, ssh_user = cluster_state.vms[&#39;pkey&#39;], cluster_state.vms[&#39;ssh_user&#39;]
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;nic-1&#39;)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(vm_ip, ssh_user, pkey=pri_key) as sh:
                    cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                    assert cloud_inited and not err, (out, err)
                    out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                    assert not err, (out, err)
                    md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                    assert not err, (md5, err)
                    assert md5 == cluster_state.vms[&#39;md5&#39;]
                    break
            except (SSHException, NoValidConnectionsError, TimeoutError):
                sleep(5)
        else:
            raise AssertionError(&#34;Unable to login to restored VM to check data consistency&#34;)

        # teardown: remove the VM
        code, data = api_client.vms.get(restored_vm_name)
        spec = api_client.vms.Spec.from_dict(data)
        _ = vm_checker.wait_deleted(restored_vm_name)
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_storageclass&#34;])
    def test_verify_storage_class(self, api_client, cluster_state):
        &#34;&#34;&#34; Verify StorageClasses and defaults
        - `new_sc` should be settle as default
        - `longhorn` should exists
        &#34;&#34;&#34;
        code, scs = api_client.scs.get()
        assert code == 200, (&#34;Failed to get StorageClasses: %d, %s&#34; % (code, scs))
        assert len(scs[&#34;items&#34;]) &gt; 0, (&#34;No StorageClasses found&#34;)

        created_sc = cluster_state.scs[-1][&#39;metadata&#39;][&#39;name&#39;]
        names = {sc[&#39;metadata&#39;][&#39;name&#39;]: sc[&#39;metadata&#39;].get(&#39;annotations&#39;) for sc in scs[&#39;items&#39;]}
        assert &#34;longhorn&#34; in names
        assert created_sc in names
        assert &#34;storageclass.kubernetes.io/is-default-class&#34; in names[created_sc]
        assert &#34;true&#34; == names[created_sc][&#34;storageclass.kubernetes.io/is-default-class&#34;]

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_os_version(self, request, api_client, cluster_state, host_shell):
        # Verify /etc/os-release on all nodes
        script = &#34;cat /etc/os-release&#34;
        if not cluster_state.version_verify:
            pytest.skip(&#34;skip verify os version&#34;)

        # Get all nodes
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        for node in data[&#39;data&#39;]:
            node_ip = node[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION]

            with host_shell.login(node_ip) as sh:
                lines, stderr = sh.exec_command(script, get_pty=True, splitlines=True)
                assert not stderr, (
                    f&#34;Failed to execute {script} on {node_ip}: {stderr}&#34;)

                # eg: PRETTY_NAME=&#34;Harvester v1.1.0&#34;
                assert cluster_state.version == re.findall(r&#34;Harvester (.+?)\&#34;&#34;, lines[3])[0], (
                    &#34;OS version is not correct&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_rke2_version(self, api_client, host_shell):
        # Verify node version on all nodes
        script = &#34;cat /etc/harvester-release.yaml&#34;

        label_main = &#34;node-role.kubernetes.io/control-plane&#34;
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        masters = [n for n in data[&#39;data&#39;] if n[&#39;metadata&#39;][&#39;labels&#39;].get(label_main) == &#34;true&#34;]

        # Verify rke2 version
        except_rke2_version = &#34;&#34;
        for node in masters:
            node_ip = node[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION]

            # Get except rke2 version
            if except_rke2_version == &#34;&#34;:
                with host_shell.login(node_ip) as sh:
                    lines, stderr = sh.exec_command(script, get_pty=True, splitlines=True)
                    assert not stderr, (
                        f&#34;Failed to execute {script} on {node_ip}: {stderr}&#34;)

                    for line in lines:
                        if &#34;kubernetes&#34; in line:
                            except_rke2_version = re.findall(r&#34;kubernetes: (.*)&#34;, line.strip())[0]
                            break

                    assert except_rke2_version != &#34;&#34;, (&#34;Failed to get except rke2 version&#34;)

            assert node.get(&#39;status&#39;, {}).get(&#39;nodeInfo&#39;, {}).get(
                   &#34;kubeletVersion&#34;, &#34;&#34;) == except_rke2_version, (
                   &#34;rke2 version is not correct&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_deployed_components_version(self, api_client):
        &#34;&#34;&#34; Verify deployed kubevirt and longhorn version
        Criteria:
        - except version(get from apps.catalog.cattle.io/harvester) should be equal to the version
          of kubevirt and longhorn
        &#34;&#34;&#34;

        kubevirt_version_existed = False
        engine_image_version_existed = False
        longhorn_manager_version_existed = False

        # Get except version from apps.catalog.cattle.io/harvester
        code, apps = api_client.get_apps_catalog(name=&#34;harvester&#34;,
                                                 namespace=DEFAULT_HARVESTER_NAMESPACE)
        assert code == 200 and apps[&#39;type&#39;] != &#34;error&#34;, (
            f&#34;Failed to get apps.catalog.cattle.io/harvester: {apps[&#39;message&#39;]}&#34;)

        # Get except image of kubevirt and longhorn
        kubevirt_operator = (
            apps[&#39;spec&#39;][&#39;chart&#39;][&#39;values&#39;][&#39;kubevirt-operator&#39;][&#39;containers&#39;][&#39;operator&#39;])
        kubevirt_operator_image = (
            f&#34;{kubevirt_operator[&#39;image&#39;][&#39;repository&#39;]}:{kubevirt_operator[&#39;image&#39;][&#39;tag&#39;]}&#34;)

        longhorn = apps[&#39;spec&#39;][&#39;chart&#39;][&#39;values&#39;][&#39;longhorn&#39;][&#39;image&#39;][&#39;longhorn&#39;]
        longhorn_images = {
            &#34;engine-image&#34;: f&#34;{longhorn[&#39;engine&#39;][&#39;repository&#39;]}:{longhorn[&#39;engine&#39;][&#39;tag&#39;]}&#34;,
            &#34;longhorn-manager&#34;: f&#34;{longhorn[&#39;manager&#39;][&#39;repository&#39;]}:{longhorn[&#39;manager&#39;][&#39;tag&#39;]}&#34;
        }

        # Verify kubevirt version
        code, pods = api_client.get_pods(namespace=DEFAULT_HARVESTER_NAMESPACE)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, (
            f&#34;Failed to get pods in namespace {DEFAULT_HARVESTER_NAMESPACE}&#34;)

        for pod in pods[&#39;data&#39;]:
            if &#34;virt-operator&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
                kubevirt_version_existed = (
                    kubevirt_operator_image == pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;])

        # Verify longhorn version
        code, pods = api_client.get_pods(namespace=DEFAULT_LONGHORN_NAMESPACE)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, (
            f&#34;Failed to get pods in namespace {DEFAULT_LONGHORN_NAMESPACE}&#34;)

        for pod in pods[&#39;data&#39;]:
            if &#34;longhorn-manager&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
                longhorn_manager_version_existed = (
                  longhorn_images[&#34;longhorn-manager&#34;] == pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;])
            elif &#34;engine-image&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
                engine_image_version_existed = (
                    longhorn_images[&#34;engine-image&#34;] == pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;])

        assert kubevirt_version_existed, &#34;kubevirt version is not correct&#34;
        assert engine_image_version_existed, &#34;longhorn engine image version is not correct&#34;
        assert longhorn_manager_version_existed, &#34;longhorn manager version is not correct&#34;

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_crds_existed(self, api_client, harvester_crds):
        &#34;&#34;&#34; Verify crds existed
        Criteria:
        - crds should be existed
        &#34;&#34;&#34;
        not_existed_crds = []
        exist_crds = True
        for crd in harvester_crds:
            code, _ = api_client.get_crds(name=crd)

            if code != 200:
                exist_crds = False
                not_existed_crds.append(crd)

        if not exist_crds:
            raise AssertionError(f&#34;CRDs {not_existed_crds} are not existed&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_upgrade_vm_deleted(self, api_client, wait_timeout):
        # max to wait 300s for the upgrade related VMs to be deleted
        endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(namespace=&#39;harvester-system&#39;)
            upgrade_vms = [vm for vm in data[&#39;data&#39;] if &#39;upgrade&#39; in vm[&#39;id&#39;]]
            if not upgrade_vms:
                break
        else:
            raise AssertionError(f&#34;Upgrade related VM still available:\n{upgrade_vms}&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_upgrade_volume_deleted(self, api_client, wait_timeout):
        # max to wait 300s for the upgrade related volumes to be deleted
        endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
        while endtime &gt; datetime.now():
            code, data = api_client.volumes.get(namespace=&#39;harvester-system&#39;)
            upgrade_vols = [vol for vol in data[&#39;data&#39;]
                            if &#39;upgrade&#39; in vol[&#39;id&#39;] and not vol[&#39;id&#39;].endswith(&#39;log-archive&#39;)]
            if not upgrade_vols:
                break
        else:
            raise AssertionError(f&#34;Upgrade related volume(s) still available:\n{upgrade_vols}&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_upgrade_image_deleted(self, api_client, wait_timeout):
        # max to wait 300s for the upgrade related volumes to be deleted
        endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
        while endtime &gt; datetime.now():
            code, data = api_client.images.get(namespace=&#39;harvester-system&#39;)
            upgrade_images = [image for image in data[&#39;items&#39;]
                              if &#39;upgrade&#39; in image[&#39;spec&#39;][&#39;displayName&#39;]]
            if not upgrade_images:
                break
        else:
            raise AssertionError(f&#34;Upgrade related image(s) still available:\n{upgrade_images}&#34;)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_perform_upgrade"><code class="name flex">
<span>def <span class="ident">test_perform_upgrade</span></span>(<span>self, api_client, unique_name, upgrade_target, upgrade_timeout, interceptor)</span>
</code></dt>
<dd>
<div class="desc"><ul>
<li>perform upgrade</li>
<li>check all nodes upgraded</li>
</ul></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_logging"><code class="name flex">
<span>def <span class="ident">test_preq_setup_logging</span></span>(<span>self, api_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_storageclass"><code class="name flex">
<span>def <span class="ident">test_preq_setup_storageclass</span></span>(<span>self, config_storageclass)</span>
</code></dt>
<dd>
<div class="desc"><p>Be used to trigger the fixture to setup storageclass</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vmnetwork"><code class="name flex">
<span>def <span class="ident">test_preq_setup_vmnetwork</span></span>(<span>self, vm_network)</span>
</code></dt>
<dd>
<div class="desc"><p>Be used to trigger the fixture to setup VM network</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vms"><code class="name flex">
<span>def <span class="ident">test_preq_setup_vms</span></span>(<span>self, api_client, ssh_keypair, unique_name, vm_checker, vm_shell, vm_network, image, config_storageclass, config_backup_target, wait_timeout, cluster_state)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_image_deleted"><code class="name flex">
<span>def <span class="ident">test_upgrade_image_deleted</span></span>(<span>self, api_client, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_vm_deleted"><code class="name flex">
<span>def <span class="ident">test_upgrade_vm_deleted</span></span>(<span>self, api_client, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_volume_deleted"><code class="name flex">
<span>def <span class="ident">test_upgrade_volume_deleted</span></span>(<span>self, api_client, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_audit_log"><code class="name flex">
<span>def <span class="ident">test_verify_audit_log</span></span>(<span>self, api_client, host_shell, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_crds_existed"><code class="name flex">
<span>def <span class="ident">test_verify_crds_existed</span></span>(<span>self, api_client, harvester_crds)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify crds existed
Criteria:
- crds should be existed</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_deployed_components_version"><code class="name flex">
<span>def <span class="ident">test_verify_deployed_components_version</span></span>(<span>self, api_client)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify deployed kubevirt and longhorn version
Criteria:
- except version(get from apps.catalog.cattle.io/harvester) should be equal to the version
of kubevirt and longhorn</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_logging_pods"><code class="name flex">
<span>def <span class="ident">test_verify_logging_pods</span></span>(<span>self, api_client)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify logging pods and logs
Criteria: <a href="https://github.com/harvester/tests/issues/535">https://github.com/harvester/tests/issues/535</a></p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_network"><code class="name flex">
<span>def <span class="ident">test_verify_network</span></span>(<span>self, api_client, cluster_state)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify cluster and VLAN networks
- cluster network <code>mgmt</code> should exists
- Created VLAN should exists</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_os_version"><code class="name flex">
<span>def <span class="ident">test_verify_os_version</span></span>(<span>self, request, api_client, cluster_state, host_shell)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_restore_vm"><code class="name flex">
<span>def <span class="ident">test_verify_restore_vm</span></span>(<span>self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify VM restored from the backup
Criteria:
- VM should able to start
- data in VM should not lost</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_rke2_version"><code class="name flex">
<span>def <span class="ident">test_verify_rke2_version</span></span>(<span>self, api_client, host_shell)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_storage_class"><code class="name flex">
<span>def <span class="ident">test_verify_storage_class</span></span>(<span>self, api_client, cluster_state)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify StorageClasses and defaults
- <code>new_sc</code> should be settle as default
- <code>longhorn</code> should exists</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_vms"><code class="name flex">
<span>def <span class="ident">test_verify_vms</span></span>(<span>self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Verify VMs' state and data
Criteria:
- VMs should keep in running state
- data in VMs should not lost</p></div>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade"><code class="flex name class">
<span>class <span class="ident">TestInvalidUpgrade</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.upgrade
@pytest.mark.negative
@pytest.mark.any_nodes
class TestInvalidUpgrade:
    def test_iso_url(self, api_client, unique_name, upgrade_timeout):
        &#34;&#34;&#34;
        Steps:
        1. Create an invalid manifest.
        2. Try to upgrade with the invalid manifest.
        3. Upgrade should not start and fail.
        &#34;&#34;&#34;
        version, url = unique_name, &#34;https://invalid_iso_url&#34;
        checksum = sha512(b&#39;not_a_valid_checksum&#39;).hexdigest()

        code, data = api_client.versions.get(version)
        if code != 200:
            code, data = api_client.versions.create(version, url, checksum)
            assert code == 201, f&#34;Failed to create invalid version: {data}&#34;

        code, data = api_client.upgrades.create(version)
        assert code == 201, f&#34;Failed to create invalid upgrade: {data}&#34;

        endtime = datetime.now() + timedelta(seconds=upgrade_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.upgrades.get(data[&#39;metadata&#39;][&#39;name&#39;])
            conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
            verified = [
                &#34;False&#34; == conds.get(&#39;Completed&#39;, {}).get(&#39;status&#39;),
                &#34;False&#34; == conds.get(&#39;ImageReady&#39;, {}).get(&#39;status&#39;),
                &#34;retry limit&#34; in conds.get(&#39;ImageReady&#39;, {}).get(&#39;message&#39;, &#34;&#34;)
            ]
            if all(verified):
                break
        else:
            raise AssertionError(f&#34;Upgrade NOT failed in expected conditions: {conds}&#34;)

        # teardown
        api_client.upgrades.delete(data[&#39;metadata&#39;][&#39;name&#39;])
        api_client.versions.delete(version)

    @pytest.mark.parametrize(
        &#34;resort&#34;, [slice(None, None, -1), slice(None, None, 2)], ids=(&#34;mismatched&#34;, &#34;invalid&#34;)
    )
    def test_checksum(self, api_client, unique_name, upgrade_target, upgrade_timeout, resort):
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;

        if resort.step == 2:
            # ref: https://github.com/harvester/harvester/issues/5480
            code, data = api_client.versions.create(version, url, checksum[resort])
            try:
                assert 400 == code, (code, data)
            finally:
                return api_client.versions.delete(version)

        code, data = api_client.versions.create(version, url, checksum[resort])
        assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
        code, data = api_client.upgrades.create(version)
        assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;

        endtime = datetime.now() + timedelta(seconds=upgrade_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.upgrades.get(data[&#39;metadata&#39;][&#39;name&#39;])
            conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
            verified = [
                &#34;False&#34; == conds.get(&#39;Completed&#39;, {}).get(&#39;status&#39;),
                &#34;False&#34; == conds.get(&#39;ImageReady&#39;, {}).get(&#39;status&#39;),
                &#34;n&#39;t match the file actual check&#34; in conds.get(&#39;ImageReady&#39;, {}).get(&#39;message&#39;, &#34;&#34;)
            ]
            if all(verified):
                break
        else:
            raise AssertionError(f&#34;Upgrade NOT failed in expected conditions: {conds}&#34;)

        # teardown
        api_client.upgrades.delete(data[&#39;metadata&#39;][&#39;name&#39;])
        api_client.versions.delete(version)

    @pytest.mark.skip(&#34;https://github.com/harvester/harvester/issues/5494&#34;)
    def test_version_compatibility(
        self, api_client, unique_name, upgrade_target, upgrade_timeout
    ):
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;

        code, data = api_client.versions.create(version, url, checksum)
        assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
        code, data = api_client.upgrades.create(version)
        assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;

        endtime = datetime.now() + timedelta(seconds=upgrade_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.upgrades.get(data[&#39;metadata&#39;][&#39;name&#39;])
            conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
            verified = []  # TODO
            if all(verified):
                break
        else:
            raise AssertionError(f&#34;Upgrade NOT failed in expected conditions: {conds}&#34;)

        # teardown
        api_client.upgrades.delete(data[&#39;metadata&#39;][&#39;name&#39;])
        api_client.versions.delete(version)

    def test_degraded_volume(
        self, api_client, wait_timeout, vm_shell_from_host, vm_checker, upgrade_target, stopped_vm
    ):
        &#34;&#34;&#34;
        Criteria: create upgrade should fails if there are any degraded volumes
        Steps:
        1. Create a VM using a volume with 3 replicas.
        2. Delete one replica of the volume. Let the volume stay in
           degraded state.
        3. Immediately upgrade Harvester.
        4. Upgrade should fail.
        &#34;&#34;&#34;
        vm_name, ssh_user, pri_key = stopped_vm
        vm_started, (code, vmi) = vm_checker.wait_started(vm_name)
        assert vm_started, (code, vmi)

        # Write date into VM
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in vmi[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(vmi[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)
        with vm_shell_from_host(host_ip, vm_ip, ssh_user, pkey=pri_key) as sh:
            stdout, stderr = sh.exec_command(
                &#34;dd if=/dev/urandom of=./generate_file bs=1M count=1024; sync&#34;
            )
            assert not stdout, (stdout, stderr)

        # Get pv name of the volume
        claim_name = vmi[&#34;spec&#34;][&#34;volumes&#34;][0][&#34;persistentVolumeClaim&#34;][&#34;claimName&#34;]
        code, data = api_client.volumes.get(name=claim_name)
        assert code == 200, f&#34;Failed to get volume {claim_name}: {data}&#34;
        pv_name = data[&#34;spec&#34;][&#34;volumeName&#34;]

        # Make the volume becomes degraded
        code, data = api_client.lhreplicas.get()
        assert code == 200 and data[&#39;items&#39;], f&#34;Failed to get longhorn replicas ({code}): {data}&#34;
        replica = next(r for r in data[&#34;items&#34;] if pv_name == r[&#39;spec&#39;][&#39;volumeName&#39;])
        api_client.lhreplicas.delete(name=replica[&#39;metadata&#39;][&#39;name&#39;])
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.lhvolumes.get(pv_name)
            if 200 == code and &#34;degraded&#34; == data[&#39;status&#39;][&#39;robustness&#39;]:
                break
        else:
            raise AssertionError(
                f&#34;Unable to make the Volume {pv_name} degraded\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # create upgrade and verify it is not allowed
        version, url, checksum = upgrade_target
        code, data = api_client.versions.create(version, url, checksum)
        assert code == 201, f&#34;Failed to create version {version}: {data}&#34;
        code, data = api_client.upgrades.create(version)
        assert code == 400, f&#34;Failed to verify degraded volume: {code}, {data}&#34;

        # Teardown invalid upgrade
        api_client.versions.delete(version)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_checksum"><code class="name flex">
<span>def <span class="ident">test_checksum</span></span>(<span>self, api_client, unique_name, upgrade_target, upgrade_timeout, resort)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_degraded_volume"><code class="name flex">
<span>def <span class="ident">test_degraded_volume</span></span>(<span>self, api_client, wait_timeout, vm_shell_from_host, vm_checker, upgrade_target, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>Criteria: create upgrade should fails if there are any degraded volumes
Steps:
1. Create a VM using a volume with 3 replicas.
2. Delete one replica of the volume. Let the volume stay in
degraded state.
3. Immediately upgrade Harvester.
4. Upgrade should fail.</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_iso_url"><code class="name flex">
<span>def <span class="ident">test_iso_url</span></span>(<span>self, api_client, unique_name, upgrade_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Steps:
1. Create an invalid manifest.
2. Try to upgrade with the invalid manifest.
3. Upgrade should not start and fail.</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_version_compatibility"><code class="name flex">
<span>def <span class="ident">test_version_compatibility</span></span>(<span>self, api_client, unique_name, upgrade_target, upgrade_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.integrations" href="index.html">harvester_e2e_tests.integrations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.cluster_network" href="#harvester_e2e_tests.integrations.test_upgrade.cluster_network">cluster_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.cluster_state" href="#harvester_e2e_tests.integrations.test_upgrade.cluster_state">cluster_state</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.config_backup_target" href="#harvester_e2e_tests.integrations.test_upgrade.config_backup_target">config_backup_target</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.config_storageclass" href="#harvester_e2e_tests.integrations.test_upgrade.config_storageclass">config_storageclass</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.harvester_crds" href="#harvester_e2e_tests.integrations.test_upgrade.harvester_crds">harvester_crds</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.image" href="#harvester_e2e_tests.integrations.test_upgrade.image">image</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.interceptor" href="#harvester_e2e_tests.integrations.test_upgrade.interceptor">interceptor</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.stopped_vm" href="#harvester_e2e_tests.integrations.test_upgrade.stopped_vm">stopped_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.upgrade_target" href="#harvester_e2e_tests.integrations.test_upgrade.upgrade_target">upgrade_target</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.vm_network" href="#harvester_e2e_tests.integrations.test_upgrade.vm_network">vm_network</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade">TestAnyNodesUpgrade</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.pytestmark" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_perform_upgrade" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_perform_upgrade">test_perform_upgrade</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_logging" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_logging">test_preq_setup_logging</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_storageclass" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_storageclass">test_preq_setup_storageclass</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vmnetwork" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vmnetwork">test_preq_setup_vmnetwork</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vms" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vms">test_preq_setup_vms</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_image_deleted" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_image_deleted">test_upgrade_image_deleted</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_vm_deleted" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_vm_deleted">test_upgrade_vm_deleted</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_volume_deleted" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_volume_deleted">test_upgrade_volume_deleted</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_audit_log" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_audit_log">test_verify_audit_log</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_crds_existed" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_crds_existed">test_verify_crds_existed</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_deployed_components_version" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_deployed_components_version">test_verify_deployed_components_version</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_logging_pods" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_logging_pods">test_verify_logging_pods</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_network" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_network">test_verify_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_os_version" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_os_version">test_verify_os_version</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_restore_vm" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_restore_vm">test_verify_restore_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_rke2_version" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_rke2_version">test_verify_rke2_version</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_storage_class" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_storage_class">test_verify_storage_class</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_vms" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_vms">test_verify_vms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade">TestInvalidUpgrade</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.pytestmark" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_checksum" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_checksum">test_checksum</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_degraded_volume" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_degraded_volume">test_degraded_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_iso_url" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_iso_url">test_iso_url</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_version_compatibility" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_version_compatibility">test_version_compatibility</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
