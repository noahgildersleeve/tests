<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Harvester Test Cases on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/</link>
    <description>Recent content in Harvester Test Cases on Harvester manual test cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://harvester.github.io/tests/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/deployment/1218-http-proxy-setting-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/1218-http-proxy-setting-harvester/</guid>
      <description>Related issue: #1218 Missing http proxy settings on rke2 and rancher pod Environment setup Setup an airgapped harvester&#xA;Clone ipxe example repository https://github.com/harvester/ipxe-examples Edit the setting.xml file under vagrant ipxe example Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Verification Steps Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt; Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Create a virtual machine Prepare an S3 account with Bucket, Bucket region, Access Key ID and Secret Access Key Setup backup target in settings Edit virtual machine and take backup ssh to server node with user rancher Run kubectl create deployment nginx --image=nginx:latest on Harvester cluster Run kubectl get pods Expected Results At Step 2, Can download and create image from URL without error At step 6, Can backup running VM to external S3 storage correctly At step 6, Can delete backup from external S3 correctly At step 9, Can pull image from internet and deploy nginx pod in running status harvester-node-0:/home/rancher # kubectl create deployment nginx --image=nginx:latest deployment.</description>
    </item>
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1330-rancher-import-harvester-enhacement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1330-rancher-import-harvester-enhacement/</guid>
      <description> Related issues: #1330 Http proxy setting download image Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6-head Create an one node harvester cluster Both harvester and rancher have internet connection Verification Steps Access rancher dashboard Open Virtualization Management page Import existing harvester Copy the registration url Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Access harvester dashboard Edit cluster-registration-url in settings Paste the registration url and save Back to rancher and wait for harvester imported in Rancher Expected Results Harvester can be imported in rancher dashboard with running status Can access harvester in virtual machine page Can create harvester cloud credential Can load harvester cloud credential while creating harvester </description>
    </item>
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/live-migration/1401-support-volume-hot-unplug-live-migrate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/1401-support-volume-hot-unplug-live-migrate/</guid>
      <description> Related issues: #1401 Http proxy setting download image Environment setup Setup an airgapped harvester&#xA;Create an 3 nodes harvester cluster with large size disks Verification Steps Scenario2: Live migrate VM not have hot-plugged volume before, do hot-plugged the unplugged. Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click Detach volume Add volume again Migrate VM from one node to another Detach volume Add unplugged volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/volumes/1401-support-volume-hot-unplug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/1401-support-volume-hot-unplug/</guid>
      <description> Related issues: #1401 Http proxy setting download image Environment setup Setup an airgapped harvester&#xA;Create an 3 nodes harvester cluster with large size disks Scenario1: Live migrate VM already have hot-plugged volume to new node, then detach (hot-unplug) it Verification Steps Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click de-attach volume Add volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    <item>
      <title>01-Import existing Harvester clusters in Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/01-import-existing-harvester-in-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/01-import-existing-harvester-in-rancher/</guid>
      <description>This feature have been deprecated which already enhanced and merge to setup from harvester settings Please refer to 02-Integrate to Rancher from Harvester settings to test this feature&#xA;Login rancher dashboard Navigate to Virtual Management Page Click import existing Copy the curl command SSH to harvester master node (user: rancher) Execute the curl command to import harvester to rancher curl --insecure -sfL https://192.168.50.82/v3/import/{identifier}.yaml | kubectl apply -f - Run sudo chmod 775 /etc/rancher/rke2/rke2.</description>
    </item>
    <item>
      <title>02-Integrate to Rancher from Harvester settings (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/02-integrate-rancher-from-harvester-settings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/02-integrate-rancher-from-harvester-settings/</guid>
      <description>Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6-head Create an one node harvester cluster Both harvester and rancher have internet connection Verification Steps Access rancher dashboard Open Virtualization Management page Import existing harvester Copy the registration url Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Access harvester dashboard Edit cluster-registration-url in settings Paste the registration url and save Back to rancher and wait for harvester imported in Rancher Expected Results Harvester can be imported in rancher dashboard with running status Can access harvester in virtual machine page Can create harvester cloud credential Can load harvester cloud credential while creating harvester </description>
    </item>
    <item>
      <title>03-Manage VM in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/03-manage-vm-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/03-manage-vm-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard&#xA;Open harvester from Virtualization Management page Open Virtual Machine page Create a single instance virtual machine in Virtual Machines page Create multiple 3 instances virtual machines in Virtual Machines page Access and check virtual machine details Edit cpu, memory and network of one virtual machine Try Stop, Restart and Migrate virtual machine Try Clone virtual machine Try Delete virtual machine Expected Results Can create a single instance vm correctly Can create multiple instances vm correctly Can diaply all virtual machine information Can change cpu, memory and network and retart vm correctly Can Stop, Restart and Migrate virtual machine correctly Can Clone virtual machine correctly Can Delete virtual machine correctly </description>
    </item>
    <item>
      <title>04-Manage Node in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/04-manage-host-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/04-manage-host-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard&#xA;Open harvester from Virtualization Management page Open Host page Access and check node details Edit node config, change network and add disk Try to Cordon and decordon node Enable and disable Maintenance mode Expected Results Can diaply all node&amp;rsquo;s information Can add disk to node correctly Can change network of node correctly Can Cordon and decordon node correctly Can enable and disable Maintenance mode </description>
    </item>
    <item>
      <title>05-Manage Image in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/05-manage-image-volume-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/05-manage-image-volume-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard&#xA;Open harvester from Virtualization Management page Open Images page Create an image from URL Create an image from file Delete created images Expected Results Can create an image from URL Can create an image from file Can create an image from file Can delete created images correctly </description>
    </item>
    <item>
      <title>06-Manage Network in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/06-manage-network-in-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/06-manage-network-in-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard&#xA;Open harvester from Virtualization Management page Open Network page Create an new virtual network Create a new virtual machine using the new virtual network Delete a virtual network Expected Results Can create an new virtual network Create create a new virtual machine using the new virtual network Virtual machine can retrieve ip address Can delete a virtual network </description>
    </item>
    <item>
      <title>07-Add and grant project-owner user to harvester (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/07-rbac-add-grant-project-owner-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/07-rbac-add-grant-project-owner-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-owner and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-owner user Assign Owner role to it Logout current user from Rancher Login with project-owner Open harvester from Virtualization Management page Expected Results Can create project-owner and set password Can assign Owner role to project-owner in default Can login correctly with project-owner Can manage all default project resources including host, virtual machines, volumes, VM and network </description>
    </item>
    <item>
      <title>08-Add and grant project-readonly user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/08-rbac-add-grant-project-readonly-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/08-rbac-add-grant-project-readonly-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-readonly and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-readonly user Assign Read Only role to it Logout current user from Rancher Login with project-readonly Open harvester from Virtualization Management page Expected Results Can create project-readonly and set password Can assign Read Only role to project-readonly in default Can login correctly with project-readonly Can&amp;rsquo;t see Host page in harvester Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; </description>
    </item>
    <item>
      <title>09-Add and grant project-member user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/09-rbac-add-grant-project-member-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/09-rbac-add-grant-project-member-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-member and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-member user Assign Member role to it Logout current user from Rancher Login with project-member Open harvester from Virtualization Management page Expected Results Can create project-member and set password Can assign Member role to project-member in default Can login correctly with project-member Can&amp;rsquo;t see Host page in harvester Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; </description>
    </item>
    <item>
      <title>10-Add and grant project-custom user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/10--rbacadd-grant-project-custom-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/10--rbacadd-grant-project-custom-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-custom and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-custom user Assign Custom role to it Set Create Namespace, Manage Volumes and View Volumes Logout current user from Rancher Login with project-custom Open harvester from Virtualization Management page Expected Results Can create project-custom and set password Can assign Custom role to project-custom in default Can login correctly with project-custom Can do Create Namespace, Manage Volumes and View Volumes in default project </description>
    </item>
    <item>
      <title>11-Create New Project in Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/11-create-project-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/11-create-project-harvester/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Click Create Project Set CPU and Memory limit in Resource Quotas Change view to testProject only Create some images Create some volumes Create a virtual machine Expected Results Can creat project correctly in Projects/Namespaces page Can create images correctly Can create volumes correctly Can create virtual machine correctly </description>
    </item>
    <item>
      <title>12-Create New Namespace in Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/12-create-namespace-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/12-create-namespace-harvester/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Select the new project created in previous test case Click Create Namespace Set CPU and Memory limit in Container Resource Limit Expected Results Can creat new namepasce in correctly in create new project </description>
    </item>
    <item>
      <title>13-Add and grant project-owner user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/13-rbac-add-grant-project-owner-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/13-rbac-add-grant-project-owner-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-owner user Assign Owner role to it Logout current user from Rancher Login with project-owner Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Owner role to project-owner in testProject project Can manage all testProject project resources including host, virtual machines, volumes, VM and network </description>
    </item>
    <item>
      <title>14-Add and grant project-readonly user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/14-rbac-add-grant-project-readonly-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/14-rbac-add-grant-project-readonly-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-readonly user Assign Read Only role to it Logout current user from Rancher Login with project-readonly Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Read Only role to in testProject project Can login correctly with project-readonly Can&amp;rsquo;t see Host page in testProject only view Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; in testProject only view </description>
    </item>
    <item>
      <title>15-Add and grant project-member user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/15-rbac-add-grant-project-member-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/15-rbac-add-grant-project-member-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-member user Assign Member role to it Logout current user from Rancher Login with project-member Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Member role to project-member in testProject project Can login correctly with project-member Can&amp;rsquo;t see Host page in testProject project Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; in testProject project </description>
    </item>
    <item>
      <title>16-Add and grant project-custom user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/16-rbac-add-grant-project-custom-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/16-rbac-add-grant-project-custom-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-custom user Assign Custom role to it Set Create Namespace, Manage Volumes and View Volumes Logout current user from Rancher Login with project-custom Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Custom role to project-custom in testProject project Can login correctly with project-custom Can do Create Namespace, Manage Volumes and View Volumes in testProject project </description>
    </item>
    <item>
      <title>17-Delete Imported Harvester Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/17-delete-imported-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/17-delete-imported-harvester-cluster/</guid>
      <description> Finish 01-Import existing Harvester clusters in Rancher Open Virtualization Management page Delete already imported harvester Expected Results Can delete imported harvester correctly </description>
    </item>
    <item>
      <title>18-Delete Failed Imported Harvester Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/18-delete-failed-imported-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/18-delete-failed-imported-harvester-cluster/</guid>
      <description> Make failure in 01-Import existing Harvester clusters in Rancher Open Virtualization Management page Delete already imported harvester Expected Results Can delete imported harvester correctly </description>
    </item>
    <item>
      <title>19-Enable Harvester Node Driver</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/19-enable-harvester-node-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/19-enable-harvester-node-driver/</guid>
      <description>From Rancher v2.6.3-rc1, harvester node driver has already builtin racher We don&amp;rsquo;t need manual activate it&#xA;Open Cluster Management Click Drivers page and navigate to Node Drivers tab Search harvester Check Harvester node driver is activated and mark as builtin Expected Results Status displayed Activated </description>
    </item>
    <item>
      <title>20-Create RKE1 Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/20-create-rke1-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/20-create-rke1-kubernetes-cluster/</guid>
      <description>Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create Expand RKE1 Configuration Add Template in Node template Select Harvester Select created cloud credential created Select default namespace Select ubuntu image Select network: vlan1 Provide SSH User: ubuntu Provide template name, click create Open Cluster page, click Create&#xA;Toggle RKE1&#xA;Provide cluster name&#xA;Provide Name Prefix</description>
    </item>
    <item>
      <title>21-Delete RKE1 Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/21-delete-rke1-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/21-delete-rke1-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned RKE1 cluster Click Delete from menu Expected Results Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>22-Create RKE2 Kubernetes Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/22-create-rke2-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/22-create-rke2-kubernetes-cluster/</guid>
      <description> Click Cluster Management Click Cloud Credentials Click create and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services </description>
    </item>
    <item>
      <title>23-Delete RKE2 Kubernetes Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/23-delete-rke2-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/23-delete-rke2-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned RKE2 cluster Click Delete from menu Expected Results Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>24-Delete RKE1 Kubernetes Cluster in Provisioning</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/24-delete-rke1-kubernetes-cluster-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/24-delete-rke1-kubernetes-cluster-provisioning/</guid>
      <description> Provision RKE1 Cluster Management When RKE1 cluster show Provisioning Click Delete from menu Expected Results Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>25-Delete RKE1 Kubernetes Cluster in Failure</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/25-delete-rke1-kubernetes-cluster-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/25-delete-rke1-kubernetes-cluster-failure/</guid>
      <description> Provision RKE1 Cluster Management When RKE1 cluster displayed in Failure Click Delete from menu Expected Results Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>26-Delete RKE2 Kubernetes Cluster in Provisioning</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/26-delete-rke2-kubernetes-cluster-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/26-delete-rke2-kubernetes-cluster-provisioning/</guid>
      <description> Provision RKE2 Cluster Management When RKE2 cluster show Provisioning Click Delete from menu Expected Results Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>27-Delete RKE2 Kubernetes Cluster in Failure</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/27-delete-rke2-kubernetes-cluster-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/27-delete-rke2-kubernetes-cluster-failure/</guid>
      <description> Provision RKE2 Cluster Management When RKE2 cluster displayed in Failure Click Delete from menu Expected Results Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>28-Deploy Harvester cloud provider to RKE1 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/28-deploy-harvester-cloud-provider-to-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/28-deploy-harvester-cloud-provider-to-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify steps Environment preparation as above steps Import harvester to rancher from harvester settings Create cloud credential Create RKE1 node template Provision a RKE1 cluster, check the Harvester as cloud provider Access RKE1 cluster Open charts in Apps &amp;amp; Market page Install harvester cloud provider Make sure cloud provider installed complete NAME: harvester-cloud-provider LAST DEPLOYED: Thu Dec 16 03:57:26 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None --------------------------------------------------------------------- SUCCESS: helm install --namespace=kube-system --timeout=10m0s --values=/home/shell/helm/values-harvester-cloud-provider-100.</description>
    </item>
    <item>
      <title>29-Deploy Harvester cloud provider to RKE2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/29-deploy-harvester-cloud-provider-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/29-deploy-harvester-cloud-provider-to-rke2-cluster/</guid>
      <description> Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Check alread set Harvester as cloud provider Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services Check cloud provider installed and configured on RKE2 cluster </description>
    </item>
    <item>
      <title>30-Configure Harvester LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/30-configure-harvester-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/30-configure-harvester-loadbalancer-service/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case&#xA;Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer Given service name to make the load balancer name composed of the cluster name, namespace, svc name, and suffix(8 characters) more than 63 characters Provide Listening port and Target port Click Add-on Config Select Health Check port Select dhcp as IPAM mode Provide Health Check Threshold Provide Health Check Failure Threshold Provide Health Check Period Provide Health Check Timeout Click Create button Create another load balancer service with the name characters.</description>
    </item>
    <item>
      <title>31-Specify &#34;pool&#34; IPAM mode in LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/31-specify-pool-ipam-mode-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/31-specify-pool-ipam-mode-loadbalancer-service/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case&#xA;Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Access Harvester dashboard UI Go to Settings Create a vip-pool in Harvester settings. Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer Given service name Provide Listending port and Target port Click Add-on Config Provide Health Check port Select pool as IPAM mode Provide Health Check Threshold Provide Health Check Failure Threshold Provide Health Check Period Provide Health Check Timeout Click Create button Expected Results Can create load balance service correctly Can operate and route to deployed service correctly </description>
    </item>
    <item>
      <title>32-Deploy Harvester CSI provider to RKE 1 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/32-deploy-harvester-csi-provider-to-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/32-deploy-harvester-csi-provider-to-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify steps Environment preparation as above steps Import harvester to rancher from harvester settings Create cloud credential Create RKE1 node template Provision a RKE1 cluster, check the Harvester as cloud provider Access RKE1 cluster Open charts in Apps &amp;amp; Market page Install Harvester CSI driver Make sure CSI driver installed complete NAME: harvester-csi-driver LAST DEPLOYED: Thu Dec 16 03:59:54 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Successfully deployed Harvester CSI driver to the kube-system namespace.</description>
    </item>
    <item>
      <title>33-Deploy Harvester CSI provider to RKE 2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/33-deploy-harvester-csi-provider-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/33-deploy-harvester-csi-provider-to-rke2-cluster/</guid>
      <description> Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Check alread set Harvester as cloud provider Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services Check CSI driver installed and configured on RKE2 cluster </description>
    </item>
    <item>
      <title>34-Hot plug and unplug volumes in RKE1 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/34-hotplug-unplug-volumes-in-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/34-hotplug-unplug-volumes-in-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify Steps Environment preparation as above steps&#xA;Import harvester to rancher from harvester settings&#xA;Create cloud credential&#xA;Create RKE1 node template Provision a RKE1 cluster, check the Harvester as cloud provider Access RKE1 cluster&#xA;Open charts in Apps &amp;amp; Market page&#xA;Install harvester cloud provider and CSI driver</description>
    </item>
    <item>
      <title>35-Hot plug and unplug volumes in RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/35-hotplug-unplug-volumes-in-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/35-hotplug-unplug-volumes-in-rke2-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify Steps Environment preparation as above steps&#xA;Import harvester to rancher from harvester settings&#xA;Create cloud credential&#xA;Create RKE2 cluster as test case #34&#xA;Access RKE2 cluster&#xA;Open charts in Apps &amp;amp; Market page&#xA;Install harvester cloud provider and CSI driver&#xA;Make sure cloud provider installed complete</description>
    </item>
    <item>
      <title>36-Remove Harvester LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/36-remove-harvester-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/36-remove-harvester-loadbalancer-service/</guid>
      <description> Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Delete previous created load balancer service Expected Results Can remove load balance service correctly Service will be removed from assigned Apps </description>
    </item>
    <item>
      <title>37-Import Online Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher-copy/</guid>
      <description>Environment Setup Setup the online harvester&#xA;Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server&#xA;Move to vagrant pxe harvester folder Execute vagrant ssh pxe_server Run apt-get install squid Edit /etc/squid/squid.</description>
    </item>
    <item>
      <title>37-Import Online Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher/</guid>
      <description>Environment Setup Setup the online harvester&#xA;Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server&#xA;Move to vagrant pxe harvester folder Execute vagrant ssh pxe_server Run apt-get install squid Edit /etc/squid/squid.</description>
    </item>
    <item>
      <title>38-Import Airgapped Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/38-import-airgapped-harvester-from-airgapped-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/38-import-airgapped-harvester-from-airgapped-rancher/</guid>
      <description>Related task: #1052 Test Air gap with Rancher integration Environment Setup Setup the airgapped harvester&#xA;Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    <item>
      <title>39-Standard user no Harvester Access</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/39-rbac-standard-user-no-access/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/39-rbac-standard-user-no-access/</guid>
      <description> As admin import/register a harvester cluster in Rancher As admin, Enable Harvester node driver As a standard user User1, login to rancher Verify User1 has no access to harvester cluster in Virtualization management page Verify User1 can not create harvester cloud credential as User1 Verify User1 can not use this cloud credential to create a node template and can not use a node driver cluster 3 and can not CRUD each resource </description>
    </item>
    <item>
      <title>40-RBAC Add restricted admin User Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/40-rbac-add-restricted-admin-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/40-rbac-add-restricted-admin-user-harvester/</guid>
      <description> As admin import/register a harvester cluster in Rancher create restricted admin user rstradm verify rstradm has access to to Virturalization management page and the harvester cluster is listed Verify rstradm has access to Harvester UI through rancher by selecting it from the list in step 3 and can CRUD each resource </description>
    </item>
    <item>
      <title>41-Import Harvester into nested Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/41-rancher-nested-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/41-rancher-nested-harvester/</guid>
      <description>Prerequisite: External network on VLAN&#xA;Install Rancher in a VM using Docker method on Harvester cluster using the external VLAN Login rancher dashboard Navigate to Virtual Management Page Click import existing Copy the curl command SSH to harvester master node (user: rancher) Execute the curl command to import harvester to rancher curl --insecure -sfL https://192.168.50.82/v3/import/{identifier}.yaml | kubectl apply -f - Run sudo chmod 775 /etc/rancher/rke2/rke2.yaml to solve the permission denied error Run curl command again, you should see the following successful import message namespace/cattle-system configured serviceaccount/cattle created clusterrolebinding.</description>
    </item>
    <item>
      <title>42-Add cloud credential KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/42-add-cloud-credential-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/42-add-cloud-credential-kubeconfig/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester&#xA;Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select external cluster Input KUBECONFIG from Harvester Click Create </description>
    </item>
    <item>
      <title>43-Scale up node driver RKE1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/43-node-driver-scale-up-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/43-node-driver-scale-up-rke1/</guid>
      <description>Prerequisite: RKE1 cluster in Harvester with at least 2 worker nodes&#xA;provision a multinode cluster using harvester node driver with at least 2 worker nodes scale up a node in the cluster </description>
    </item>
    <item>
      <title>44-Scale up node driver RKE2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/44-node-driver-scale-up-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/44-node-driver-scale-up-rke2/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester&#xA;provision a multinode cluster using harvester node driver with at least 2 worker nodes scale up a node in the cluster </description>
    </item>
    <item>
      <title>45-Scale down node driver RKE1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/45-node-driver-scale-down-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/45-node-driver-scale-down-rke1/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester&#xA;provision a multinode cluster using harvester node driver with at least 2 worker nodes scale down a node in the cluster </description>
    </item>
    <item>
      <title>46-Scale down node driver RKE2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/46-node-driver-scale-down-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/46-node-driver-scale-down-rke2/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester&#xA;provision a multinode cluster using harvester node driver with at least 2 worker nodes scale down a node in the cluster </description>
    </item>
    <item>
      <title>47-Verify Backup and restore on same server</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/47-verify-backup-restore-same-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/47-verify-backup-restore-same-server/</guid>
      <description></description>
    </item>
    <item>
      <title>48-Verify Backup and restore on server migration</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/48-verify-backup-restore-server-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/48-verify-backup-restore-server-migration/</guid>
      <description> Create a harvester cluster Deploy a harvester node driver cluster preupgrade checks on both Take a backup Restore on a new rancher server from backup taken ins step 4 Run post-upgrade checks for both clusters Verify virtualization management → harvester is accessible Run p0 use case </description>
    </item>
    <item>
      <title>49-Overprovision Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/49-overprovision-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/49-overprovision-harvester/</guid>
      <description> import harvester into rancher over-provision the connected harvester cluster (i.e. deploy large number of nodes) note: the number will depend on the resources available in the harvester cluster you&amp;rsquo;ve imported. i.e. a harvester setup with 24 cores, 64 GB of ram, you could try provisioning a 3cp, 2cp, 2w cluster of size 4 vCPU 8GB ram to over-provision CPU i.e. a harvester setup with 24 cores, 64 GB of ram, you could try provisioning a 3cp, 2cp, 2w cluster of size 2 vCPU 10GB ram to over-provision CPU </description>
    </item>
    <item>
      <title>50-Use fleet when a harvester cluster is imported to rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/50-fleet-with-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/50-fleet-with-harvester/</guid>
      <description> deploy rancher with harvester enabled docker: &amp;ndash;features=harvester=enabled helm: &amp;ndash;set &amp;rsquo;extraEnv[0].name=CATTLE_FEATURES&amp;rsquo; &amp;ndash;set &amp;rsquo;extraEnv[0].value=harvester=enabled import a harvester setup go to fleet → repos -&amp;gt; create validate that that the harvester cluster is NOT in the dropdown for cluster deployments validate that selecting the &amp;lsquo;all clusters&amp;rsquo; option for deployment does NOT deploy to the harvester cluster </description>
    </item>
    <item>
      <title>51-Use harvester cloud provider to provision an LB - rke1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/51-harvester-cloud-provider-loadbalancer-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/51-harvester-cloud-provider-loadbalancer-rke1/</guid>
      <description> Provision cluster using rke1 with harvester as the node driver enable the cloud driver for harvester while provisioning the cluster run jenkins v3 validation checks once cluster comes to active </description>
    </item>
    <item>
      <title>52-Use harvester cloud provider to provision an LB - rke2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/52-harvester-cloud-provider-loadbalancer-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/52-harvester-cloud-provider-loadbalancer-rke2/</guid>
      <description> Provision cluster using rke1 with harvester as the node driver enable the cloud driver for harvester while provisioning the cluster run jenkins v3 validation checks once cluster comes to active </description>
    </item>
    <item>
      <title>53-Disable Harvester flag with Harvester cluster added</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/53-disable-harvester-flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/53-disable-harvester-flag/</guid>
      <description>Pre-requisites: Rancher with Harvester imported&#xA;Disable Harvester feature flag on Rancher Expected Results Harvester should show up in cluster management Virtualization management tab should be hidden. </description>
    </item>
    <item>
      <title>54-Import Airgapped Harvester From the Online Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/54-import-airgapped-harvester-from-online-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/54-import-airgapped-harvester-from-online-rancher/</guid>
      <description>Environment Setup Setup the airgapped harvester&#xA;Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt; Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server</description>
    </item>
    <item>
      <title>55-Import Harvester to Rancher in airgapped different subnet</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/55-import-harvester-rancher-airgapped-different-subnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/55-import-harvester-rancher-airgapped-different-subnet/</guid>
      <description>Environment Setup Note: Harvester and Rancher are under different subnet, can access to each other&#xA;Setup the airgapped harvester&#xA;Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt; Create ubuntu cloud image from URL Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server</description>
    </item>
    <item>
      <title>56-Import Harvester to Rancher in airgapped different subnet</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/56-import-harvester-rancher-online-different-subnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/56-import-harvester-rancher-online-different-subnet/</guid>
      <description>Environment Setup Note: Harvester and Rancher are under different subnet, can access to each other&#xA;Setup the online harvester&#xA;Iso or vagrant ipxe install harvester on network with internet connection Enable vlan on harvester-mgmt Create virtual machine with name vlan1 and id: 1 Create ubuntu cloud image from URL Create virtual machine and assign vlan network, confirm can get ip address Setup the online rancher&#xA;Install rancher on network with internet connection throug docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.</description>
    </item>
    <item>
      <title>57-Import airgapped harvester from airgapped rancher with Proxy</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/57-import-airgapped-harvester-from-airgapped-rancher-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/57-import-airgapped-harvester-from-airgapped-rancher-proxy/</guid>
      <description>Related task: #1052 Test Air gap with Rancher integration Environment Setup Setup the airgapped harvester&#xA;Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    <item>
      <title>58-Negative-Fully power cycle harvester node machine should recover RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/58-negative-fully-power-cycle-harvester-node-machine-should-recover-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/58-negative-fully-power-cycle-harvester-node-machine-should-recover-rke2-cluster/</guid>
      <description>Related issue: #1561 Fully shutdown then power on harvester node machine can&amp;rsquo;t get provisioned RKE2 cluster back to work&#xA;Related issue: #1428 rke2-coredns-rke2-coredns-autoscaler timeout&#xA;Environment Setup The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan Verification Step Prepare a 3 nodes harvester cluster (provo bare machine) Enable virtual network with harvester-mgmt Create vlan1 with id 1 Import harvester from rancher and create cloud credential Provision a RKE2 cluster with vlan 1 Wait for build up ready Shutdown harvester node 3 Shutdown harvester node 2 Shutdown harvester node 1 Wait for 20 minutes Power on node 1, wait 10 seconds Power on node 2, wait 10 seconds Power on node 3 Wait for harvester startup complete Wait for RKE2 cluster back to work Check node and VIP accessibility Check the rke2-coredns pod status kubectl get pods --all-namespaces | grep rke2-coredns Expected Results RKE2 cluster on harvester can recover to Active status</description>
    </item>
    <item>
      <title>59-Create K3s Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/59-create-k3s-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/59-create-k3s-kubernetes-cluster/</guid>
      <description>Click Cluster Management Click Cloud Credentials Click create and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create Click Clusters&#xA;Click Create&#xA;Toggle RKE2/K3s&#xA;Select Harvester&#xA;Input Cluster Name&#xA;Select default namespace&#xA;Select ubuntu image&#xA;Select network vlan1&#xA;Input SSH User: ubuntu Click Show Advanced&#xA;Add the following user data:&#xA;password: 123456 chpasswd: { expire: false } ssh_pwauth: true Click the drop down Kubernetes version list</description>
    </item>
    <item>
      <title>60-Delete K3s Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/60-delete-k3s-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/60-delete-k3s-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned K3s cluster Click Delete from menu Expected Results Can remove K3s Cluster and disapper on Cluster page K3s Cluster will be removed from rancher menu under explore cluster K3s virtual machine should be also be removed from Harvester </description>
    </item>
    <item>
      <title>61-Deploy Harvester cloud provider to k3s Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/61-deploy-harvester-cloud-provider-to-k3s-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/61-deploy-harvester-cloud-provider-to-k3s-cluster/</guid>
      <description>Related task: #1812 K3s cloud provider and csi driver support Environment Setup Docker install rancher v2.6.4 Create one node harvester with enough resource Verify steps Follow step 1~13 in tets plan 59-Create K3s Kubernetes Cluster&#xA;Click the Edit yaml button Set disable-cloud-provider: true to disable default k3s cloud provider. Add cloud-provider=external to use harvester cloud provider. Create K3s cluster Download the Generate addon configuration for cloud provider Download Harvester kubeconfig and add into your local ~/.</description>
    </item>
    <item>
      <title>62-Configure the K3s &#34;DHCP&#34; LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/62-configure-k3s-dhcp-loadbalancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/62-configure-k3s-dhcp-loadbalancer/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan&#xA;59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster Create Nginx workload for testing Create a test-nginx deployment with image nginx:latest. Add pod label test: test. Create a DHCP LoadBalancer Open Kubectl shell. Create test-dhcp-lb.yaml file. apiVersion: v1 kind: Service metadata: annotations: cloudprovider.harvesterhci.io/ipam: dhcp name: test-dhcp-lb namespace: default spec: ports: - name: http nodePort: 30172 port: 8080 protocol: TCP targetPort: 80 selector: test: test sessionAffinity: None type: LoadBalancer Run k apply -f test-dhcp-lb.</description>
    </item>
    <item>
      <title>62-Configure the K3s &#34;DHCP&#34; LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/64-configure-k3s-dhcp-lb-healcheck/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/64-configure-k3s-dhcp-lb-healcheck/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan&#xA;59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster 62-Configure the K3s &amp;ldquo;DHCP&amp;rdquo; LoadBalancer service A Working DHCP load balancer service created on K3s cluster Edit Load balancer config Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs Configure port, IPAM and health check related setting on Add-on Config page Expected Results Can create load balance service correctly Can route workload to nginx deployment </description>
    </item>
    <item>
      <title>63-Configure the K3s &#34;Pool&#34; LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/63-configure-k3s-pool-loadbalancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/63-configure-k3s-pool-loadbalancer/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan&#xA;59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster Create Nginx workload for testing Create a test-nginx deployment with image nginx:latest. Add pod label test: test. Create a Pool LoadBalancer Modify vip-pool in Harvester settings. Open Kubectl shell.&#xA;Create test-pool-lb.yaml file.&#xA;apiVersion: v1 kind: Service metadata: annotations: cloudprovider.harvesterhci.io/ipam: pool name: test-pool-lb namespace: default spec: ports: - name: http nodePort: 32155 port: 8080 protocol: TCP targetPort: 80 selector: test: test sessionAffinity: None type: LoadBalancer Run k apply -f test-pool-lb.</description>
    </item>
    <item>
      <title>65-Configure the K3s &#34;Pool&#34; LoadBalancer health check</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/65-configure-k3s-pool-lb-healthcheck/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/65-configure-k3s-pool-lb-healthcheck/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan&#xA;59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster 63-Configure the K3s &amp;ldquo;Pool&amp;rdquo; LoadBalancer service A Working DHCP load balancer service created on K3s cluster Edit Load balancer config Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs Configure port, IPAM and health check related setting on Add-on Config page Expected Results Can create load balance service correctly Can route workload to nginx deployment </description>
    </item>
    <item>
      <title>66-Deploy Harvester csi driver to k3s Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/66-deploy-harvester-csi-driver-to-k3s-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/66-deploy-harvester-csi-driver-to-k3s-cluster/</guid>
      <description>Related task: #2755 Steps to manually install Harvester csi-driver on K3s cluster Reference Document Deploying with Harvester K3s Node Driver&#xA;Verify steps Prepare a Harvester cluster with enough cpu, memory and disks for K3s guest cluster&#xA;Create a Rancher instance&#xA;Import Harvester in Rancher and create cloud credential&#xA;ssh to Harvester management node&#xA;Extract the kubeconfig of Harvester with cat /etc/rancher/rke2/rke2.yaml&#xA;Change the server value from https://127.0.0.1:6443/ to your VIP</description>
    </item>
    <item>
      <title>67-Harvester persistent volume on k3s Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/67-harvester-persistent-volume-on-k3s-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/67-harvester-persistent-volume-on-k3s-cluster/</guid>
      <description>Related task: #2755 Steps to manually install Harvester csi-driver on K3s cluster Verify steps Follow test case 66-Deploy Harvester csi driver to k3s Cluster to manually install csi-driver on k3s cluster&#xA;Create a nginx deployment in Workload -&amp;gt; Deployments&#xA;Create a Persistent Volume Claims, select storage class to harvester&#xA;Select the Single-Node Read/Write&#xA;Open Harvester Volumes page, check the corresponding volume exists Click Execute shell to access Nginx container.</description>
    </item>
    <item>
      <title>68-Fully airgapped rancher integrate with harvester with no proxy</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/68-fully-airgapped-rancher-integrate-harvester-no-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/68-fully-airgapped-rancher-integrate-harvester-no-proxy/</guid>
      <description>Related task: #1808 RKE2 provisioning fails when Rancher has no internet access (air-gapped)&#xA;Note1: in fully air gapped environment, you have to setup private docker hub registry and pull all rancher related offline image&#xA;Note2: Please use SUSE SLES JeOS image, it have qemu-guest-agent already installed, thus the guest VM can get IP correctly&#xA;Environment Setup Setup the airgapped harvester&#xA;Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.</description>
    </item>
    <item>
      <title>69-DHCP Harvester LoadBalancer service no health check</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/69-dhcp-loadbalancer-service-no-health-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/69-dhcp-loadbalancer-service-no-health-check/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case&#xA;Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer Given service name to make the load balancer name composed of the cluster name, namespace, svc name, and suffix(8 characters) more than 63 characters Provide Listening port and Target port Click Add-on Config</description>
    </item>
    <item>
      <title>70-Pool LoadBalancer service no health check</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/70-pool-loadbalancer-service-no-health-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/70-pool-loadbalancer-service-no-health-check/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case&#xA;Open Global Settings in hamburger menu&#xA;Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html&#xA;Change ui-offline-preferred to Remote&#xA;Refresh the current page (ctrl + r)&#xA;Access Harvester dashboard UI&#xA;Go to Settings&#xA;Create a vip-pool in Harvester settings. Open provisioned RKE2 cluster from hamburger menu&#xA;Drop down Service Discovery&#xA;Click Services&#xA;Click Create&#xA;Select Load Balancer Given service name&#xA;Provide Listening port and Target port Click Add-on Config</description>
    </item>
    <item>
      <title>71-Manually Deploy Harvester csi driver to RKE2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/71-manually-deploy-csi-driver-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/71-manually-deploy-csi-driver-to-rke2-cluster/</guid>
      <description>Related task: #2755 Steps to manually install Harvester csi-driver on RKE2 cluster Reference Document Deploying with Harvester RKE2 Node Driver&#xA;Verify steps ssh to Harvester management node&#xA;Extract the kubeconfig of Harvester with cat /etc/rancher/rke2/rke2.yaml&#xA;Change the server value from https://127.0.0.1:6443/ to your VIP&#xA;Copy the kubeconfig and add into your local ~/.kube/config file&#xA;Import Harvester in Rancher&#xA;Create cloud credential&#xA;Provision a RKE2 cluster Provide the login credential in user data</description>
    </item>
    <item>
      <title>72-Use ipxe example to test fully airgapped rancher integration</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/72-ipxe-auto-airgapped-rancher-integrate-harvester-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/72-ipxe-auto-airgapped-rancher-integrate-harvester-/</guid>
      <description>Related task: #1808 RKE2 provisioning fails when Rancher has no internet access (air-gapped)&#xA;Note1: In this test, we use vagrant-pxe-airgap-harvester to automatically provide the fully airgapped environment&#xA;Note1: Compared to test case 68, we don&amp;rsquo;t need to manually create a separate VM for the Rancher instance and docker private registry, all the prerequisite environment can be done with the vagrant-pxe-airgap-harvester solution&#xA;Environment Setup Phase 1: Create airgapped Harvester cluster, Rancher and private registry Clone the latest ipxe-example which include the vagrant-pxe-airgap-harvester Follow the Sample Host Loadout and Prerequisites in readme to prepare the prerequisite package If you use Opensuse Leap operating system, you may need to comment out the following line in Vagrantfile file # libvirt.</description>
    </item>
    <item>
      <title>Adapt alertmanager to dedicated storage network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2715_adapt_alertmanager_to_dedicated_storage_network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2715_adapt_alertmanager_to_dedicated_storage_network/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2715&#xA;criteria PVCs (alertmanager/grafana/Prometheus) will attach back after dedicated storage network switched.&#xA;Verify Steps: Install Harvester with any nodes Navigate to Networks -&amp;gt; Cluster Networks/Configs, create Cluster Network named vlan, create Network Config for all nodes Navigate to Advanced -&amp;gt; Settings, edit storage-network Select Enable then select vlan as cluster network, fill in VLAN ID and IP Range Wait until error message (displayed under storage network setting) disappeared Navigate to Monitoring &amp;amp; Logging -&amp;gt; Monitoring -&amp;gt; Configuration Dashboard of Prometheus Graph, Grafana and Altertmanager should able to access, and should contain old data.</description>
    </item>
    <item>
      <title>Add a custom &#34;Docker Install URL&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-docker-install-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-docker-install-url/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Add a custom &#34;Insecure Registries&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-insecure-registries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-insecure-registries/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute docker info, check the &amp;ldquo;Insecure Registries&amp;rdquo; setting is &amp;ldquo;harbor.wujing.site&amp;rdquo; Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Add a custom &#34;Registry Mirrors&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-registry-mirrors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-registry-mirrors/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute &amp;ldquo;docker info&amp;rdquo;, check the &amp;ldquo;Registry Mirrors&amp;rdquo; setting is &amp;ldquo;https://s06nkgus.mirror.aliyuncs.com&amp;rdquo; Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Add a custom &#34;Storage Driver&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-storage-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-storage-driver/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute &amp;ldquo;docker info&amp;rdquo;, check the Storage Driver setting is overlay Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Add a network to an existing VM with only 1 network (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-only-1-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-only-1-network/</guid>
      <description> Add a network to the VM Save the VM Wait for it to start/restart Expected Results the VM should start successfully The already existing network connectivity should still work The new connectivity should also work </description>
    </item>
    <item>
      <title>Add a network to an existing VM with two networks</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-two-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-two-networks/</guid>
      <description> Add a network to the VM Save the VM Wait for it to start/restart Expected Results the VM should start successfully The already existing network connectivity should still work The new connectivity should also work </description>
    </item>
    <item>
      <title>Add a node to existing cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/add-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/add-node-cluster/</guid>
      <description> Start with harvester installer and select &amp;lsquo;Join an existing Harvester cluster&amp;rsquo; Provide the management ip and cluster token Expected Results On completion, Harvester should show the same management url as of existing node and status as ready. Check the host section, the joined node must appear </description>
    </item>
    <item>
      <title>Add backup-taget connection status</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2631_add_backup-taget_connection_status/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2631_add_backup-taget_connection_status/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2631&#xA;Verified this feature has been implemented.&#xA;Test Information Environment: qemu/KVM 2 nodes Harvester Version: master-032742f0-head ui-source Option: Auto Verify Steps: Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings Setup a invalid NFS/S3 backup-target, then click Test connection button, error message should displayed Setup a valid NFS/S3 backup-target, then click Test connection button, notify message should displayed Navigate to Advanced/VM Backups, notify message should NOT displayed Navigate to Advanced/Settings and stop the backup-target server, then navigate to Advanced/VM Backups, error message should displayed </description>
    </item>
    <item>
      <title>Add cluster driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/add-cluster-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/add-cluster-driver/</guid>
      <description> Cluster Management &amp;gt; Drivers &amp;gt; Node Drivers Click &amp;ldquo;Add Node driver&amp;rdquo; Add the correct configuration and save Expected Results Created successfully, status is active </description>
    </item>
    <item>
      <title>Add extra disks by using raw disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</guid>
      <description>Prepare a disk (with WWN) and attach it to the node. Navigate to &amp;ldquo;Host&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo; and open the dropdown menu &amp;ldquo;Add disks&amp;rdquo;. Choose a disk to add, e.g. /dev/sda but not /dev/sda1. Expected Results The raw disk shall be schedulable as a longhorn disk as a whole (without any partition). Ths raw disk shall be in provisioned phase. Reboot the host and the disk shall be reattached and added back as a longhorn disk.</description>
    </item>
    <item>
      <title>Add Labels (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/add-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/add-labels/</guid>
      <description> Add multiple labels to the images. Click save Expected Results Labels should be added successfully </description>
    </item>
    <item>
      <title>Add multiple Networks via form</title>
      <link>https://harvester.github.io/tests/manual/network/add-multiple-networks-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/add-multiple-networks-form/</guid>
      <description> Create a new VM via the web form Add both a management network and an external VLAN network Validate both interfaces exist in the VM ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should create You should see three interfaces listed in VM You should get responses from pinging the VM You should get responses from pinging the VM </description>
    </item>
    <item>
      <title>Add multiple Networks via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/add-multiple-networks-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/add-multiple-networks-yaml/</guid>
      <description> Create a new VM via YAML Add both a management network and an external VLAN network Validate both interfaces exist in the VM ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should create You should see three interfaces listed in VM You should get responses from pinging the VM You should get responses from pinging the VM </description>
    </item>
    <item>
      <title>Add network reachability detection from host for the VLAN network</title>
      <link>https://harvester.github.io/tests/manual/network/add-network-reachability-detection-from-host-for-vlan-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/add-network-reachability-detection-from-host-for-vlan-network/</guid>
      <description>Related issue: #1476 Add network reachability detection from host for the VLAN network Category: Network Environment Setup The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan Verification Steps Enable virtual network with harvester-mgmt in harvester Create VLAN 806 with id 806 and set to default auto mode Import harvester to rancher 1 .Create cloud credential Provision a rke2 cluster to harvester Deploy a nginx server workload Open Service Discover -&amp;gt; Services</description>
    </item>
    <item>
      <title>Add the different roles to the cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-different-roles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-different-roles/</guid>
      <description> Create three users user1, user2, user3 Give the roles of Cluster Owner to user1, Create Project to user2 and Cluster Member to user3 respectively. Login with these three roles Expected Results </description>
    </item>
    <item>
      <title>Add VLAN network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/add-vlan-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/add-vlan-network/</guid>
      <description>Environment setup This should be done on a Harvester setup with at least 2 NICs and at least 2 nodes. This is easily tested in Vagrant&#xA;Verification Steps Open settings on a harvester cluster Navigate to the VLAN settings page Click Enabled Check dropdown for NICs and verify that percentage is showing 100% Add the NIC Click Save Validate that it has updated in settings Expected Results You should be able to add the VLAN network device You should see in the settings list that it has your new default NIC </description>
    </item>
    <item>
      <title>Add websocket disconnect notification</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2186_add_websocket_disconnect_notification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2186_add_websocket_disconnect_notification/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2186&#xA;Verify Steps: Install Harvester with at least 2 nodes Login to Dashboard via Node IP Navigate to Advanced/Settings and update ui-index to https://releases.rancher.com/harvester-ui/dashboard/release-harvester-v1.0/index.html and force refresh to make it applied. restart the Node which holding the IP Notification of websocket disconnected should appeared </description>
    </item>
    <item>
      <title>Add/remove a node in the created harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-add-remove-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-add-remove-node/</guid>
      <description> add/remove a node in the created harvester cluster Expected Results rancher on the cluster modified successfully harvester corresponding VM node added/removed successfully </description>
    </item>
    <item>
      <title>Add/remove disk to Host config</title>
      <link>https://harvester.github.io/tests/manual/hosts/1623-add-disk-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1623-add-disk-to-host/</guid>
      <description> Related issues: #1623 Unable to add additional disks to host config Environment setup Add Disk that isn&amp;rsquo;t assigned to host Verification Steps Head to &amp;ldquo;Hosts&amp;rdquo; page Click &amp;ldquo;Edit Config&amp;rdquo; on a node and switch to &amp;ldquo;Disks&amp;rdquo; tab Validate: Open dropdown and see no disks Attach a disk on that node Validate: Open dropdown and see some disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Detach a disk on that node Validate: Open dropdown and see no disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Expected Results Disk space should show appropriately </description>
    </item>
    <item>
      <title>Add/remove disk to Host config</title>
      <link>https://harvester.github.io/tests/manual/volumes/1623-add-disk-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/1623-add-disk-to-host/</guid>
      <description> Related issues: #1623 Unable to add additional disks to host config Environment setup Add Disk that isn&amp;rsquo;t assigned to host Verification Steps Head to &amp;ldquo;Hosts&amp;rdquo; page Click &amp;ldquo;Edit Config&amp;rdquo; on a node and switch to &amp;ldquo;Disks&amp;rdquo; tab Validate: Open dropdown and see no disks Attach a disk on that node Validate: Open dropdown and see some disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Detach a disk on that node Validate: Open dropdown and see no disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Expected Results Disk space should show appropriately </description>
    </item>
    <item>
      <title>Additional trusted CA configure-ability</title>
      <link>https://harvester.github.io/tests/manual/deployment/additional-trusted-ca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/additional-trusted-ca/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1260&#xA;Verify Items Image download with self-signed additional-ca VM backup with self-signed additional-ca Case: Image downlaod Install Harvester with ipxe-example which includes https://github.com/harvester/ipxe-examples/pull/36 Upload any valid iso to pxe-server&amp;rsquo;s /var/www/ Use Browser to access https://&amp;lt;pxe-server-ip&amp;gt;/&amp;lt;iso-file&amp;gt; should be valid Add self-signed cert to Harvester Navigate to Harvester Advanced Settings, edit additional-ca cert content can be retrieved in pxe-server /etc/ssl/certs/nginx-selfsigned.crt Create Image with the same URL https://&amp;lt;pxe-server-ip&amp;gt;/&amp;lt;iso-file&amp;gt; Image should be downloaded Case: VM backup Install Harvester with ipxe-example setup Minio in pxe-server follow instruction to download binary and start the service login to UI console then add region and create bucket follow instruction to generate self-signed cert with IP SANs restart service with self-signed cert Add self-signed cert to Harvester Add local Minio info as S3 into backup-target Backup-Target Should not pop up any Error Message Create Image for VM creation Create VM with any resource Perform VM backup VM&amp;rsquo;s data Should be backup into Minio&amp;rsquo;s folder </description>
    </item>
    <item>
      <title>Agent Node should not rely on specific master Node</title>
      <link>https://harvester.github.io/tests/manual/hosts/agent_node_connectivity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/agent_node_connectivity/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1521&#xA;Verify Items Agent Node should keep connection when any master Node is down Case: Agent Node&amp;rsquo;s connecting status Install Harvester with 4 nodes which joining node MUST join by VIP (point server-url to use VIP) Make sure all nodes are ready Login to dashboard, check host state become Active SSH to the 1st node, run command kubectl get node to check all STATUS should be Ready SSH to agent nodes which ROLES IS &amp;lt;none&amp;gt; in Step 2i&amp;rsquo;s output Output should contains VIP in the server URL, by run command cat /etc/rancher/rke2/config.</description>
    </item>
    <item>
      <title>Alertmanager supports main stream receivers</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2521-alertmanager-supports-main-stream-receivers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2521-alertmanager-supports-main-stream-receivers/</guid>
      <description>Related issues: #2521 [FEATURE] Alertmanager supports main stream receivers Category: Alter manager Verification Steps Prepare another VM or machine have the same subnet with the Harvester Prepare a webhook server on the VM, reference to https://github.com/w13915984028/harvester-develop-summary/blob/main/test-log-event-audit-with-webhook-server.md You may need to install python3 web package, refer to https://webpy.org/install Run export PORT=8094 on the webhook server VM Launch the webhook server python3 simple-webhook-server.py davidtclin@ubuntu-clean:~$ python3 simple-webhook-server.py usage: export PORT=1234 to set http server port number as 1234 start a simple webhook server, PORT 8094 @ 2022-09-21 16:39:58.</description>
    </item>
    <item>
      <title>All Namespace filtering in VM list</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</guid>
      <description> Related issues: #2578 [BUG] When first entering the harvester cluster from Virtualization Managements, some vm&amp;rsquo;s in namespace are not shown in the list Category: UI Verification Steps Create a harvester cluster Create a VM in the default namespace Creating a Namespace (eg: test-vm) Import the Harvester cluster in Rancher access to the harvester cluster from Virtualization Management click Virtual Machines tab Expected Results test-vm-1 should also be shown in the list </description>
    </item>
    <item>
      <title>allow users to create cloud-config template on the VM creating page</title>
      <link>https://harvester.github.io/tests/manual/templates/allow-users-to-create-cloud-config-template-on-vm-creating-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/templates/allow-users-to-create-cloud-config-template-on-vm-creating-page/</guid>
      <description> Related issues: #1433 allow users to create cloud-config template on the VM creating page Category: Virtual Machine Verification Steps Create a new virtual machine Click advanced options Drop down user data template -&amp;gt; create new Drop down network data template -&amp;gt; create new Expected Results User can create user and network data template when create virtual machine Created cloud-init template template can be saved and auto selected to the latest one </description>
    </item>
    <item>
      <title>Attach unpartitioned NVMe disks to host</title>
      <link>https://harvester.github.io/tests/manual/hosts/attach-unpartitioned-nvme-disks-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/attach-unpartitioned-nvme-disks-to-host/</guid>
      <description>Related issues: #1414 Adding unpartitioned NVMe disks fails Category: Storage Verification Steps Use qemu-img create -f qcow2 command to create three disk image locally Shutdown target node VM machine Directly edit VM xml content in virt manager page Add to the first line Add the following line before the end of quote &amp;lt;qemu:commandline&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme301.img,if=none,id=D22&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D22,serial=1234&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme302.img,if=none,id=D23&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D23,serial=1235&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme303.</description>
    </item>
    <item>
      <title>Authentication Validation</title>
      <link>https://harvester.github.io/tests/manual/authentication/general-authentication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/general-authentication/</guid>
      <description>Enable Access Control . Choose “Allow any valid User” as “Site Access”. Make sure any user is able to access the site. Enable Access Control . Choose “Restrict to Specific User” and add few users. Make sure only the specified users have access to the server. Others should get authentication error. Enable Access Control . Choose “Restrict to Specific User” and add a group. Make sure only all users belonging to the group have access to the server Others should get authentication error.</description>
    </item>
    <item>
      <title>Auto provision lots of extra disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</guid>
      <description>:warning: This is a heuristic test plan since real world race condition is hard to reproduce. If you find any better alternative, feel free to update.&#xA;This test is better to perform under QEMU/libvirt environment.&#xA;Related issues: #1718 [BUG] Automatic disk provisioning result in unusable ghost disks on NVMe drives Category: Storage Verification Steps Create a harvester cluster and attach 10 or more extra disks (needs WWN so that they can be identified uniquely).</description>
    </item>
    <item>
      <title>Automatically get VIP during PXE installation</title>
      <link>https://harvester.github.io/tests/manual/deployment/1410-pxe-installation-automatically-get-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/1410-pxe-installation-automatically-get-vip/</guid>
      <description> Related issues: #1410 Support getting VIP automatically during PXE boot installation Verification Steps Comment vip and vip_hw_addr in ipxe-examples/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2 Start vagrant-pxe-harvester Run kubectl get cm -n harvester-system vip Check whether we can get ip and hwAddress in it Run ip a show harvester-mgmt Check whether there are two IPs in it and one is the vip. Expected Results VIP should automatically be assigned </description>
    </item>
    <item>
      <title>Automatically get VIP during PXE installation</title>
      <link>https://harvester.github.io/tests/manual/hosts/1410-pxe-installation-automatically-get-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1410-pxe-installation-automatically-get-vip/</guid>
      <description> Related issues: #1410 Support getting VIP automatically during PXE boot installation Verification Steps Comment vip and vip_hw_addr in ipxe-examples/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2 Start vagrant-pxe-harvester Run kubectl get cm -n harvester-system vip Check whether we can get ip and hwAddress in it Run ip a show harvester-mgmt Check whether there are two IPs in it and one is the vip. Expected Results VIP should automatically be assigned </description>
    </item>
    <item>
      <title>Backup and restore of harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-backup-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-backup-restore/</guid>
      <description> create a deployment in harvester cluster Go to the rancher&amp;rsquo;s cluster list and make a backup of the harvester cluster After the backup is complete, delete the deployment created in the harvester cluster go to the list of clusters in the rancher and restore the harvester cluster Expected Results </description>
    </item>
    <item>
      <title>Backup S3 reduce permissions</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup_s3_permission/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup_s3_permission/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1339&#xA;Verify Items Backup target connect to S3 should only require the permission to access the specific bucket Case: S3 Backup with single-bucket-user Install Harvester with any nodes Setup Minio then follow the instruction to create a single-bucket-user. Create specific bucket for the user Create other buckets setup backup-target with the single-bucket-user permission When assign the dedicated bucket (for the user), connection should success. When assign other buckets, connection should failed with AccessDenied error message </description>
    </item>
    <item>
      <title>Backup Single VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm/</guid>
      <description> Click take backup in virtual machine list Expected Results Backup should be created Backup should be listed in backups list Backup should be available on remote storage (S3/NFS) </description>
    </item>
    <item>
      <title>Backup Single VM that has been live migrated before (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-that-has-been-live-migrated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-that-has-been-live-migrated/</guid>
      <description> Click take backup in virtual machine list Expected Results Backup should be created Backup should be listed in backups list Backup should be available on remote storage (S3/NFS) </description>
    </item>
    <item>
      <title>Backup single VM with node off</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-node-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-node-off/</guid>
      <description>On multi-node setup bring down node that is hosting VM Click take backup in virtual machine list Expected Results The backup should complete successfully Comments We do allow taking backup even if the VM is down, as you can take backup when the VM is off, this is because the volume still exists with longhorn&amp;rsquo;s multi replicas, but weneed to check the data integrity.&#xA;Known Bugs https://github.com/harvester/harvester/issues/1483</description>
    </item>
    <item>
      <title>Backup Target error message</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup_target_errmsg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup_target_errmsg/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1051&#xA;Verify Items Backup target should check input before Click Save Error message should displayed on edit page when input is wrong Case: Connect to invalid Backup Target Install Harvester with any node Login to dashboard, then navigate to Advanced Settings Edit backup-target,then input invalid data for NFS/S3 and click Save The Page should not be redirect to Advanced Settings Error Message should displayed under Save button </description>
    </item>
    <item>
      <title>Basic functional verification of Harvester cluster after creation</title>
      <link>https://harvester.github.io/tests/manual/node-driver/verify-cluster-functionality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/verify-cluster-functionality/</guid>
      <description> create the project. deploy deployment Expected Results The project is created successfully Deployment successfully deployed </description>
    </item>
    <item>
      <title>Better Load Balancer Config of Harvester cloud provider</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/better-load-balancer-config-rke2-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/better-load-balancer-config-rke2-cloud-provider/</guid>
      <description>Related issue: #1435 better loadblancer config of Harvester cloud provider Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Import harvester to rancher virtualization management Create a harvester cluster by harvester driver Access the new harvester cluster from rancher cluster management Create a load balancer from service discovery -&amp;gt; services Re login rancher Open create load-balance page Click ctrl+R to refresh page Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs Expected Results User can configure port, IPAM and health check related setting on Add-on Config page Can create load balancer correctly with health check setting</description>
    </item>
    <item>
      <title>Boot installer under Legacy BIOS and UEFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</guid>
      <description>Related issues #2023 Legacy Iso for older servers Verification Steps BIOS Test Build harvester-installer Boot build artifact using BIOS Legacy mode: qemu-system-x86_64 -m 2048 -cdrom ../dist/artifacts/harvester-master-amd64 Verify that the installer boot process reaches the screen that says &amp;ldquo;Create New Cluster&amp;rdquo; or &amp;ldquo;Join existing cluster&amp;rdquo; UEFI Test Build harvester-installer (or use the same one from the BIOS Test, it&amp;rsquo;s a hybrid ISO) Boot build artifact using UEFI mode: qemu-system-x86_64 -m 2048 -cdrom .</description>
    </item>
    <item>
      <title>Button of `Download KubeConfig` (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/misc/download_kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/download_kubeconfig/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1349&#xA;Verify Items Download KubeConfig should not exist in general views Download Kubeconfig should exist in Support page Downloaded file should be named with suffix .yaml Case: Download KubeConfig navigate to every pages to make sure download kubeconfig icon will not appear in header section navigate to support page to check Download KubeConfig is work normally </description>
    </item>
    <item>
      <title>Chain VM templates and images</title>
      <link>https://harvester.github.io/tests/manual/templates/760-chained-vm-templates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/templates/760-chained-vm-templates/</guid>
      <description> Related issues: #760 cloud config byte limit Verification Steps Create a vm and add userData or networkData, test if it works Run VM health checks create a vm template and add userData create a new vm and use the template Run VM health checks use the existing vm to generate a template, then use the template to create a new vm Run VM health Checks Expected Results All VM&amp;rsquo;s should create All VM Health Checks should pass </description>
    </item>
    <item>
      <title>Chain VM templates and images</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/760-chained-vm-templates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/760-chained-vm-templates/</guid>
      <description> Related issues: #760 cloud config byte limit Verification Steps Create a vm and add userData or networkData, test if it works Run VM health checks create a vm template and add userData create a new vm and use the template Run VM health checks use the existing vm to generate a template, then use the template to create a new vm Run VM health Checks Expected Results All VM&amp;rsquo;s should create All VM Health Checks should pass </description>
    </item>
    <item>
      <title>Change api-ui-source bundled	(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-bundled/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-bundled/</guid>
      <description> Log in as admin Navigate to advanced settings Change api-ui-source to bundled Save Refresh page Check page source for dashboard loading location Expected Results Log in should complete Settings should save dashboard location should be loading from /dashboard/_nuxt/ (verify it in browser&amp;rsquo;s developers tools) </description>
    </item>
    <item>
      <title>Change api-ui-source external (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-external/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-external/</guid>
      <description> Log in as admin Navigate to advanced settings Change api-ui-source to external Save Refresh page Check page source for dashboard loading location Expected Results Log in should complete Settings should save dashboard location should be loading from https://releases.rancher.com/harvester-ui/latest (verify it in browser&amp;rsquo;s developers tools) </description>
    </item>
    <item>
      <title>Change DNS servers while installing</title>
      <link>https://harvester.github.io/tests/manual/deployment/1590-change-dns-server-for-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/1590-change-dns-server-for-install/</guid>
      <description>Related issues: #1590 Harvester installer can&amp;rsquo;t resolve hostnames Known Issues When supplying multiple ip=&amp;hellip; kernel cmdline arguments, only one of them will be configured by dracut, therefore only the configured interface would have ifcfg generated. So for now, we can&amp;rsquo;t support multiple ip=&amp;hellip; kernel cmdline arguments&#xA;Verification Steps Because configuring the network of the installation environment only works with PXE installation, you could use ipxe-examples/vagrant-pxe-harvester/ to set it up. Be sure you can run setup_harvester.</description>
    </item>
    <item>
      <title>Change DNS settings on vagrant-pxe-harvester install</title>
      <link>https://harvester.github.io/tests/manual/deployment/ipxe-dns-change/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/ipxe-dns-change/</guid>
      <description>Install using ipxe-examples Also change harvester_network_config.dns_servers in the settings.yml for the vagrant environment before deploy. This will change the DNS in the harvester OS config. If you also want to change the DNS for everything in the DHCP scope change harvester_network_config.dhcp_server.dns_server. Expected Results On completion of the installation, Harvester should provide the management url and show status. SSH into one of the nodes. If you use the default configuration you can use ssh rancher@192.</description>
    </item>
    <item>
      <title>Change log level debug</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-debug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-debug/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Debug Save Create two VMs Reboot both VMs Download Logs Expected Results Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Debug level output </description>
    </item>
    <item>
      <title>Change log level Info	(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-info/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Info Save Create two VMs Reboot both VMs Download Logs Expected Results Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Info level output </description>
    </item>
    <item>
      <title>Change log level Trace	(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-trace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-trace/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Trace Save Create two VMs Reboot both VMs Download Logs Expected Results Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Trace level output </description>
    </item>
    <item>
      <title>Change user password (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/authentication/1409-change-password/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/1409-change-password/</guid>
      <description> Related issues: #1409 There&amp;rsquo;s no way to change user password in single cluster UI Verification Steps Logged in with user Changed password Logged out Logged back in with new password Verified old password didn&amp;rsquo;t work Expected Results Password should change and be accepted on new login Old password shouldn&amp;rsquo;t work </description>
    </item>
    <item>
      <title>Check can apply the resource quota limit to project and namespace</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/check-can-apply-the-resource-quota-limit-to-project-and-namespace-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/check-can-apply-the-resource-quota-limit-to-project-and-namespace-/</guid>
      <description>Related issues: #1454 Incorrect memory unit conversion in namespace resource quota Category: Rancher Integration Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Access Rancher dashboard Open Cluster management -&amp;gt; Explore the active cluster Create a new project test-1454-proj in Projects/Namespaces Set resource quota for the project Memory Limit: Project Limit: 512 Namespace default limit: 256 Memory Reservation: Project Limit: 256 Namespace default limit: 128 Click create namespace test-1454-ns under project test-1454-proj Click Kubectl Shell and run the following command kubectl get ns kubectl get quota -n test-1454-ns Check the output Click Workload -&amp;gt; Deployments -&amp;gt; Create Given the Name, Namespace and Container image Click Create Expected Results Based on configured project resource limit and namespace default limit,</description>
    </item>
    <item>
      <title>Check can start VM after Harvester upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/start-vm-after-harvester-upgrade-complete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/start-vm-after-harvester-upgrade-complete/</guid>
      <description> Related issues: #2270 [BUG] Unable start VM after upgraded v1.0.1 to v1.0.2-rc2 Category: Harvester Upgrade Verification Steps Prepare the previous stable Harvester release cluster Create image Enable Network and create VM Create several virtual machine Follow the official document steps to prepare the online or offline upgrade Shutdown all virtual machines Start the upgrade Confirm all the upgrade process complete Start all the virtual machines Expected Results All virtual machine could be correctly started and work as expected </description>
    </item>
    <item>
      <title>Check conditions when stop/pause VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</guid>
      <description>Related issues: #1987 Verification Steps Stop Request should not have failure message&#xA;Create a VM with runStrategy: RunStrategyAlways. Stop the VM. Check there is no Failure attempting to delete VMI: &amp;lt;nil&amp;gt; in VM status. UI should not show pause message&#xA;Create a VM. Pause the VM. Although the message The status of pod readliness gate &amp;quot;kubevirt.io/virtual-machine-unpaused&amp;quot; is not &amp;quot;True&amp;quot;, but False is in the VM condition, UI should not show it.</description>
    </item>
    <item>
      <title>Check crash dump when there&#39;s a kernel panic</title>
      <link>https://harvester.github.io/tests/manual/hosts/1357-kernel-panic-check-crash-dump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1357-kernel-panic-check-crash-dump/</guid>
      <description> Related issues: #1357 Crash dump not written when kernel panic occurs Verification Steps Created new single node cluster with 16GB RAM Booted into debug mode from GRUB entry Created several VMs triggered kernel panic with echo c &amp;gt;/proc/sysrq-trigger Waited for reboot Verified that dump was saved in /var/crash Expected Results dump should be saved in /var/crash </description>
    </item>
    <item>
      <title>Check default and customized project and namespace details page</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/check-default-customized-project-and-namespace-details-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/check-default-customized-project-and-namespace-details-page/</guid>
      <description> Related issue: #1574 Multi-cluster projectNamespace details page error Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Import harvester from rancher dashboard Access harvester from virtualization management page Create several new projects Create several new namespaces under each new projects Access all default and self created namespace Check can display namespace details Check all new namespaces can display correctly under each projects Expected Results Access harvester from rancher virtualization management page Click any namespace in the Projects/Namespace can display details correctly with no page error Default namespace Customized namespace Newly created namespace will display under project list </description>
    </item>
    <item>
      <title>check detailed network status in host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/check-detailed-network-status-in-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/check-detailed-network-status-in-host-page/</guid>
      <description> Related issues: #531 Better error messages when misconfiguring multiple nics Category: Host Verification Steps Enable vlan cluster network setting and set a default network interface Wait a while for the setting take effect on all harvester nodes Click nodes on host page Check the network tab Expected Results On the Host view page, now we can see detailed network status including Name, Type, IP Address, Status etc.. Check all network interface can display Check the Name, Type, IP Address, Status display correct values </description>
    </item>
    <item>
      <title>Check DNS on install with Github SSH keys</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</guid>
      <description>Related issues: #1903 DNS server not available during install Verification Steps Without PXE Start a new install Set DNS as 8.8.8.8 Add in github SSH keys Finish install SSH into node with SSH keys from github (rancher@hostname) Verify login was successful With PXE Got vagrant setup from https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Changed settings.yml DHCP config and added dns: 8.8.8.8 dhcp_server: ip: 192.168.0.254 subnet: 192.168.0.0 netmask: 255.255.255.0 range: 192.168.0.50 192.168.0.130 dns: 8.8.8.8 https: false Also changed ssh_authorized_keys and commented out default SSH key and added username for github</description>
    </item>
    <item>
      <title>Check favicon and title on pages</title>
      <link>https://harvester.github.io/tests/manual/misc/1520-check-title-and-favicon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/1520-check-title-and-favicon/</guid>
      <description> Related issues: #1520 incorrect title and favicon Verification Steps Log into Harvester Check page title and favicon on each of these pages dashboard main page settings support Volumes SSH Keys Host info Expected Results Harvester favicon and title should show on each page </description>
    </item>
    <item>
      <title>Check Harvester CloudInit CRDs within Harvester, Terraform &amp; Rancher</title>
      <link>https://harvester.github.io/tests/manual/misc/3902-elemental-cloud-init-harvester-crds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/3902-elemental-cloud-init-harvester-crds/</guid>
      <description>Related issues: #3902 support elemental cloud-init via harvester-node-manager Testing With Terraform TBD Testing From Harvester UI TBD Testing From Rancher Fleet UI / Harvester Fleet Controller TBD Testing w/ Harvester Kubeconfig via Kubectl &amp;amp; K9s (or similar tool) Pre-Reqs: Have an available multi-node Harvester cluster, w/out your ssh-key present on any nodes Provision cluster however is easiest K9s (or other similar kubectl tooling) kubectl audit elemental toolkit for an understanding of stages audit harvester configuration to correlate properties to elemental-toolkit based stages / functions Negative Tests: Validate Non-YAML Files Get .</description>
    </item>
    <item>
      <title>Check IPAM configuration with IPAM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</guid>
      <description> Related issues: #1697 Optimization for the Harvester load balancer Verification Steps Install the latest rancher and import a Harvester cluster Create a cluster by Harvester node driver Navigate to the workload Page, create a workload Click &amp;ldquo;Add ports&amp;rdquo;, select type as LB, protocol as TCP Check IPAM selector Navigate to the service page, create a LB Click &amp;ldquo;Add-on config&amp;rdquo; tab and check IPAM and port </description>
    </item>
    <item>
      <title>Check IPv4 static method in ISO installer</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2796-check-ipv4-static-method-in-iso-installer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2796-check-ipv4-static-method-in-iso-installer/</guid>
      <description> Related issues: #2796 [BUG] configure network failed if use static mode Category: Newtork Harvester Installer Verification Steps Use latest ISO to install Enter VLAN field with empty 1 1000 choose static method fill other fields press enter to the next page no error found, and show DNS config page Expected Results During Harvester ISO installer We can configure VLAN network on the static mode with the following settings:&#xA;No error message blocked Can proceed to dns config page </description>
    </item>
    <item>
      <title>Check logs on Harvester</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</guid>
      <description> Related issues: #2528 [BUG] Tons of AppArmor denied messages Category: Logging Environment Setup This should be run on a Harvester node that has been up for a while and has been in use Verification Steps SSH to harvester node Execute journalctl -b -f Look through logs and verify that there isn&amp;rsquo;t anything generating lots of erroneous messages Expected Results There shouldn&amp;rsquo;t be large volumes of erroneous messages </description>
    </item>
    <item>
      <title>Check Longhorn volume mount point</title>
      <link>https://harvester.github.io/tests/manual/hosts/1667-check-longhorn-volume-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1667-check-longhorn-volume-mount/</guid>
      <description> Related issues: #1667 data partition is not mounted to the LH path properly Verification Steps Install Harvester node in VM from ISO Check partitions with lsblk -f Verify mount point of /var/lib/longhorn Expected Results Mount point should show /var/lib/longhorn </description>
    </item>
    <item>
      <title>Check Longhorn volume mount point</title>
      <link>https://harvester.github.io/tests/manual/volumes/1667-check-longhorn-volume-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/1667-check-longhorn-volume-mount/</guid>
      <description> Related issues: #1667 data partition is not mounted to the LH path properly Verification Steps Install Harvester node in VM from ISO Check partitions with lsblk -f Verify mount point of /var/lib/longhorn Expected Results Mount point should show /var/lib/longhorn </description>
    </item>
    <item>
      <title>Check Network interface link status can match the available NICs in Harvester vlanconfig</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2988-check-network-link-match-vlanconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2988-check-network-link-match-vlanconfig/</guid>
      <description>Related issues: #2988 [BUG] Network interface link status judgement did not match the available NICs in Harvester vlanconfig Category: Network Verification Steps Create cluster network cn1 Create a vlanconfig config-n1 on cn1 which applied to node 1 only Select an available NIC on the Uplink Create a vlan, the cluster network cn1 vlanconfig and provide valid vlan id 91 Edit config-n1,&#xA;Check NICs list in Uplink&#xA;ssh to node 1</description>
    </item>
    <item>
      <title>Check rancher-monitoring-grafana volume size</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2282-check-rancher-monitoring-grafana-volume-size/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2282-check-rancher-monitoring-grafana-volume-size/</guid>
      <description> Related issues: #2282 [BUG] rancher-monitoring-grafana is too small and it keeps growing Category: Monitoring Verification Steps Harvester cluster running after 24 hours Access Harvester Longhorn dashboard via https:///dashboard/c/local/longhorn Open the Longhorn UI Open the volume page Check the rancher-monitoring-grafana size and usage Shutdown a management node machine Power on the management node machine Wait for 60 minutes Check the rancher-monitoring-grafana size and usage in Longhorn UI Shutdown all management node machines in sequence Power on all management node machines in sequence Wait for 60 minutes Check the rancher-monitoring-grafana size and usage in Longhorn UI Expected Results The rancher-monitoring-grafana default allocated with 2Gi and Actual usage 108 Mi after running after 24 hours Turn off then turn on the specific vip harvester node machine, the The rancher-monitoring-grafana keep stable in 107 Mi after turning on 60 minutes Turn off then turn on all four harvester node machines, the The rancher-monitoring-grafana keep stable in 107 Mi after turning on 60 minutes </description>
    </item>
    <item>
      <title>Check redirect for editing server URL setting</title>
      <link>https://harvester.github.io/tests/manual/hosts/1489-redirect-for-server-url-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1489-redirect-for-server-url-setting/</guid>
      <description> Related issues: #1489 Edit Advanced Setting option server-url will redirect to inappropriate page Verification Steps Install harvester Access harvester Edit server-url form settings Check server-url save, cancel, and back. Additional context: Expected Results URL should stay the same when navigating </description>
    </item>
    <item>
      <title>Check support bundle for SLE Micro OS</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2420-2464-check-support-bundle-sle-micro-os/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2420-2464-check-support-bundle-sle-micro-os/</guid>
      <description>Related issues: #2420 [FEATURE] support bundle: support SLE Micro OS Related issues: #2464 [backport v1.0] [FEATURE] support bundle: support SLE Micro OS Category: Support Bundle Verification Steps Download support bundle in support page Extract the support bundle, check every file have content ssh to harvester node Check the /etc/os-release file content Expected Results Check can download support bundle correctly, check can access every file without empty&#xA;Checked every harvester nodes, the ID have changed to sle-micro-rancher</description>
    </item>
    <item>
      <title>Check that you can communicate with the Harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/terraformer/harvester-cluster-communicate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/harvester-cluster-communicate/</guid>
      <description>Set the KUBECONFIG env variable with the path of your kubeconfig file Try to import any resource to test the connectivity with the Harvester cluster For instance, try to import ssh-key with: terraformer import harvester -r ssh_key Expected Results You should see:&#xA;terraformer import harvester -r ssh_key 2021/08/04 15:18:59 harvester importing... ssh_key 2021/08/04 15:18:59 harvester done importing ssh_key ... And the generated files should appear in ./generated/harvester/ssh_key/</description>
    </item>
    <item>
      <title>Check the OS types in Advanced Options</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2776-check-os-types-in-advanced-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2776-check-os-types-in-advanced-options/</guid>
      <description> Related issues: #2776 [FEATURE] remove some dead OS types Category: Network Verification Steps Login harvester dashboard Open the VM create page, check the OS type list Open the image create page, check the OS type list Open the template create page, check the OS type list Expected Results The following OS types should be removed from list&#xA;Turbolinux Mandriva Xandros In v1.1.0 master we add the SUSE Linux Enterprise in the VM creation page In the image create page In the template create page </description>
    </item>
    <item>
      <title>Check the VM is available when Harvester upgrade failed</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm-availability-when-harvester-upgrade-failed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/vm-availability-when-harvester-upgrade-failed/</guid>
      <description>Category: Harvester Upgrade Verification Steps Prepare the previous stable Harvester release cluster Create image Enable Network and create VM Create several virtual machine Follow the official document steps to prepare the online or offline upgrade Do not shutdown virtual machine Start the upgrade Check the VM status if the upgrade failed at Preload images, Upgrade Rancher and Upgrade Harvester phase Check the VM status if the upgrade failed at the Pre-drain, Post-drain and RKE2 &amp;amp; OS upgrade phase Expected Results The VM should be work when upgrade failed at Preload images, Upgrade Rancher and Upgrade Harvester phase The VM could not able to function well when upgrade failed at the Pre-drain, Post-drain and RKE2 &amp;amp; OS upgrade phase </description>
    </item>
    <item>
      <title>Check version compatibility during an upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2431-check-version-compatibility-during-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2431-check-version-compatibility-during-upgrade/</guid>
      <description>Related issues: #2431 [FEATURE] Check version compatibility during an upgrade Category: Upgrade Verification Steps Test Plan 1: v1.0.2 upgrade to v1.1.0 with release tag Test Plan 2: v1.0.3 upgrade to v1.1.0 with release tag Test Plan 3: v1.0.2 upgrade to v1.1.0 without release tag Prepare v1.0.2, v1.0.3 Harvester ISO image Prepare v1.1.0 ISO image with release tag Prepare v1.1.0 ISO image without release tag Put different ISO image to HTTP server Create the upgrade yaml to create service cat &amp;lt;&amp;lt;EOF | kubectl apply -f - apiVersion: harvesterhci.</description>
    </item>
    <item>
      <title>Check VM creation required-fields</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1283-vm-creation-required-fields/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1283-vm-creation-required-fields/</guid>
      <description> Related issues: #1283 Fix required fields on VM creation page Verification Steps Create VM without image name and size Create VM without size Create VM wihout image name Create VM without hostname Expected Results You should get an error trying to create VM without image name and size You should get an error trying to create VM without image name You should get an error trying to create VM without size You should not get an error trying to create a VM without hostname </description>
    </item>
    <item>
      <title>Check volume status after upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2920-volume-status-after-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2920-volume-status-after-upgrade/</guid>
      <description>Related issues: #2920 [BUG] Volume can&amp;rsquo;t turn into healthy when upgrading from v1.0.3 to v1.1.0-rc2 Category: Volume Verification Steps Prepare a 4 nodes v1.0.3 Harvester cluster Install several images Create three VMs Enable Network Create vlan1 network Shutdown all VMs Upgrade to v1.1.0-rc3 Check the volume status in Longhorn UI Open K9s, Check the pvc status after upgrade Expected Results Can finish the pre-drain of each node and successfully upgrade to v1.</description>
    </item>
    <item>
      <title>Clone image (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</guid>
      <description> Related issues: #2562 [[BUG] Image&amp;rsquo;s labels will not be copied when execute Clone Category: Images Verification Steps Install Harvester with any nodes Create a Image via URL Clone the Image and named image-b Check image-b labels in Labels tab Expected Results All labels should be cloned and shown in labels tab </description>
    </item>
    <item>
      <title>Clone VM and don&#39;t select start after creation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-and-dont-select-start-after-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-and-dont-select-start-after-creation/</guid>
      <description>Case 1 Clone VM from Virtual Machine list and don&amp;rsquo;t select start after creation Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console Case 2 Clone VM with volume from Virtual Machine list and don&amp;rsquo;t select start after creation Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.</description>
    </item>
    <item>
      <title>Clone VM that is turned off</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</guid>
      <description>Case 1 Clone VM from Virtual Machine list that is turned off Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console Case 2 Clone VM with volume from Virtual Machine list that is turned off Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.</description>
    </item>
    <item>
      <title>Clone VM that is turned on</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</guid>
      <description>Case 1 Clone VM from Virtual Machine list that is turned on Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console Case 2 Clone VM with volume from Virtual Machine list that is turned on Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.</description>
    </item>
    <item>
      <title>Clone VM that was created from existing volume</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-existing-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-existing-volume/</guid>
      <description>Case 1 Clone VM from Virtual Machine list that was created from existing volume Expected Results When completing the clone you should get an error that the volume is already in use Case 2 Clone VM with volume from Virtual Machine list that was created from existing volume Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.txt &amp;amp;&amp;amp; sync Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console file test.</description>
    </item>
    <item>
      <title>Clone VM that was created from image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-image/</guid>
      <description>Case 1 Clone VM from Virtual Machine list that was created from image Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console Case 2 Clone VM with volume from Virtual Machine list that was created from image Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.</description>
    </item>
    <item>
      <title>Clone VM that was created from template</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-template/</guid>
      <description>Case 1 Clone VM from Virtual Machine list that was created from template Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console Case 2 Clone VM with volume from Virtual Machine list that was created from template Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.</description>
    </item>
    <item>
      <title>Clone VM that was not created from image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-not-created-from-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-not-created-from-image/</guid>
      <description>Case 1 Clone VM from Virtual Machine list that was not created from image Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console Case 2 Clone VM with volume from Virtual Machine list that was not created from image Expected Results Before cloning machine create file run command echo &amp;quot;123&amp;quot; &amp;gt; test.</description>
    </item>
    <item>
      <title>Cluster add labs</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-add-labs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/create-add-labs/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results Use the command &amp;ldquo;kubectl get node &amp;ndash;show-labels&amp;rdquo; to see the success of the added tabs Go to the node details page of UI, click the &amp;ldquo;Edit Node&amp;rdquo; button, and check Labels Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Cluster add Taints</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-add-taints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-add-taints/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results Use the command kubectl describe node test-tain5 | grep Taint to see if Taint was added successfully. Go to the node details page of UI, click the &amp;ldquo;Edit Node&amp;rdquo; button, and check Taint Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Cluster TLS customization</title>
      <link>https://harvester.github.io/tests/manual/advanced/tls_customize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/tls_customize/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1046&#xA;Verify Items Cluster&amp;rsquo;s SSL/TLS parameters could be configured in install option Cluster&amp;rsquo;s SSL/TLS parameters could be updated in dashboard Case: Configure TLS parameters in dashboard Install Harvester with any nodes Navigate to Advanced Settings, then edit ssl-parameters Select Protocols TLSv1.3, then save execute command echo QUIT | openssl s_client -connect &amp;lt;VIP&amp;gt;:443 -tls1_2 | grep &amp;quot;Cipher is&amp;quot; Output should contain error...SSL routines... and Cipher is (NONE) execute command echo QUIT | openssl s_client -connect &amp;lt;VIP&amp;gt;:443 -tls1_3 | grep &amp;quot;Cipher is&amp;quot; Output should contain Cipher is &amp;lt;one_of_TLS1_3_Ciphers&amp;gt;1 and should not contain error.</description>
    </item>
    <item>
      <title>Cluster with Witness Node</title>
      <link>https://harvester.github.io/tests/manual/hosts/3266-witness-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/3266-witness-node/</guid>
      <description>Witness node is a lightweight node only runs etcd which is not schedulable and also not for workloads. The main use case is to form a quorum with the other 2 nodes.&#xA;Kubernetes need at least 3 etcd nodes to form a quorum, so Harvester also suggests using at least 3 nodes with similar hardware spec. This witness node feature aims for the edge case that user only have 2 powerful + 1 lightweight nodes thus helping benefit both cost and high availability.</description>
    </item>
    <item>
      <title>collect Fleet logs and YAMLs in support bundles</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2297_collect_fleet_logs_and_yamls_in_support_bundles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2297_collect_fleet_logs_and_yamls_in_support_bundles/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2297&#xA;Verify Steps: Install Harvester with any nodes Login to Dashboard then navigate to support page Click Generate Support Bundle and do Generate log files should be exist in the zipfile of support bundle: logs/cattle-fleet-local-system/fleet-agent-&amp;lt;randomID&amp;gt;/fleet-agent.log logs/cattle-fleet-system/fleet-controller-&amp;lt;randomID&amp;gt;/fleet-controller.log logs/cattle-fleet-system/gitjob-&amp;lt;randomID&amp;gt;/gitjob.log </description>
    </item>
    <item>
      <title>Collect system logs</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2647_collect_system_logs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2647_collect_system_logs/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2647&#xA;Verify Steps: Install Graylog via docker[^1] Install Harvester with any nodes Login to Dashboard then navigate to Monitoring &amp;amp; Logging/Logging Create Cluster Output with following: Name: gelf-evts Type: Logging/Event Output: GELF Target: &amp;lt;Graylog_IP&amp;gt;, &amp;lt;Graylog_Port&amp;gt;, &amp;lt;UDP&amp;gt; Create Cluster Flow with following: Name: gelf-flow Type of Matches: Logging Cluster Outputs: gelf-evts Create an Image for VM creation Create a vm vm1 and start it Login to Graylog dashboard then navigate to search Select update frequency New logs should be posted continuously.</description>
    </item>
    <item>
      <title>Config logging in Harvester Dashboard</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2646_config_logging_in_harvester_dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2646_config_logging_in_harvester_dashboard/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2646&#xA;Verify Steps: Install Harvester with any nodes Login to Dashboard then navigate to Monitoring &amp;amp; Logging/Logging Configurations of Fluentbit and Fluentd should be available in Logging/Configuration </description>
    </item>
    <item>
      <title>Configure VLAN interface on ISO installer UI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1647-configure-vlan-interface-on-iso-installer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1647-configure-vlan-interface-on-iso-installer/</guid>
      <description>Related issues: #1647 [FEATURE] Support configuring a VLAN at the management interface in the ISO installer UI Category: Network Harvester Installer Environment Setup Prepare a No VLAN network environment Prepare a VLAN network environment Verification Steps Boot Harvester ISO installer Set VLAN id or keep empty Keep installing Check can complete installation Check harvester has network connectivity Test Plan Matrix Create mode No VLAN DHCP VIP + DHCP node ip DHCP VIP + Static node ip static VIP + DHCP node ip static VIP + Static node ip VLAN DHCP VIP + DHCP node ip DHCP VIP + Static node ip static VIP + DHCP node ip static VIP + Static node ip Join mode No VLAN DHCP VIP + DHCP node ip DHCP VIP + Static node ip static VIP + DHCP node ip static VIP + Static node ip VLAN DHCP VIP + DHCP node ip DHCP VIP + Static node ip static VIP + DHCP node ip static VIP + Static node ip Expected Results Check can complete installation Check harvester has network connectivity ip a show dev mgmt-br [VLAN ID] has IP e.</description>
    </item>
    <item>
      <title>CPU overcommit on VM (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/cpu_overcommit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/cpu_overcommit/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1429&#xA;Verify Items Overcommit can be edit on Dashboard VM can allocate exceed CPU on the host Node VM can chage allocated CPU after created Case: Update Overcommit configuration Install Harvester with any Node Login to Dashboard, then navigate to Advanced Settings Edit overcommit-config The field of CPU should be editable Created VM can allocate maximum CPU should be &amp;lt;HostCPUs&amp;gt; * [&amp;lt;overcommit-CPU&amp;gt;/100] - &amp;lt;Host Reserved&amp;gt; Case: VM can allocate CPUs more than Host have Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostCPUs&amp;gt; * 5 CPUs VM should start successfully lscpu in VM should display allocated CPUs Page of Virtual Machines should display allocated CPUs correctly Case: Update VM allocated CPUs Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostCPUs&amp;gt; * 5 CPUs VM should start successfully Increase/Reduce VM allocated CPUs to minimum/maximum VM should start successfully lscpu in VM should display allocated CPUs Page of Virtual Machines should display allocated CPUs correctly </description>
    </item>
    <item>
      <title>Create a 3 nodes harvester cluster with RKE1 (only with mandatory info, other values stays with default)</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke1/</guid>
      <description> From the Rancher home page, click on Create Select RKE1 on the right and click on Harvester Enter a cluster name Give a prefix name for the VMs Increase count to 3 nodes Check etcd, Control Plane and Worker boxes Select or create a node template if needed Click on Add node template Create credentials by selecting your harvester cluster Fill the instance option fields, pay attention to correctly write the default ssh user of the chosen image in the SSH user field Give a name to the rancher template and click on Create Click on create to spin the cluster up Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The 3 nodes should be with the active status </description>
    </item>
    <item>
      <title>Create a 3 nodes harvester cluster with RKE2 (only with mandatory info, other values stays with default)</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke2/</guid>
      <description> From the Rancher home page, click on Create Select RKE2 on the right and click on Harvester Create the credential to talk with the harvester provider Select your harvester cluster (external or internal) Enter a cluster name Increase machine count to 3 Fill the mandatory fields Namespace Image Network SSH User (default ssh user of the chosen image) Click on create to spin the cluster up Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The 3 nodes should be with the active status </description>
    </item>
    <item>
      <title>Create a harvester cluster and add Taint to a node</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-add-taint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-add-taint/</guid>
      <description>Expected Results </description>
    </item>
    <item>
      <title>Create a harvester cluster with 3 master nodes</title>
      <link>https://harvester.github.io/tests/manual/node-driver/add-3-master-nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/add-3-master-nodes/</guid>
      <description> add a harvester node template Create harvester cluster count set to 3 Expected Results The status of the created cluster shows active show the 3 created node status running in harvester&amp;rsquo;s vm list the information displayed on rancher and harvester matches the template configuration </description>
    </item>
    <item>
      <title>Create a harvester cluster with a non-default version of k8s</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-k8s/</guid>
      <description> Verify versions 1.19.10, 1.18.18, 1.17.17, 1.16.15 respectively Expected Results k8s displayed on the UI is consistent with the created version (cluster list, host list) Use kubectl version to see that the version information is the same as the created version </description>
    </item>
    <item>
      <title>Create a harvester cluster with different images</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-images/</guid>
      <description>d a harvester node template Set the image, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values ubuntu-18.04-server-cloudimg-amd64.img focal-server-cloudimg-amd64-disk-kvm.img Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration The drop-down list of images in the harvester node template corresponds to the list of images in the harvester Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Create a harvester cluster, template drop-down list validation</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-template-dropdown-multi-user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-template-dropdown-multi-user/</guid>
      <description> Create multiple harvester Node Templates with different users Add harvester cluster and set Template Expected Results pop up a template list pop-up box Show the templates you created and the templates created by other users </description>
    </item>
    <item>
      <title>Create a harvester-specific StorageClass for Longhorn</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2692_create_a_harvester-specific_storageclass_for_longhorn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2692_create_a_harvester-specific_storageclass_for_longhorn/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2692&#xA;Verify Steps: Install Harvester with 2+ nodes Login to Dashboard and create an image for VM creation Navigate to Advanced/Storage Classes, harvester-longhorn and longhorn should be available, and harvester-longhorn should be settled as Default Navigate to Volumes and create vol-old where Storage Class is longhorn and vol-new where Storage Class is harvester-longhorn Create VM vm1 attaching vol-old and vol-new Login to vm1 and use fdisk format volumes and mount to folders: old and new Create file and move into both volumes as following commands: dd if=/dev/zero of=file1 bs=10485760 count=10 cp file1 old &amp;amp;&amp;amp; cp file1 new Migrate vm1 to another host, migration should success Login to vm1, volumes should still attaching to folders old and new Execute command sha256sum on old/file1 and new/file1 should show the same value.</description>
    </item>
    <item>
      <title>Create a new VM and add Enable USB tablet option	(e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-enable-usb-tablet-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-enable-usb-tablet-option/</guid>
      <description> Add Enable usb tablet Option Save/Create VM Expected Results Machine starts successfully Enable usb tablet shows In YAML In Form </description>
    </item>
    <item>
      <title>Create a new VM and add Install guest agent option (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-install-guest-agent-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-install-guest-agent-option/</guid>
      <description> Add install Guest Agent Option Save/Create VM Validate that qemu-guest-agent was installed You can do this on ubuntu with the command dpkg -l | grep qemu Expected Results Machine starts successfully Guest Agent Option shows In YAML In Form Guest Agent is installed </description>
    </item>
    <item>
      <title>Create a new VM with Network Data from the form (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-the-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-the-form/</guid>
      <description> Add Network Data to the VM&#xA;Here is an example of Network Data config to add DHCP to the physical interface eth0&#xA;network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM&#xA;Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    <item>
      <title>Create a new VM with Network Data from YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-yaml/</guid>
      <description> Add Network Data to the VM via YAML Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    <item>
      <title>Create a new VM with User Data from the form</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-user-data-from-the-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-user-data-from-the-form/</guid>
      <description> Add User data to the VM Here is an example of user data config to add a password #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM Expected Results Machine starts succesfully User data should exist In YAML In Form Machine should have user password set </description>
    </item>
    <item>
      <title>Create a VM on a VLAN with an existing machine and then change the existing machine&#39;s VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-on-a-vlan-with-an-existing-machine-and-then-change-the-existing-machines-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-on-a-vlan-with-an-existing-machine-and-then-change-the-existing-machines-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    <item>
      <title>Create a VM through the Rancher dashboard</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1613-create-vm-through-rancher-dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1613-create-vm-through-rancher-dashboard/</guid>
      <description> Related issues: #1613 VM memory shows NaN Gi Verification Steps import harvester into rancher&amp;rsquo;s virtualization management Load Harvester dashboard by going to virtualization management then clicking on harvester cluster Create a new VM on Harvester Validate the following in the VM list page, the form, and YAML&amp;gt; Memory CPU Disk space Expected Results VM should create VM should start All specifications should show correctly </description>
    </item>
    <item>
      <title>Create a VM with 2 networks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-2-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-2-networks/</guid>
      <description>Add a network to the VM Save the VM Wait for it to start/restart Expected Results the VM should start successfully The already existing network connectivity should still work The new connectivity should also work Comments one default management network and one VLAN</description>
    </item>
    <item>
      <title>Create a vm with all the default values (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</guid>
      <description> Create a VM with all default values Save Expected Results VM should save VM should start if start after creation checkbox is checked Config should show In Form In YAML </description>
    </item>
    <item>
      <title>Create a VM with Start VM on Creation checked (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-checked/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-checked/</guid>
      <description> Create VM Expected Results VM should start Checkbox for start virtual machine on creation should show as appropriate while editing machine after creation </description>
    </item>
    <item>
      <title>Create a VM with start VM on creation unchecked (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</guid>
      <description> Create VM Expected Results VM should start or not start as appropriate Checkbox for start virtual machine on creation should show as appropriate while editing machine after creation </description>
    </item>
    <item>
      <title>Create Backup Target (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/create-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/create-backup-target/</guid>
      <description> Open up Backup-target in settings Input server info Save Expected Results Backup Target should show in settings </description>
    </item>
    <item>
      <title>Create harvester cluster using non-default CPUs, Memory, Disk</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-resources/</guid>
      <description>add a harvester node template The set CPUs, Memory, and Disk values, refer to &amp;ldquo;Test Data&amp;rdquo; for other values Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:4 Memorys:8 Disk:50 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Create harvester clusters with different Bus</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-bus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-bus/</guid>
      <description>add a harvester node template Set the “Network Name”, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values VirtIO SATA SCSI Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration The drop-down list of &amp;ldquo;BUS&amp;rdquo; in the harvester node template corresponds to the list of “BUS” in the harvester Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Create harvester clusters with different Networks</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-networks/</guid>
      <description>add a harvester node template Set the “Network Name”, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values vlan1 vlan2 Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration The drop-down list of &amp;ldquo;Network Name&amp;rdquo; in the harvester node template corresponds to the list of “Network Name” in the harvester Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Create image from Volume(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-image-from-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/create-image-from-volume/</guid>
      <description>Create new VM Add SSH key Run through iterations for 1, 2, and 3 for attached bash script Export volume to image from volumes page Create new VM from image Run md5sum -c file2.md5 file1-2.md5 file2-2.md5 file3.md5 Expected Results image should upload/complete in images page New VM should create SSH key should work on new VM file2.md5 should fail and the other three md5 checks should pass Comments #!/bin/bash # first file if [ $1 = 1 ] then dd if=/dev/urandom of=file1.</description>
    </item>
    <item>
      <title>Create Images with valid image URL (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/create-images-with-valid-image-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/create-images-with-valid-image-url/</guid>
      <description>Create image with cloud image available for openSUSE. http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 Expected Results Image should show state as Active. Check the backing image in Longhorn Known Bugs https://github.com/harvester/harvester/issues/1269</description>
    </item>
    <item>
      <title>Create multiple instances of the vm with ISO image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-iso-image/</guid>
      <description>Create images using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the 3 vms and wait for vm to start Expected Results 3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes.</description>
    </item>
    <item>
      <title>Create multiple instances of the vm with raw image (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-raw-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-raw-image/</guid>
      <description>Create images using the external path for cloud image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the 3 vms and wait for vm to start. Expected Results 3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes.</description>
    </item>
    <item>
      <title>Create multiple instances of the vm with Windows Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-windows-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-windows-image/</guid>
      <description>Create images using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the 3 vms and wait for vm to start. Expected Results 3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes.</description>
    </item>
    <item>
      <title>Create multiple VM instances using VM template with EFI mode selected</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-create-multiple-vm-using-template-efi-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-create-multiple-vm-using-template-efi-mode/</guid>
      <description>Related issues: #2577 [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected. Category: Virtual Machine Verification Steps Create a VM template, check the Booting in EFI mode Create multiple VM instance and use the VM template have Booting in EFI mode checked Wait for all VM running Check the EFI mode is enabled in VM config ssh to each VM Check the /etc/firmware/efi file Expected Results Can create multiple VM instance using VM template with EFI mode selected</description>
    </item>
    <item>
      <title>Create new network (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/network/create-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/create-network/</guid>
      <description> Navigate to the networks page in harvester Click Create Add a name Add a VLAN ID Click Create Expected Results You should be able to add the VLAN You should see the VLAN show up in the networks page </description>
    </item>
    <item>
      <title>Create new VM with a machine type of PC (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</guid>
      <description> Set up the VM with the appropriate machine type Save/create Expected Results Machine should start sucessfully Machine should show the new machine type in the config and in the YAML </description>
    </item>
    <item>
      <title>Create new VM with a machine type of q35 (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</guid>
      <description> Set up the VM with the appropriate machine type Save/create Expected Results Machine should start sucessfully Machine should show the new machine type in the config and in the YAML </description>
    </item>
    <item>
      <title>Create one VM on a VLAN and then move another VM to that VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-and-then-move-another-vm-to-that-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-and-then-move-another-vm-to-that-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should be able to connect on network This can be verified with a ping over the IP, or via other options if ICMP is disabled </description>
    </item>
    <item>
      <title>Create one VM on a VLAN that has other VMs then change it to a different VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-that-has-other-vms-then-change-it-to-a-different-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-that-has-other-vms-then-change-it-to-a-different-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    <item>
      <title>Create RKE2 cluster with no cloud provider</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1577-create-rke2-cluster-no-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1577-create-rke2-cluster-no-cloud-provider/</guid>
      <description> Related issues: #1577 Option to disable load balancer feature in cloud provider Verification Steps Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select existing cluster in the Imprted Cluster list Click Create Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Select None for cloud provider Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services by clicking manage </description>
    </item>
    <item>
      <title>Create Single instances of the vm with ISO image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-pc/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    <item>
      <title>Create Single instances of the vm with ISO image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    <item>
      <title>Create Single instances of the vm with ISO image with machine type pc</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-q35/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    <item>
      <title>Create Single instances of the vm with raw image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-raw-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-raw-image/</guid>
      <description> Create vm using the external path for cloud image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    <item>
      <title>Create Single instances of the vm with Windows Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-windows-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-windows-image/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    <item>
      <title>Create SSH key from templates page</title>
      <link>https://harvester.github.io/tests/manual/authentication/1619-create-ssh-key-from-templates-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/1619-create-ssh-key-from-templates-page/</guid>
      <description> Related issues: #1619 User is unable to create ssh key through the templates page Verification Steps on a harvester deployment, navigate to advanced -&amp;gt; templates and click create Click create new under SSH section enter valid credentials and save Expected Results SSH key should be created and show in the SSH key section </description>
    </item>
    <item>
      <title>Create SSH key from templates page</title>
      <link>https://harvester.github.io/tests/manual/templates/1619-create-ssh-key-from-templates-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/templates/1619-create-ssh-key-from-templates-page/</guid>
      <description> Related issues: #1619 User is unable to create ssh key through the templates page Verification Steps on a harvester deployment, navigate to advanced -&amp;gt; templates and click create Click create new under SSH section enter valid credentials and save Expected Results SSH key should be created and show in the SSH key section </description>
    </item>
    <item>
      <title>Create support bundle in multi-node Harvester cluster with one node off</title>
      <link>https://harvester.github.io/tests/manual/misc/1524-create-support-bundle-with-one-node-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/1524-create-support-bundle-with-one-node-off/</guid>
      <description> Related issues: #1524 Can&amp;rsquo;t create support bundle if one node is off Verification Steps On a multi-node harvester cluster power off one node Navigate to support create support bundle Expected Results Support bundle should create and be downloaded YOu should be able to extract and examine support bundle </description>
    </item>
    <item>
      <title>Create two VMs in the same VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-in-the-same-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-in-the-same-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should be able to connect on network This can be verified with a ping over the IP, or via other options if ICMP is disabled </description>
    </item>
    <item>
      <title>Create two VMs on separate VLANs</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-separate-vlans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-separate-vlans/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    <item>
      <title>Create two VMs on the same VLAN and change one</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-the-same-vlan-and-change-one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-the-same-vlan-and-change-one/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    <item>
      <title>Create VM and add SSH key (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-and-add-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-and-add-ssh-key/</guid>
      <description> Create VM Add SSH key if not already in VM Logon with SSH Expected Results You should be prompted for SSH key passphrase if appropriate You should connect You should be able to execute shell commands The SSH Key should show in the SSH key list </description>
    </item>
    <item>
      <title>Create vm using a template of default version</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version/</guid>
      <description> Create a new VM with a template of default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    <item>
      <title>Create vm using a template of default version with machine type pc</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-pc/</guid>
      <description> Create a new VM with a template of default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    <item>
      <title>Create vm using a template of default version with machine type q35</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-q35/</guid>
      <description> Create a new VM with a template of default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    <item>
      <title>Create vm using a template of non-default version (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-non-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-non-default-version/</guid>
      <description> Create a new VM with a template of non-default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info </description>
    </item>
    <item>
      <title>Create vm with both CPU and Memory not in cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources Expected Results You should get errors for each resource you over provisioned The VM should not create until errors are resolved </description>
    </item>
    <item>
      <title>Create vm with CPU not in cluster. (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources Expected Results You should get errors for each resource you over provisioned The VM should not create until errors are resolved </description>
    </item>
    <item>
      <title>Create VM with existing Volume (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</guid>
      <description> Create VM with an existing volume Expected Results VM should create and start You should be able to open the console for the VM and see it boot Volume should show in volumes list VM should appear to the &amp;ldquo;Attached VM&amp;rdquo; column of the existing volume </description>
    </item>
    <item>
      <title>Create vm with Memory not in cluster. (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources Expected Results You should get errors for each resource you over provisioned The VM should not create until errors are resolved </description>
    </item>
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster. Expected Results VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</guid>
      <description> Create a VM with resources that are only available on one node in cluster Expected Results VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU and Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</guid>
      <description> Create a VM with resources that are only available on one node in cluster Expected Results VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    <item>
      <title>Create VM with resources that are only on one node in cluster Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster. Expected Results VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    <item>
      <title>Create VM with resources that are only on one node in cluster Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</guid>
      <description> Create a VM with resources that are only available on one node in cluster Expected Results VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    <item>
      <title>Create VM with saved SSH key (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-saved-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-saved-ssh-key/</guid>
      <description> Create VM Add SSH key if not already in VM Logon with SSH Expected Results You should be prompted for SSH key passphrase if appropriate You should connect You should be able to execute shell commands The SSH Key should show in the SSH key list </description>
    </item>
    <item>
      <title>Create VM with the default network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-the-default-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-the-default-network/</guid>
      <description> Create a VM with the default network Let VM boot up after creation Expected Results VM should start VM should be able to ping other machines in the VLAN VM should be able to ping servers on the internet if the VLAN has external access </description>
    </item>
    <item>
      <title>Create VM with two disk volumes (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</guid>
      <description> Create a VM with the appropriate number of volumes Expected Results Verify after creation that the appropriate volumes are in the config for the VM Verify that the volumes are created and listed in the volumes section </description>
    </item>
    <item>
      <title>Create VM without memory provided (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-without-memory-provided/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-without-memory-provided/</guid>
      <description>Related issues: #1477 intimidating error message when missing mandatory field Category Virtual Machine Verification Steps Create some image and volume Create virtual machine Fill out all mandatory field but leave memory blank. Click create Expected Results Leave empty memory field empty when create virtual machine will show &amp;ldquo;Memory is required&amp;rdquo; error message</description>
    </item>
    <item>
      <title>Create Volume root disk blank Form with label</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-blank-form-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-blank-form-label/</guid>
      <description> Navigate to volumes page Click Create Don&amp;rsquo;t select an image Input a size Click Create Expected Results Page should load Volume should create successfully and go to succeeded in the list The label can be seen when you edit the volume config </description>
    </item>
    <item>
      <title>Create volume root disk VM Image Form with label (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form-label/</guid>
      <description> Navigate to volumes page Click Create Select an image Input a size Click Create Expected Results Page should load Volume should create successfully and go to succeeded in the list The label can be seen when you edit the volume config </description>
    </item>
    <item>
      <title>Create volume root disk VM Image Form(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form/</guid>
      <description> Navigate to volumes page Click Create Select an image Input a size Click Create Expected Results VM should create VM should pass health checks </description>
    </item>
    <item>
      <title>Create Windows VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-windows-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-windows-vm/</guid>
      <description>Create a VM with the VM template with windows-iso-image-base-temp Config the CPU and Memory to 4 and 8 respectively Select the windows ISO image Click the Volumes tab and update the root disk size to 50GB Click create to launch the windows VM Optional: you can increase the second disk size or add an additional one. Click create to launch the VM (this will take a couple of minutes upon your network speed of download the ISO image) Click the Console to launch a VNC console of the windows server, and you will need to find an evaluation key of the windows server 2012 installation.</description>
    </item>
    <item>
      <title>Create with invalid image (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/negative-create-with-invalid-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/negative-create-with-invalid-image/</guid>
      <description> Create image with invalid URL. e.g. - https://test.img Expected Results Image state show as Failed </description>
    </item>
    <item>
      <title>Dashboard Storage usage display when node disk have warning</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2622-dashboard-storage-usage-display-when-node-disk-have-warning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2622-dashboard-storage-usage-display-when-node-disk-have-warning/</guid>
      <description>Related issues: #2622 [BUG] Dashboard Storage used is wrong when a node disk is warning Category: Storage Verification Steps Login harvester dashboard Access Longhorn UI from url https://192.168.122.136/dashboard/c/local/longhorn Go to Node page Click edit node and disks Select disabling Node scheduling Select disabling storage scheduling on the bottom Open Longhorn dashboard page, check the Storage Schedulable Open Harvester dashboard page, check the used and scheduled storage size Expected Results After disabling the node and storage scheduling on Longhorn UI.</description>
    </item>
    <item>
      <title>datavolumes.cdi.kubevirt.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/datavolumes.cdi.kubevirt.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/webhooks/datavolumes.cdi.kubevirt.io/</guid>
      <description>GUI Create a VM in GUI and wait until it&amp;rsquo;s running. Assume its name is test. kube-api Try to delete its datavolume: $ kubectl get vms NAME AGE STATUS READY test 5m16s Running True There should be an datavolume bound to that VM $ kubectl get dvs NAME PHASE PROGRESS RESTARTS AGE test-disk-0-klrft Succeeded 100.0% 5m18s The user should not be able to delete the datavolume $ kubectl delete dv test-disk-0-klrft The request is invalid: : can not delete the volume test-disk-0-klrft which is currently attached to VMs: default/test `` ## Expected Results ### kube-api The deletion of its datavolume should fail.</description>
    </item>
    <item>
      <title>Deactivate/activate/delete Harvester Node Driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/deactivate-activate-deletenode-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/deactivate-activate-deletenode-driver/</guid>
      <description>With Rancher &amp;lt; 2.6:&#xA;Tools-&amp;gt;Driver Management→Node Driver Deactivate/activate/delete Harvester Node Driver With Rancher 2.6: Cluster Management &amp;gt; Drivers &amp;gt; Node Drivers Deactivate/activate/delete Harvester Node Driver Expected Results Harvester icon is not visible when creating a cluster / Harvester icon is visible when creating a cluster /Harvester icon is not visible when creating a cluster </description>
    </item>
    <item>
      <title>Dedicated storage network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1055_dedicated_storage_network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1055_dedicated_storage_network/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1055&#xA;Verified this feature has been implemented partially. Mentioned problem in https://github.com/harvester/harvester/issues/1055#issuecomment-1283754519 will be introduced as a enhancement in #2995&#xA;Test Information Environment: baremetal DL360G9 5 nodes Harvester Version: master-bd1d49a9-head ui-source Option: Auto Verify Steps: Install Harvester with any nodes Navigate to Networks -&amp;gt; Cluster Networks/Configs, create Cluster Network named vlan Navigate to Advanced -&amp;gt; Settings, edit storage-network Select Enable then select vlan as cluster network, fill in VLAN ID and IP Range Click Save, warning or error message should displayed.</description>
    </item>
    <item>
      <title>Delete 3 node RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1311-delete-3-node-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1311-delete-3-node-rke2-cluster/</guid>
      <description> Related issues: #1311 Deleting a cluster in rancher dashboard doesn&amp;rsquo;t fully remove Verification Steps Create 3 node RKE2 cluster on Harvester through node driver with Rancher Wait fo the nodes to create, but not fully provision Delete the cluster Wait for them to be removed from Harvester Check Rancher cluster management Expected Results Cluster should be removed from Rancher VMs should be removed from Harvester </description>
    </item>
    <item>
      <title>Delete backup from backups list (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-single-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-single-backup/</guid>
      <description> Delete backup from backups list Expected Results Backup should be removed from list Backup should be removed from remote storage </description>
    </item>
    <item>
      <title>Delete Cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-delete/</guid>
      <description> Delete Cluster Expected Results successful cluster deletion in rancher the corresponding VM node in harvester is deleted successfully </description>
    </item>
    <item>
      <title>Delete external VLAN network via form</title>
      <link>https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the external VLAN via the web form Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the external VLAN You should get responses from the VM </description>
    </item>
    <item>
      <title>Delete external VLAN network via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/delete-vlan-network-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/delete-vlan-network-yaml/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the external VLAN via YAML Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the external VLAN You should get responses from the VM </description>
    </item>
    <item>
      <title>Delete first backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-first-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-first-backup-chained-backup/</guid>
      <description> Create a new VM Create a file named 1 and add text Create a backup Edit text in file 1 create file 2 Create Backup Edit file 2 text Create file 3 and add text Create backup Delete backup 1 Validate file 2 and 3 are the same as they were Restore to backup 2 Validate that md5sum -c file1-2.md5 file2.md5 file3.md5 file 1 is in second format file 2 is in first format file 3 doesn&amp;rsquo;t exist Expected Results Vm should create All file operations should create Backup should run All file operations should create Backup should run All file operations should create files should be as expected </description>
    </item>
    <item>
      <title>Delete Host (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host/</guid>
      <description> Navigate to the Hosts page and select the node Click Delete Expected Results SSH to the node and check the nodes has components deleted. </description>
    </item>
    <item>
      <title>Delete host that has VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</guid>
      <description>Navigate to the Hosts page and select the node Click Delete Expected Results An alert message should appear. If VM exists it should stop user to delete the node or move VM to other node. If VM is getting moved to another node and there is no space, it should stop user to delete the node. Existing bugs https://github.com/harvester/harvester/issues/1004</description>
    </item>
    <item>
      <title>Delete last backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-last-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-last-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup delete backup 3 Validate that files didn&amp;rsquo;t change Restore to backup 2 Validate that md5sum -c file1-2.</description>
    </item>
    <item>
      <title>Delete management network via form</title>
      <link>https://harvester.github.io/tests/manual/network/delete-management-network-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/delete-management-network-form/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the management VLAN via the web form Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the management VLAN You should get responses from the VM </description>
    </item>
    <item>
      <title>Delete management network via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/delete-management-network-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/delete-management-network-yaml/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the management network via YAML Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the management network You should get responses from the VM </description>
    </item>
    <item>
      <title>Delete middle backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-middle-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-middle-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Delete backup 2 Validate file 2 and 3 are the same as they were Restore to backup 1 Validate that md5sum -c file1.</description>
    </item>
    <item>
      <title>Delete multiple backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-multiple-backups/</guid>
      <description> Select multiple Backups from Backups list Click Delete Expected Results Backups should be removed from list Backups should be removed from remote storage </description>
    </item>
    <item>
      <title>Delete multiple VMs with disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-with-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-with-disks/</guid>
      <description> Delete VM Select whether you want to delete disks Expected Results You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node. </description>
    </item>
    <item>
      <title>Delete multiple VMs without disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-without-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-without-disks/</guid>
      <description> Delete VM Select whether you want to delete disks Expected Results You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node. </description>
    </item>
    <item>
      <title>Delete single vm all disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-single-vm-all-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-single-vm-all-disks/</guid>
      <description> Delete VM Select whether you want to delete disks Expected Results You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node. </description>
    </item>
    <item>
      <title>Delete the image	(e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/delete-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/delete-image/</guid>
      <description> Select an image with state active. Delete the image. Create another image with same name. Delete the newly created image. Delete an image with failed state Expected Results The image should be deleted successfully. Check the CRDS VirtualMachineImage. User should be able to create a new image with same name. Check the backing image in Longhorn. </description>
    </item>
    <item>
      <title>Delete VM Negative (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-delete-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-delete-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Delete VM and all disks Expected Results You should not be able to delete the VM </description>
    </item>
    <item>
      <title>Delete VM template default version (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2376-2379-delete-vm-template-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2376-2379-delete-vm-template-default-version/</guid>
      <description> Related issues: #2376 [BUG] Cannot delete Template Related issues: #2379 [backport v1.0.3] Cannot delete Template Category: VM Template Verification Steps Go to Advanced -&amp;gt; Templates Create a new template Modify the template to create a new version Click the config button of the default version template Click the config button of the non default version template Expected Results If the template is the default version, it will not display the delete button If the template is not the default version, it will display the delete button We can also delete the entire template from the config button </description>
    </item>
    <item>
      <title>Delete VM with exported image (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1602-delete-vm-with-exported-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1602-delete-vm-with-exported-image/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; delete image &amp;ldquo;img-1&amp;rdquo; Expected Results image &amp;ldquo;img-1&amp;rdquo; will be deleted </description>
    </item>
    <item>
      <title>Delete VM with exported image(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/1602-delete-vm-with-exported-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/1602-delete-vm-with-exported-image/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; delete image &amp;ldquo;img-1&amp;rdquo; Expected Results image &amp;ldquo;img-1&amp;rdquo; will be deleted </description>
    </item>
    <item>
      <title>Delete volume that is not attached to a VM (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/delete-volume-that-is-not-attached-to-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/delete-volume-that-is-not-attached-to-vm/</guid>
      <description> Create volume Validate that it created Check the volume crd. Delete the volume Verify that volume is removed from list Check the volume object doesn&amp;rsquo;t exist anymore. Expected Results Volume should create It should show in volume list Volume crd should have correct info. Volume should delete. Volume should be removed from list </description>
    </item>
    <item>
      <title>Delete volume that was attached to VM but now is not (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/delete-volume-that-was-attached-to-vm-but-is-not-now/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/delete-volume-that-was-attached-to-vm-but-is-not-now/</guid>
      <description> Create a VM with a root volume Write 10Gi data into it. Delete the VM but not the volume Verify Volume still exists Check disk space on node Delete the volume Verify that volume is removed from list Check disk space on node Expected Results VM should create 10Gi space should be consumed on the disk. VM should delete Volume should still show in Volume list Disk space should show 10Gi + Volume should delete Volume should be removed from list Space should be less than before </description>
    </item>
    <item>
      <title>Deny the vlanconfigs overlap with the other</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2828-deny-vlanconfig-overlap-others/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2828-deny-vlanconfig-overlap-others/</guid>
      <description>Related issues: #2828 [BUG][FEATURE] Deny the vlanconfigs overlap with the other Category: Network Verification Steps Prepare a 3 nodes Harvester on local kvm Each VM have five NICs attached. Create a cluster network cn1 Create a vlanconfig config-all which applied to all nodes Set one of the NIC On the same cluster network, create another vlan network config-one which applied to only node 1 Provide another NIC Click the create button Expected Results Under the same Cluster Network:</description>
    </item>
    <item>
      <title>Deploy guest cluster to specific node with Node selector label</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2316-2384-deploy-guest-cluster-node-selector-label-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2316-2384-deploy-guest-cluster-node-selector-label-copy/</guid>
      <description>Related issues: #2316 [BUG] Guest cluster nodes distributed across failure domain Related issues: #2384 [backport v1.0.3] Guest cluster nodes distributed across failure domains Category: Rancher integration Verification Steps RKE2 Verification Steps Open Harvester Host page then edit host config Add the following key value in the labels page: topology.kubernetes.io/zone: zone_bp topology.kubernetes.io/region: region_bp Open the RKE2 provisioning page Expand the show advanced Click add Node selector in Node scheduling Use default Required priority Click Add Rule Provide the following key/value pairs topology.</description>
    </item>
    <item>
      <title>Detach volume from virtual machine (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/detach-volume-from-virtual-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/detach-volume-from-virtual-machine/</guid>
      <description>Related issues: #1708 After click &amp;ldquo;Detach volume&amp;rdquo; button, nothing happend Category: Volume Verification Steps Create several new volume in volumes page Create a virtual machine Click the config button on the selected virtual machine Click Add volume and add at least two new volume Click the Detach volume button on the attached volume Repeat above steps several times Expected Results Currently when click the Detach volume button, attached volume can be detach successfully.</description>
    </item>
    <item>
      <title>Disable and enable vlan cluster network</title>
      <link>https://harvester.github.io/tests/manual/network/disable-and-enable-vlan-cluster-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/disable-and-enable-vlan-cluster-network/</guid>
      <description> Related issues: #1529 Failed to enable vlan cluster network after disable and enable again, display &amp;ldquo;Network Error&amp;rdquo; Category: Network Verification Steps Open settings and config vlan network Enable network and set default harvester-mgmt Disable network Enable network again Check Host, Network and harvester dashboard Repeat above steps several times Expected Results User can disable and enable network with default harvester-mgmt. Harvester dashboard and network work as expected </description>
    </item>
    <item>
      <title>Disk can only be added once on UI</title>
      <link>https://harvester.github.io/tests/manual/hosts/add_disk_on_ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/add_disk_on_ui/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1608&#xA;Verify Items NVMe disk can only be added once on UI Case: add new NVMe disk on dashboard UI Install Harvester with 2 nodes Power off 2nd node Update VM&amp;rsquo;s xml definition (by using virsh edit or virt-manager) Create nvme.img block: dd if=/dev/zero of=/var/lib/libvirt/images/nvme.img bs=1M count=4096 change owner chown qemu:qemu /var/lib/libvirt/images/nvme.img update &amp;lt;domain type=&amp;quot;kvm&amp;quot;&amp;gt; to &amp;lt;domain type=&amp;quot;kvm&amp;quot; xmlns:qemu=&amp;quot;http://libvirt.org/schemas/domain/qemu/1.0&amp;quot;&amp;gt; append xml node into domain as below: &amp;lt;qemu:commandline&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/var/lib/libvirt/images/nvme.</description>
    </item>
    <item>
      <title>Disk devices used for VM storage should be globally configurable</title>
      <link>https://harvester.github.io/tests/manual/hosts/disk-devices-used-for-vm-storage-globally-configurable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/disk-devices-used-for-vm-storage-globally-configurable/</guid>
      <description>Related issue: #1241 Disk devices used for VM storage should be globally configurable&#xA;Related issue: #1382 Exclude OS root disk and partitions on forced GPT partition&#xA;Related issue: #1599 Extra disk auto provision from installation may cause NDM can&amp;rsquo;t find a valid longhorn node to provision&#xA;Category: Storage Test Scenarios (Checked means verification PASS)&#xA;BIOS firmware + No MBR (Default) + Auto disk` provisioning config BIOS firmware + MBR + Auto disk provisioning config UEFI firmware + GPT (Default) + Auto disk provisioning config BIOS firmware + GPT (Default) +Auto Provisioning on harvester-config Environment setup Scenario 1: Node type: Create</description>
    </item>
    <item>
      <title>Download backing images</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1436__allowing_users_to_download_backing_images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1436__allowing_users_to_download_backing_images/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1436&#xA;Verify Steps: Install Harvester with any nodes Create a Image img1 Click the details of img1, Download Button should be available Click Download button, img1 should able to be downloaded and downloaded successfully. </description>
    </item>
    <item>
      <title>Download host YAML</title>
      <link>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</guid>
      <description> Navigate to the Hosts page and select the node Click Download Yaml Expected Results The Yaml should get downloaded. </description>
    </item>
    <item>
      <title>Download kubeconfig after shutting down harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/misc/download-kubeconfig-after-shutting-down-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/download-kubeconfig-after-shutting-down-harvester-cluster/</guid>
      <description>Related issues: #1475 After shutting down the cluster the kubeconfig becomes invalid Category: Host Verification Steps Shutdown harvester node 3, wait for fully power off&#xA;Shutdown harvester node 2, wait for fully power off&#xA;Shutdown harvester node 1, wait for fully power off&#xA;Wait for more than hours or over night&#xA;Power on node 1 to console page until you see management url Power on node 2 to console page until you see management url</description>
    </item>
    <item>
      <title>Edit a VM and add install Enable usb tablet option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-enable-usb-tablet-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-enable-usb-tablet-option/</guid>
      <description> Add Enable usb tablet Option Save/Create VM Expected Results Machine starts successfully Enable usb tablet shows In YAML In Form </description>
    </item>
    <item>
      <title>Edit a VM and add install guest agent option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-guest-agent-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-guest-agent-option/</guid>
      <description> Add install Guest Agent Option Save/Create VM Expected Results Machine starts successfully Guest Agent Option shows In YAML In Form Guest Agent is installed </description>
    </item>
    <item>
      <title>Edit a VM from the form to add Network Data</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-network-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-network-data/</guid>
      <description> Add Network Data to the VM Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    <item>
      <title>Edit a VM from the form to add user data (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-user-data/</guid>
      <description> Add User data to the VM&#xA;Here is an example of user data config to add a password `` #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM&#xA;Expected Results Machine starts succesfully User data should In YAML In Form Machine should have user password set </description>
    </item>
    <item>
      <title>Edit a VM from the YAML to add Network Data (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-network-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-network-data/</guid>
      <description> Add Network Data to the VM Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    <item>
      <title>Edit a VM from the YAML to add user data (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-user-data/</guid>
      <description> Add User data to the VM Here is an example of user data config to add a password `` #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM Expected Results Machine starts succesfully User data should In YAML In Form Machine should have user password set </description>
    </item>
    <item>
      <title>Edit an existing VM to another machine type (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-an-existing-vm-to-another-machine-type/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-an-existing-vm-to-another-machine-type/</guid>
      <description> Set up the VM with the appropriate machine type Save/create Expected Results Machine should start sucessfully Machine should show the new machine type in the config and in the YAML </description>
    </item>
    <item>
      <title>Edit backup read YAML from file</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-read-yaml-from-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-read-yaml-from-file/</guid>
      <description> Edit YAML for backup Read from File Show Diff Save Expected Results Diff should show changes Backup should be updated </description>
    </item>
    <item>
      <title>Edit backup via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-yaml/</guid>
      <description> Edit YAML for backup Show Diff Save Expected Results Diff should show changes Backup should be updated </description>
    </item>
    <item>
      <title>Edit Config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config/</guid>
      <description> Navigate to the Hosts page and select the node Click edit config. Add description and other details Try to modify the network config Expected Results The edited values should be saved and reflected on the page. </description>
    </item>
    <item>
      <title>Edit Config YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</guid>
      <description> Navigate to the Hosts page and select the node Click edit config through YAML. Add description and other details Try to modify the network config Expected Results The edited values should be saved and reflected on the page. </description>
    </item>
    <item>
      <title>Edit images (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/edit-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/edit-images/</guid>
      <description> Edit image. Try to edit the description Try to edit the URL Try to edit the Labels Expected Results User should be able to edit the description and Labels User should not be able to edit the URL </description>
    </item>
    <item>
      <title>Edit network via form change external VLAN to management network</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-form-change-vlan-to-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/edit-network-form-change-vlan-to-management/</guid>
      <description> Edit VM and change external VLAN to management network with bridge type via the web form Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    <item>
      <title>Edit network via form change management network to external VLAN</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</guid>
      <description> Edit VM and change management network to external VLAN with bridge type via the web form Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    <item>
      <title>Edit network via YAML change external VLAN to management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-vlan-to-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-vlan-to-management/</guid>
      <description> Edit VM and change external VLAN to management network with bridge type via YAML Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    <item>
      <title>Edit network via YAML change management network to external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-management-to-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-management-to-vlan/</guid>
      <description> Edit VM and change management network to external VLAN with bridge type via YAML Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    <item>
      <title>Edit vm and insert ssh and check the ssh key is accepted for the login (e2e_be_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-and-insert-ssh-and-check-the-ssh-key-is-accepted-for-the-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-and-insert-ssh-and-check-the-ssh-key-is-accepted-for-the-login/</guid>
      <description> Edit VM and add SSH Key Save VM Expected Results You should be able to ssh in with correct SSH private key You should not be able to SSH in with incorrect SSH private key </description>
    </item>
    <item>
      <title>Edit vm config after Eject CDROM and delete volume</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/5264-edit-vm-config-after-eject-cdrom-delete-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/5264-edit-vm-config-after-eject-cdrom-delete-volume/</guid>
      <description> Related issues: #5264 [BUG] After EjectCD from vm and edit config of vm displays empty page: &amp;ldquo;Cannot read properties of null&amp;rdquo; Category: Virtual Machines Verification Steps Upload the ISO type desktop image (e.g ubuntu-20.04.4-desktop-amd64.iso) Create a vm named vm1 with the iso image Open the web console to check content Click EjectCD after vm running Select the delete volume option Wait until vm restart to running Click the edit config Back to the virtual machine page Click the vm1 name Expected Results Check can edit vm config of vm1 to display all settings correctly Check can display the current vm1 settings correctly </description>
    </item>
    <item>
      <title>Edit VM Form Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-edit-vm-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-edit-vm-form/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Edit the VM via form Save the VM Expected Results You should not be able to save the edited Form You should get an error </description>
    </item>
    <item>
      <title>Edit vm network and verify the network is working as per configuration (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-network-and-verify-the-network-is-working-as-per-configuration-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-network-and-verify-the-network-is-working-as-per-configuration-/</guid>
      <description> Edit VM network Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML Network should function as desired </description>
    </item>
    <item>
      <title>Edit VM via form with CPU</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    <item>
      <title>Edit VM via form with CPU and Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu-and-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    <item>
      <title>Edit VM via form with Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    <item>
      <title>Edit VM via YAML with CPU (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    <item>
      <title>Edit VM via YAML with CPU and Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu-and-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    <item>
      <title>Edit VM via YAML with Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    <item>
      <title>Edit VM with resources that are only on one node in cluster CPU and Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster. Expected Results VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    <item>
      <title>Edit VM YAML Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/q-negative-edit-vm-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/q-negative-edit-vm-yaml/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Edit the VM via YAML Save the VM Expected Results SSH to the node and check the nodes has components deleted. </description>
    </item>
    <item>
      <title>Edit volume decrease size via YAML</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-decrase-size-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-decrase-size-yaml/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Decrease size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    <item>
      <title>Edit volume decrease size via YAML</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-decrease-size-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-decrease-size-form/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Decrease size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    <item>
      <title>Edit Volume Form add label</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-form-add-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-form-add-label/</guid>
      <description> Navigate to volumes page Edit Volume with Form Click Labels Add label Click Save Open VM again and click the config tab Verify that label was saved Expected Results Volume should save Label should add Label should show when re-opened </description>
    </item>
    <item>
      <title>Edit volume increase size via form (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-form/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume via form Increase size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    <item>
      <title>Edit volume increase size via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-yaml/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Increase size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    <item>
      <title>Edit Volume YAML add label (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-yaml-add-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-yaml-add-label/</guid>
      <description> Navigate to volumes page Edit Volume as YAML Add label to config Click Save Open VM again and click the config tab Verify that label was saved Expected Results Volume should save Label should add Label should show when re-opened </description>
    </item>
    <item>
      <title>Enable Harvester addons and check deployment state</title>
      <link>https://harvester.github.io/tests/manual/advanced/addons/5337-enable-addons-check-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/addons/5337-enable-addons-check-deployment/</guid>
      <description> Related issues: #5337 [BUG] Failed to enable vm-importer, pcidevices and harvester-seeder controller addons, keep stuck in &amp;ldquo;Enabling&amp;rdquo; state Category: Addons Verification Steps Prepare three nodes Harvester cluster Open Advanced -&amp;gt; Addons page Access to harvester node machine Switch to root user and open k9s Enable the vm-importer, pci-devices and harvester-seeder addons Check the corresponding jobs and logs Enable rest of the addons nvidia-driver-toolkit, rancher-monitoring and rancher-logging Expected Results Check the vm-importer, pci-devices and harvester-seeder display in Deployment Successful Check the vm-importer-controller, pci-devices-controller and harvester-seeder jobs and the related helm-install chart job all running well on the K9s Check the nvidia-driver-toolkit, rancher-monitoring and rancher-logging display in Deployment Successful Check the nvidia-driver-toolkit, rancher-monitoring and rancher-logging jobs and the related helm-install chart job all running well on the K9s </description>
    </item>
    <item>
      <title>enable/disable alertmanager on demand</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2518_enabledisable_alertmanager_on_demand/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2518_enabledisable_alertmanager_on_demand/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2518&#xA;Verify Steps: Install Harvester with any nodes Login to Dashboard, navigate to Monitoring &amp;amp; Logging/Monitoring/Configuration then select Alertmanager tab Option Button Enabled should be checked Select Grafana tab then access Grafana Search Alertmanager to access Overview dashboard Data should be available and keep updating </description>
    </item>
    <item>
      <title>Enabling and Tuning KSM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2302_enabling_and_tuning_ksm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2302_enabling_and_tuning_ksm/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2302&#xA;Verify Steps: Install Harvester with any nodes Login to Dashboard and Navigate to hosts Edit node1&amp;rsquo;s Ksmtuned to Run and ThresCoef to 85 then Click Save Login to node1&amp;rsquo;s console, execute kubectl get ksmtuned -oyaml --field-selector metadata.name=&amp;lt;node1&amp;gt; Fields in spec should be the same as Dashboard configured Create an image for VM creation Create multiple VMs with 2Gi+ memory and schedule on &amp;lt;node1&amp;gt; (memory size reflect to &amp;rsquo;s maximum size, total of VMs&amp;rsquo; memory should greater than 40%) Execute watch -n1 grep .</description>
    </item>
    <item>
      <title>Enabling vlan on a bonded NIC on vagrant install</title>
      <link>https://harvester.github.io/tests/manual/network/enabling-vlan-on-bonded-nic-vagrant-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/enabling-vlan-on-bonded-nic-vagrant-install/</guid>
      <description>Related issues: #1541 Enabling vlan on a bonded NIC breaks the Harvester setup Category: Network Verification Steps Pull ipxe example from https://github.com/harvester/ipxe-examples Vagrant pxe install 3 nodes harvester Access harvester settings page Open settings -&amp;gt; vlan Enable virtual network and set with bond0 Navigate to every page to check harvester is working Create a vlan based on bon0 Expected Results Enable virtual network with bond0 will not make harvester service out of work.</description>
    </item>
    <item>
      <title>enhance double check of VM&#39;s resource modification</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2869_enhance_double_check_of_vms_resource_modification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2869_enhance_double_check_of_vms_resource_modification/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2869&#xA;Verify Steps: Install Harvester with any nodes Create an Image for VM creation Create VM vm1 Imitate video recording (as below) to test https://user-images.githubusercontent.com/5169694/193790263-19379641-e282-445f-831f-8da039c15e77.mp4</description>
    </item>
    <item>
      <title>enhance node scheduling when vm selects network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2982_enhance_node_scheduling_when_vm_selects_network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2982_enhance_node_scheduling_when_vm_selects_network/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2982&#xA;Criteria Scheduling rule added automatically when select specific network Verify Steps: go to Cluster Networks / Config page, create a new Cluster Network (eg: test) Create a new network config in the test Cluster Network. (Select a specific node) go to Network page to create a new network (e.g: test-untagged), select UntaggedNetwork type and select test cluster network. click Create button go to VM create page, fill all required value, Click Networks tab, select default/test-untagged network, click Create button The VM is successfully created, but the scheduled node may not match the Network Config !</description>
    </item>
    <item>
      <title>Filter backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/filter-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/filter-backups/</guid>
      <description> Enter in string in filter input field Columns available for matching: State &amp;ldquo;Ready&amp;rdquo; &amp;ldquo;Progressing&amp;rdquo; Name Target VM With string With matching string Input Clear With non-matching string Input Clear Clear String Expected Results List should filter based on string List should re-populate after clearing string </description>
    </item>
    <item>
      <title>First Time Login	 (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/authentication/first-time-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/first-time-login/</guid>
      <description> After successful installation of Harvester using Iso, on navigating to UI, user should be prompted to change the password. Verify the password rules Expected Results User should be able to login </description>
    </item>
    <item>
      <title>Fleet support with Harvester</title>
      <link>https://harvester.github.io/tests/manual/advanced/fleet-support-with-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/fleet-support-with-harvester/</guid>
      <description>Fleet Support Pathways Fleet Support is enabled out of the box with Harvester, no Rancher integration needed, as Fleet Support does not need any Rancher integration to function Fleet Support can be used from within Rancher w/ Harvester Fleet Support w/ Rancher Prerequisites Harvester cluster is imported into Rancher. Rancher Feature Flag harvester-baremetal-container-workload is enabled. Harvester cluster is available to view via the Explore Cluster section of Rancher. Explore the Harvester cluster: Toggle &amp;ldquo;All Namespaces&amp;rdquo; to be selected Search for &amp;amp; &amp;ldquo;star&amp;rdquo; (marking favorite for ease of navigation): Git Repo Git Job Git Restrictions Fleet Support w/out Rancher Prerequisites An active Harvester Cluster Kubeconfig Additional Prerequisites Fork ibrokethecloud&amp;rsquo;s Harvester Fleet Demo into your own personal GitHub Repository Take a look at the different Harvester API Resources as YAML will be scaffolded to reflect those objects respectively Additional Prerequisites Airgapped, if desired Have an Airgapped GitLab Server Running somewhere with a Repo that takes the shape of ibrokethecloud&amp;rsquo;s Harvester Fleet Demo (setting up AirGapped GitLab Server is outside of this scope) Additional Prerequisites (Private Repository Testing), if desired Private Git Repo Key, will need to be added to -n fleet-local namespace Build a private GitHub Repo Add similar content to what ibrokethecloud&amp;rsquo;s Harvester Fleet Demo holds but take into consideration the following ( references: GitRepo CRD &amp;amp; Rancher Fleet Private Git Repo Blurb ): building a &amp;ldquo;separate&amp;rdquo; SINGLE REPOSITORY ONLY (zero-trust based) SSH Key Via something like: ssh-keygen -t rsa -b 4096 -m pem -C &amp;#34;testing-test-key-for-private-repo-deploy-key@email.</description>
    </item>
    <item>
      <title>Function keys on web VNC interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1461-function-keys-on-web-vnc-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1461-function-keys-on-web-vnc-interface/</guid>
      <description>Related issues: #1461 [UI] F keys and Alt-F keys in web VNC interface Category: Network Verification Steps Create a new VM with Ubuntu desktop 20.04 Prepare two volume Complete the installation process Open a web browser on Ubuntu desktop Check the shortcut keys combination Expected Results Check the soft shortcut keys can display and work correctly on Linux OS VM (Ubuntu desktop 20.04) Checked the following short cut can work as expected</description>
    </item>
    <item>
      <title>Generate Install Support Config Bundle For Single Node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</guid>
      <description>Related issue: #1864 Support bundle for a single node (Live/Installed)&#xA;Related issue: #272 Generate supportconfig for failed installations&#xA;Category: Support Environment setup Setup a single node harvester from ISO install but don&amp;rsquo;t complete the installation&#xA;Gain SSH Access to the Single Harvester Node Once Shelled into the Single Harvester Node edit the /usr/sbin/harv-install Using: harvester-installer&amp;rsquo;s harv-install as a reference edit around line #362 adding exit 1: exit 1 trap cleanup exit check_iso save the file.</description>
    </item>
    <item>
      <title>Guest CSI Driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/guest-csi-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/guest-csi-driver/</guid>
      <description>Start rancher using docker in a vm and start harvester in another Import harvester into rancher from &amp;ldquo;Virtualization Management&amp;rdquo; page On rancher, enable harvester node driver at &amp;ldquo;Cluster Management&amp;rdquo; -&amp;gt; &amp;ldquo;Drivers&amp;rdquo; -&amp;gt; &amp;ldquo;Node Driver&amp;rdquo; Go back to &amp;ldquo;Cluster Management&amp;rdquo; and create a rke2 cluster using Harvester Once the created cluster is active on the &amp;ldquo;Cluster Management&amp;rdquo; page, click on the &amp;ldquo;Explore&amp;rdquo; Go to &amp;ldquo;Workload&amp;rdquo; -&amp;gt; &amp;ldquo;Deployment&amp;rdquo; and &amp;ldquo;Create&amp;rdquo; a new deployment, during which in the page of &amp;ldquo;Storage&amp;rdquo;, click on &amp;ldquo;Add Volume&amp;rdquo; and select &amp;ldquo;Create Persistent Volume Claim&amp;rdquo; and select &amp;ldquo;Harvester&amp;rdquo; in the &amp;ldquo;Storage Class&amp;rdquo; Click &amp;ldquo;Create&amp;rdquo; to create the deployment Verify that on the Harvester side, a new volume is created.</description>
    </item>
    <item>
      <title>Harvester Cloud Provider compatibility check</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2753-harvester-cloud-provider-compatibility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2753-harvester-cloud-provider-compatibility/</guid>
      <description>Related issues: #2753 [FEATURE] Harvester Cloud Provider compatibility check enhancement Category: Rancher Integration Verification Steps Open Rancher Global settings Edit the rke-metadata-config Change the default url to https://harvester-dev.oss-cn-hangzhou.aliyuncs.com/Untitled-1.json which include the following cloud provider and csi-driver chart changes &amp;#34;charts&amp;#34;: { &amp;#34;harvester-cloud-provider&amp;#34;: { &amp;#34;repo&amp;#34;: &amp;#34;rancher-rke2-charts&amp;#34;, &amp;#34;version&amp;#34;: &amp;#34;1.1.0&amp;#34; }, &amp;#34;harvester-csi-driver&amp;#34;: { &amp;#34;repo&amp;#34;: &amp;#34;rancher-rke2-charts&amp;#34;, &amp;#34;version&amp;#34;: &amp;#34;1.1.0&amp;#34; }, Save and reload page Open the create RKE2 cluster page Select the incomparable RKE2 version Check the Cloud provider drop down Enable Harvester API in Preference -&amp;gt; Enable Developer Tools &amp;amp; Features Open settings Click view API of any setting Click up open the id&amp;quot;: &amp;ldquo;harvester-csi-ccm-versions&amp;rdquo; Or directly access https://192.</description>
    </item>
    <item>
      <title>Harvester pull Rancher agent image from private registry</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2175-2332-harvester-pull-rancher-image-private-registry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2175-2332-harvester-pull-rancher-image-private-registry/</guid>
      <description>Related issues: #2175 [BUG] Harvester fails to pull Rancher agent image from private registry Related issues: #2332 [Backport v1.0] Harvester fails to pull Rancher agent image from private registry Category: Virtual Machine Verification Steps Create a harvester cluster and a ubuntu server. Make sure they can reach each other. On each harvester node, add ubuntu IP to /etc/hosts. # vim /etc/hosts &amp;lt;host ip&amp;gt; myregistry.local On the ubuntu server, install docker and run the following commands.</description>
    </item>
    <item>
      <title>Harvester rebase check on SLE Micro</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1933-2420-harvester-rebase-check-on-sle-micro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1933-2420-harvester-rebase-check-on-sle-micro/</guid>
      <description>Related issues: #1933 [FEATURE] Rebase Harvester on SLE Micro for Rancher&#xA;Related issues: #2420 [FEATURE] support bundle: support SLE Micro OS&#xA;Category: System Verification Steps Download support bundle in support page Extract support bundle and check every file content Vagrant install master release Execute backend E2E regression test Run frontend Cypress automated test against feature Images, Networks, Virtual machines Run manual test against feature Volume, Live migration and Backup and rancher integration Expected Results Check can download support bundle correctly, check can access every file without empty</description>
    </item>
    <item>
      <title>Harvester supports event log</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2748_harvester_supports_event_log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2748_harvester_supports_event_log/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2748&#xA;Verified this feature has been implemented.&#xA;Test Information Environment: qemu/KVM 3 nodes Harvester Version: master-250f41e4-head ui-source Option: Auto Verify Steps: Install Graylog via docker[^1] Install Harvester with any nodes Login to Dashboard then navigate to Monitoring &amp;amp; Logging/Logging Create Cluster Output with following: Name: gelf-evts Type: Logging/Event Output: GELF Target: &amp;lt;Graylog_IP&amp;gt;, &amp;lt;Graylog_Port&amp;gt;, &amp;lt;UDP&amp;gt; Create Cluster Flow with following: Name: gelf-flow Type of Matches: Event Cluster Outputs: gelf-evts Create an Image for VM creation Create a vm vm1 and start it Login to Graylog dashboard then navigate to search Select update frequency New logs should be posted continuously.</description>
    </item>
    <item>
      <title>Harvester supports kube-audit log</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2747_harvester_supports_kube-audit_log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2747_harvester_supports_kube-audit_log/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2747&#xA;Verify Steps: Install Graylog via docker[^1] Install Harvester with any nodes Login to Dashboard then navigate to Monitoring &amp;amp; Logging/Logging Create Cluster Output with following: Name: gelf-evts Type: Audit Only Output: GELF Target: &amp;lt;Graylog_IP&amp;gt;, &amp;lt;Graylog_Port&amp;gt;, &amp;lt;UDP&amp;gt; Create Cluster Flow with following: Name: gelf-flow Type of Matches: Audit Cluster Outputs: gelf-evts Create an Image for VM creation Create a vm vm1 and start it Login to Graylog dashboard then navigate to search Select update frequency New logs should be posted continuously.</description>
    </item>
    <item>
      <title>Harvester uses active-backup as the default bond mode</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2472_harvester_uses_active-backup_as_the_default_bond_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2472_harvester_uses_active-backup_as_the_default_bond_mode/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2472&#xA;Verify Steps: Install Harvester via ISO The default Bond Mode should select active-backup Ater installed with active-backup mode, login to console Execute cat /etc/sysconfig/network/ifcfg-harvester-mgmt, BONDING_MODULE_OPTS should contains mode=active-backup </description>
    </item>
    <item>
      <title>Host list should display the disk error message on failure</title>
      <link>https://harvester.github.io/tests/manual/hosts/host-list-should-display-disk-error-message-on-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/host-list-should-display-disk-error-message-on-failure/</guid>
      <description> Related issue: #1167 Host list should display the disk error message on table Category: Storage Verification Steps Shutdown existing node vm machine Run &amp;ldquo;qemu-img create&amp;rdquo; command to make a nvme.img Edit quem/kvm xml setting to attach the nvme image Start VM Open hostpage and edit your target node config Add the new nvme disk Shutdown VM Remove the attach device setting in VＭ xml file Start VM Open Host page, the targe node will show warning with unready and unscheduable disk exists Expected Results If host encounter disk ready or schedule failure, on host page the &amp;ldquo;disk state&amp;rdquo; will show warning With a hover tip &amp;ldquo;Host have unready or unschedulable disks&amp;rdquo; Can create load balancer correctly with health check setting </description>
    </item>
    <item>
      <title>Http proxy setting on harvester</title>
      <link>https://harvester.github.io/tests/manual/deployment/http-proxy-setting-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/http-proxy-setting-harvester/</guid>
      <description>Related issue: #1218 Missing http proxy settings on rke2 and rancher pod&#xA;Related issue: #1012 Failed to create image when deployed in private network environment&#xA;Category: Network Environment setup Setup an airgapped harvester&#xA;Clone ipxe example repository https://github.com/harvester/ipxe-examples Edit the setting.xml file under vagrant ipxe example Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Verification Steps Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    <item>
      <title>Image filtering by labels</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2319-image-filtering-by-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2319-image-filtering-by-labels/</guid>
      <description>Related issues: #2319 [FEATURE] Image filtering by labels Category: Image Verification Steps Upload several images and add related label Go to the image list page Add filter according to test plan 1 Go to VM creation page Check the image list and search by name Import Harvester in Rancher Go to cluster management page Create a RKE2 cluster Check the image list and search by name Expected Results Test Result 1: The image list page can be filtered by label in the following cases</description>
    </item>
    <item>
      <title>Image filtering by labels (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2474-image-filtering-by-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2474-image-filtering-by-labels/</guid>
      <description>Related issues: #2474 [backport v1.0] [FEATURE] Image filtering by labels Category: Image Verification Steps Upload several images and add related label Go to the image list page Add filter according to test plan 1 Go to VM creation page Check the image list and search by name Import Harvester in Rancher Go to cluster management page Create a RKE2 cluster Check the image list and search by name Expected Results The image list page can be filtered by label in the following cases</description>
    </item>
    <item>
      <title>Image handling consistency between terraform data resource and Harvester UI created image</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2443-image-consistency-terraform-data-harvester-ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2443-image-consistency-terraform-data-harvester-ui/</guid>
      <description>Related issues: #2443 [BUG] Image handling inconsistency between &amp;ldquo;Harvester Terraform harvester_image data source&amp;rdquo; vs. &amp;ldquo;UI created Image&amp;rdquo; Category: Terraform Verification Steps Download latest terraform-provider terraform-provider-harvester_0.5.1_linux_amd64.zip&#xA;Extra the zip file&#xA;Create the install-terraform-provider-harvester.sh with the following content&#xA;#!/usr/bin/env bash [[ -n $DEBUG ]] &amp;amp;&amp;amp; set -x set -eou pipefail usage() { cat &amp;lt;&amp;lt;HELP USAGE: install-terraform-provider-harvester.sh HELP } version=0.5.1 arch=linux_amd64 terraform_harvester_provider_bin=./terraform-provider-harvester terraform_harvester_provider_dir=&amp;#34;${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/&amp;#34; mkdir -p &amp;#34;${terraform_harvester_provider_dir}&amp;#34; cp ${terraform_harvester_provider_bin} &amp;#34;${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}&amp;#34; Rename the extraced terraform-provider-harvester_v0.</description>
    </item>
    <item>
      <title>Image naming with inline CSS (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</guid>
      <description> Related issues: #2563 [[BUG] harvesterhci.io.virtualmachineimage spec.displayName displays differently in single view of image Category: Images Verification Steps Go to images Click &amp;ldquo;Create&amp;rdquo; Upload an image or leverage an url - but name the image something like: &amp;lt;strong&amp;gt;&amp;lt;em&amp;gt;something_interesting&amp;lt;/em&amp;gt;&amp;lt;/strong&amp;gt; Wait for upload to complete. Observe the display name within the list of images Compare that to clicking into the single image and viewing it Expected Results The list view naming would be the same as the single view of the image </description>
    </item>
    <item>
      <title>Image upload does not start when HTTP Proxy is configured</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2436-2524-image-upload-failed-when-http-proxy-configured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2436-2524-image-upload-failed-when-http-proxy-configured/</guid>
      <description>Related issues: #2436 [BUG] Image upload does not start when HTTP Proxy is configured Related issues: #2524 [backport v1.0] [BUG] Image upload does not start when HTTP Proxy is configured Category: Image Verification Steps Clone ipxe-example vagrant project https://github.com/harvester/ipxe-examples Edit settings.yml Set harvester_network_config.offline=true Create a one node air gapped Harvester with a HTTP proxy server Access Harvester settings page Add the following http proxy configuration { &amp;#34;httpProxy&amp;#34;: &amp;#34;http://192.168.0.254:3128&amp;#34;, &amp;#34;httpsProxy&amp;#34;: &amp;#34;http://192.</description>
    </item>
    <item>
      <title>Import and make changes to clusternetwork resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-clusternetwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-clusternetwork/</guid>
      <description>Import clusternetwork resource terraformer import harvester -r clusternetwork Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: default_physical_nic, enable in the clusternetwork.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r clusternetwork 2021/08/04 15:43:25 harvester importing.</description>
    </item>
    <item>
      <title>Import and make changes to image resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-image/</guid>
      <description>Import image resource terraformer import harvester -r image Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: description, display_name, name, namespace and url in the image.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r image 2021/08/04 16:14:52 harvester importing.</description>
    </item>
    <item>
      <title>Import and make changes to network resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-network/</guid>
      <description>Import network resource terraformer import harvester -r network Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace and vlan_id in the network.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r network 2021/08/04 16:14:08 harvester importing.</description>
    </item>
    <item>
      <title>Import and make changes to ssh_key resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-ssh-key/</guid>
      <description>Import ssh_key resource terraformer import harvester -r ssh_key Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace and public_key in the ssh_key.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r ssh_key 2021/08/04 16:14:36 harvester importing.</description>
    </item>
    <item>
      <title>Import and make changes to virtual machine resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-virtual-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-virtual-machine/</guid>
      <description>Import virtual machine resource terraformer import harvester -r virtualmachine Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: cpu, memory, name in the virtualmachine.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r virtualmachine 2021/08/04 16:15:08 harvester importing.</description>
    </item>
    <item>
      <title>Import and make changes to volume resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-volume/</guid>
      <description>Import volume resource terraformer import harvester -r volume Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace in the volume.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r volume 2021/08/04 16:15:29 harvester importing.</description>
    </item>
    <item>
      <title>Import External Harvester</title>
      <link>https://harvester.github.io/tests/manual/node-driver/import-external-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/import-external-harvester/</guid>
      <description>With Rancher &amp;lt; 2.6:&#xA;Deploy the rancher and harvester clusters separately In the rancher, add a harvester node template Select &amp;ldquo;External Harvester&amp;rdquo;, and refer to &amp;ldquo;Test Data&amp;rdquo; for other value settings. Use this template to create the corresponding cluster With Rancher 2.6: Home page / Import Existing / Generic Add cluster name and click on Create Follow the registration steps Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access External Harvester Host: Port: 443 Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Import internal harvester</title>
      <link>https://harvester.github.io/tests/manual/node-driver/import-internal-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/import-internal-harvester/</guid>
      <description>enable harvester&amp;rsquo;s rancher-enabled setting Click the rancher button in the upper right corner to access the internal rancher add a harvester node template Select &amp;ldquo;Internal Harvester&amp;rdquo;, and refer to &amp;ldquo;Test Data&amp;rdquo; for other value settings. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Improved resource reservation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2347_improved_resource_reservation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2347_improved_resource_reservation/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2347, https://github.com/harvester/harvester/issues/1700&#xA;Test Information Environment: Baremetal DL160G9 5 nodes Harvester Version: master-96b90714-head ui-source Option: Auto Verify Steps: Install Harvester with any nodes Login and Navigate to Hosts CPU/Memory/Storage should display Reserved and Used percentage. Navigate to Host&amp;rsquo;s details Monitor Data should display Reserved and Used percentage, and should equals to the value in Hosts. </description>
    </item>
    <item>
      <title>Initiate multiple migrations at one time</title>
      <link>https://harvester.github.io/tests/manual/live-migration/initiate-multple-migrations-same-time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/initiate-multple-migrations-same-time/</guid>
      <description> Initiate live migration for a vm. While the live migration is in progress, initiate another migration Expected Results Both migration should work fine. The VMs should be accessible after the migration </description>
    </item>
    <item>
      <title>Install 2 node Harvester with a Harvester token with multiple words</title>
      <link>https://harvester.github.io/tests/manual/deployment/812-multiple-word-harvester-token/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/812-multiple-word-harvester-token/</guid>
      <description> Related issues: #812 ISO install accepts multiple words for &amp;lsquo;cluster token&amp;rsquo; value resulting in failure to join cluster Verification Steps Start Harvester install from ISO At the &amp;lsquo;Cluster token&amp;rsquo; prompt, enter, here are words Proceed to complete the installation Boot a secondary host from the installation ISO and select the option to join an existing cluster At the &amp;lsquo;Cluster token&amp;rsquo; prompt, enter, here are words Proceed to complete the installation Verify both hosts show in hosts list at VIP Expected Results Install should complete successfully Host should add with no errors Both hosts should show up </description>
    </item>
    <item>
      <title>Install Harvester from USB disk</title>
      <link>https://harvester.github.io/tests/manual/deployment/install_via_usb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/install_via_usb/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1200&#xA;Verify Items Harvester can be installed via USB stick Case: Install Harvester via USB disk Follow the instruction to create USB disk Harvester should able to be installed via the USB on UEFI-based bare metals </description>
    </item>
    <item>
      <title>Install Harvester on a bare Metal node using ISO image</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-bare-metal-iso/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/install-bare-metal-iso/</guid>
      <description>Install using ISO image&#xA;Expected Results On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI </description>
    </item>
    <item>
      <title>Install Harvester on a bare Metal node using PXE boot (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-bare-metal-pxe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/install-bare-metal-pxe/</guid>
      <description>Install Harvester using PXE boot&#xA;Expected Results On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI </description>
    </item>
    <item>
      <title>Install Harvester on a virtual nested node using ISO image</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-nested-virtualization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/install-nested-virtualization/</guid>
      <description>Install using ISO image&#xA;Expected Results On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI </description>
    </item>
    <item>
      <title>Install Harvester on NVMe SSD</title>
      <link>https://harvester.github.io/tests/manual/deployment/install_on_nvme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/install_on_nvme/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1627&#xA;Verify Items Harvester can detect NVMe SSD when installing Harvester can be installed on NVMe SSD Case: Install Harvester on NVMe disk Create block image as NVMe disk Run dd if=/dev/zero of=/var/lib/libvirt/images/nvme145.img bs=1M count=148480 Then Change file owner chown qemu:qemu /var/lib/libvirt/images/nvme145.img Create VM via virt-manager Select Manual install, set Generic OS, Memory:9216, CPUs:8, Uncheck enable storage&amp;hellip; and check customize configuration before install Select Firmware to use UEFI x86_64 (use usr/share/qemu/ovmf-x86_64-code.</description>
    </item>
    <item>
      <title>Install Harvester over previous GNU/Linux install</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</guid>
      <description> Related issues: #2230 [BUG] harvester installer - always first attempt failed if before was linux installed Related issues: #2450 [backport v1.0][BUG] harvester installer - always first attempt failed if before was linux installed #2450 Category: Installtion Verification Steps Install GNU/LInux LVM configuration reboot Install Harvester via ISO over previous linux install Verifiy Harvester install by changing password and logging in. Expected Results Install should complete </description>
    </item>
    <item>
      <title>Install Option `HwAddr` for Network Interface</title>
      <link>https://harvester.github.io/tests/manual/deployment/hwaddr_configre_option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/hwaddr_configre_option/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1064&#xA;Verify Items Configure Option HwAddr is working on install configuration Case: Use HwAddr to install harvester via PXE Install Harvester with PXE installation, set hwAddr instead of name in install.networks Harvester should installed successfully </description>
    </item>
    <item>
      <title>Install Option `install.device` support symbolic link</title>
      <link>https://harvester.github.io/tests/manual/deployment/install_symblic_link/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/install_symblic_link/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1462&#xA;Verify Items Disk&amp;rsquo;s symbolic link can be used in install configure option install.device Case: Harvester install with configure symbolic link on install.device Install Harvester with any nodes login to console, use ls -l /dev/disk/by-path to get disk&amp;rsquo;s link name Re-install Harvester with configure file, with set the disk&amp;rsquo;s link name instead. Harvester should be install successfully </description>
    </item>
    <item>
      <title>Installation of the Harvester terraform provider (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/install-terraform-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/install-terraform-provider/</guid>
      <description>Follow the instruction of the README&#xA;Expected Results The provider is initialized and the terraform init command succeeds:&#xA;Initializing provider plugins... - Finding harvester/harvester versions matching &amp;#34;~&amp;gt; 0.1.0&amp;#34;... - Installing harvester/harvester v0.1.0... - Installed harvester/harvester v0.1.0 (unauthenticated) ... Terraform has been successfully initialized! </description>
    </item>
    <item>
      <title>Instance metadata variables are not expanded</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2342_instance_metadata_variables_are_not_expanded/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2342_instance_metadata_variables_are_not_expanded/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2342&#xA;Verify Steps: Install Harvester with any nodes Create Image for VM creation Create VM with following CloudConfig ## template: jinja #cloud-config package_update: true password: password chpasswd: { expire: False } sshpwauth: True write_files: - content: | #!/bin/bash vmName=$1 echo &amp;#34;VM Name is: $vmName&amp;#34; &amp;gt; /home/cloudinitscript.log path: /home/exec_initscript.sh permissions: &amp;#39;0755&amp;#39; runcmd: - - systemctl - enable - --now - qemu-guest-agent.service - - echo - &amp;#34;{{ ds.meta_data.local_hostname }}&amp;#34; - - /home/exec_initscript.</description>
    </item>
    <item>
      <title>ISO installation console UI Display</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2402-iso-installation-console-ui-display/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2402-iso-installation-console-ui-display/</guid>
      <description>Related issues: #2402 [FEATURE] Enhance the information display of ISO installation console UI (tty) Category: Harvester Installer Verification Steps ISO install a single node Harvester Monitoring the ISO installation console UI ISO install a three node Harvester cluster Monitoring the ISO installation console UI of the first node Monitoring the ISO installation console UI of the second node Monitoring the ISO installation console UI of the third node Expected Results The ISO installation console UI enhancement can display correctly under the following single and multiple nodes scenarios.</description>
    </item>
    <item>
      <title>keypairs.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/keypairs.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/webhooks/keypairs.harvesterhci.io/</guid>
      <description>GUI Enable VLAN network in settings Create a network with VLAN 5 and assume its name is my-network. C1. reate another network with VLAN 5: it should fails with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: VLAN ID 5 is already allocated Create a VM on VLAN 5, delete network my-network and it should fail with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: network my-network is still used by vm(s): vm-test in a modal.</description>
    </item>
    <item>
      <title>Ksmd support merge_across_node on/off</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2827_ksmd_support_merge_across_node_onoff_/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2827_ksmd_support_merge_across_node_onoff_/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2827&#xA;Verify Steps: Install Harvester with any nodes Login to Dashboard and Navigate to hosts Edit node1&amp;rsquo;s Ksmtuned to Run and ThresCoef to 85 then Click Save Login to node1&amp;rsquo;s console, execute kubectl get ksmtuned -oyaml --field-selector metadata.name=&amp;lt;node1&amp;gt; Fields in spec should be the same as Dashboard configured Create an image for VM creation Create multiple VMs with 2Gi+ memory and schedule on &amp;lt;node1&amp;gt; (memory size reflect to &amp;rsquo;s maximum size, total of VMs&amp;rsquo; memory should greater than 40%) Execute watch -n1 grep .</description>
    </item>
    <item>
      <title>Limit VM of guest cluster in the same namespace</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2354-limit-vm-of-guest-cluster-same-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2354-limit-vm-of-guest-cluster-same-namespace/</guid>
      <description>Related issues: #2354 [FEATURE] Limit all VMs of the Harvester guest cluster in the same namespace Category: Rancher integration Verification Steps Import Harvester from Rancher Access Harvester via virtualization management Create a test project and ns1 namespace Create two RKE1 node template, one set to default namespace and another set to ns1 namespace Create a RKE1 cluster, select the first pool using the first node template Create another pool, check can&amp;rsquo;t select the second node template Create a RKE2 cluster, set the first pool using specific namespace Add another machine pool, check it will automatically assigned the same namespace as the first pool Expected Results On RKE2 cluster page, when we select the first machine pool to specific namespace, then the second pool will automatically and can only use the same namespace as the first pool</description>
    </item>
    <item>
      <title>Local cluster user input topology key</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2567-local-cluster-user-input-topology-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2567-local-cluster-user-input-topology-key/</guid>
      <description> Related issues: #2567 [BUG] Local cluster owner create Harvester cluster failed(RKE2) Category: Rancher integration Verification Steps Import Harvester from Rancher Create a standard user local in Rancher User &amp;amp; Authentication Open Cluster Management page Edit cluster config Expand Member Roles Add local user with Cluster Owner role Create cloud credential of Harvester Login with local user Open the provisioning RKE2 cluster page Select Advanced settings Add Pod Scheduling Select Pods in these namespaces Check can input Topology key value Expected Results Login with cluster owner role and provision a RKE2 cluster we can input the topology key in the Topology key field of the pod selector </description>
    </item>
    <item>
      <title>Logging Output Filter</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2817-logging-output-filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2817-logging-output-filter/</guid>
      <description> Related issues: #2817 [BUG]Logging Output needs filter Category: Audit Logging Verification Steps Create an Audit Only type of Output named audit-output Create an Audit Only type of ClusterOutput named audit-cluster-output Create a Flow, select the type to Logging or Event Check you can&amp;rsquo;t select the audit-output and audiot-cluster-output select the type to Audit Check you can select the audit-output and audit-cluster-output Create a ClusterFlow, select the type to Logging or Event Check you can&amp;rsquo;t select the audiot-cluster-output select the type to Audit Check you can select the audiot-cluster-output Create an logging/event type of Output named logging-event-output Create an logging/event type of ClusterOutput named logging-event-cluster-output Create a Flow, select the type to Logging or Event Check you can select the logging-event-output and logging-event-output Create a ClusterFlow, select the type to Logging or Event Check you can select the logging-event-output and logging-event-output Expected Results The logging or the Event type of Flow can only select Logging or Event type of Output Can&amp;rsquo;t select the Audit type of Output The logging or the Event type of ClusterFlow can only select Logging or Event type of ClusterOutput Can&amp;rsquo;t select the Audit type of ClusterOutput The Audit type of Flow can only select Audit type of Output The Audit type of ClusterFlow can only select Audit type of ClusterOutput </description>
    </item>
    <item>
      <title>Login after password reset	(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/authentication/login-after-password-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/login-after-password-reset/</guid>
      <description> Enter the wrong credential. Enter the correct credential Expected Results Login should fail. Login should pass </description>
    </item>
    <item>
      <title>Logout from the UI and login again</title>
      <link>https://harvester.github.io/tests/manual/authentication/logout-then-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/logout-then-login/</guid>
      <description> Logout from the UI and Log in again Expected Results User should be able to logout/login successfully. </description>
    </item>
    <item>
      <title>Maintenance mode for host with multiple VMs</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</guid>
      <description> Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Do health check on VMs Expected Results Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode </description>
    </item>
    <item>
      <title>Maintenance mode for host with one VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</guid>
      <description> Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Do health check on VMs Expected Results Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode </description>
    </item>
    <item>
      <title>Maintenance mode on node with no vms (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</guid>
      <description> Put host in maintenance mode Wait for host to go from entering maintenance mode to maintenance mode. Expected Results Host should start to go into maintenance mode Host should go into maintenance mode </description>
    </item>
    <item>
      <title>Manual upgrade from 0.3.0 to 1.0.0</title>
      <link>https://harvester.github.io/tests/manual/deployment/manual-upgrade-from-0.3.0-to-1.0.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/manual-upgrade-from-0.3.0-to-1.0.0/</guid>
      <description>Related issues: #1644 Harvester pod crashes after upgrading from v0.3.0 to v1.0.0-rc1 (contain vm backup before upgrade)&#xA;Related issues: #1588 VM backup cause harvester pod to crash&#xA;Notice We recommend using zero downtime upgrade to upgrade harvester. Manual upgrade is for advance usage and purpose.&#xA;Category: Manual Upgrade Verification Steps Download harvester v0.3.0 iso and do checksum Download harvester v1.0.0 iso and do checksum Use ISO Install a 4 nodes harvester cluster Create several OS images from URL Create ssh key Enable vlan network with harvester-mgmt Create virtual network vlan1 with id 1 Create 2 virtual machines ubuntu-vm: 2 core, 4GB memory, 30GB disk Setup backup target Take a backup from ubuntu vm Peform manual upgrade steps in the following docudment upgrade process Follow the manual upgrade steps to upgrade from v0.</description>
    </item>
    <item>
      <title>Mark some features as experimental</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1671-mark-experimental-features/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1671-mark-experimental-features/</guid>
      <description> Related issues: #1671 Mark external Harvester cluster provisioning support as experimental Verification Steps Verify that external Harvester is marked as experiemental Verify that Cloud Credentials is marked as experimental Verify that external is marked as experimental in add node template Expected Results All external Harvester fields should be marked as experimental </description>
    </item>
    <item>
      <title>Memory overcommit on VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/memory_overcommit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/memory_overcommit/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1537&#xA;Verify Items Overcommit can be edit on Dashboard VM can allocate exceed Memory on the host Node VM can chage allocated Memory after created Case: Update Overcommit configuration Install Harvester with any Node Login to Dashboard, then navigate to Advanced Settings Edit overcommit-config The field of Memory should be editable Created VM can allocate maximum Memory should be &amp;lt;HostMemory&amp;gt; * [&amp;lt;overcommit-Memory&amp;gt;/100] - &amp;lt;Host Reserved&amp;gt; Case: VM can allocate Memory more than Host have Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostMemory&amp;gt; * 1.</description>
    </item>
    <item>
      <title>Migrate a turned on VM from one host to another</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</guid>
      <description> Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM created with cloud init config data</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-cloud-init/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-cloud-init/</guid>
      <description> Create a new VM with cloud init config data Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM created with user data config</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-user-data/</guid>
      <description> Create a new VM with a password specified by user data config Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM that has multiple volumes</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-volumes/</guid>
      <description> Create a new VM with a root disk and a CDROM volume Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM that was created from a template</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-template/</guid>
      <description> Create a new VM from a template Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM that was created using a restore backup to new VM</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-restore-to-new/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-restore-to-new/</guid>
      <description> Take an existing backup Restore the backup to a new VM Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM with 1 backup</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-one-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-one-backup/</guid>
      <description> Create a new VM Create a backup Add a new file to the home directory Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM with a saved SSH Key</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-ssh/</guid>
      <description> Create a new VM with an SSH key Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM with multiple backups</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-backups/</guid>
      <description> Create a new VM Create a backup Add a new file to the home directory Create a new backup Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate a VM with multiple networks</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-networks/</guid>
      <description> Create a new VM with one management network in masquerade mode one VLAN network Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    <item>
      <title>Migrate back VMs that were on host after taking host out of maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</guid>
      <description>Prerequisite: Have a Harvester cluster with at least 2 nodes setup.&#xA;Test Steps: Given Create a vm with node selector lets say node-1.&#xA;And Create a vm without node selector on node-1.&#xA;AND Write some data into both the VMs.&#xA;When Put the host node-1 into maintenance mode.&#xA;Then All the Vms on node-1 should be migrated to other nodes or the node should show warning that the vm with node selector can&amp;rsquo;t migrate.</description>
    </item>
    <item>
      <title>Migrate to Node without replicaset</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-to-node-without-replicaset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-to-node-without-replicaset/</guid>
      <description> Create a new VM on a 4 node cluster Check which nodes have copies of the replica set Migrate the VM to the host that does not have the volume Expected Results VM should create correctly </description>
    </item>
    <item>
      <title>Migrate VM from Restored backup</title>
      <link>https://harvester.github.io/tests/manual/live-migration/restored_vm_migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/restored_vm_migration/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1086&#xA;Verify Items VM can be migrate to any node with any times Case: Migrate a restored VM Install Harvester with at least 2 nodes setup backup-target with NFS Create image for VM creation Create VM a Add file with some data in VM a Backup VM a as a-bak Restore backup a-bak into VM b Start VM b then check added file should exist with same content Migrate VM b to another node, then check added file should exist with same content Migrate VM b again, then check added file should exist with same content </description>
    </item>
    <item>
      <title>Move Longhorn storage to another partition</title>
      <link>https://harvester.github.io/tests/manual/hosts/move-longhorn-storage-to-another-partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/move-longhorn-storage-to-another-partition/</guid>
      <description>Related issue: #1316 Move Longhorn storage to another partition Category: Storage Test Scenarios Case 1: UEFI + GPT (Disk &amp;lt; MBR Limit) Case 2: BIOS + No MBR (Disk &amp;lt; MBR Limit) Case 3: BIOS + Force MBR (Disk &amp;lt; MBR Limit) Case 4: BIOS + No MBR (Disk &amp;gt; MBR Limit) Case 5: BIOS + Force MBR (Disk &amp;gt; MBR Limit) Case 6: UEFI + GPT (Disk &amp;gt; MBR Limit) Environment setup Test Environment: 1 node harvester on local kvm machine</description>
    </item>
    <item>
      <title>Multi-browser login</title>
      <link>https://harvester.github.io/tests/manual/authentication/multi-browser-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/multi-browser-login/</guid>
      <description> Login via Chrome, firefox, edge, safari etc Expected Results Chrome, firefox, edge, safari etc should have same behavior. </description>
    </item>
    <item>
      <title>Multiple Disks Swapping Paths</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</guid>
      <description>Related issues: #1874 Multiple Disks Swapping Paths Verification Steps Prepare a harvester cluster (single node is sufficient) Prepare two additional disks and format both of them. Hotplug both disks and add them to the host via Harvester Dashboard (&amp;ldquo;Hosts&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo;) Shutdown the host. Swap the address and slot of the two disks in order to make their dev paths swapped For libvirt environment, you can swap &amp;lt;address&amp;gt; and &amp;lt;target&amp;gt; in the XML of the disk.</description>
    </item>
    <item>
      <title>Namespace pending on terminating</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2591_namespace_pending_on_terminating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2591_namespace_pending_on_terminating/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2591&#xA;Verify Steps: Install Harvester with any nodes Login to dashboard and navigate to Namespaces Trying to delete any namespaces, prompt windows should shows warning message </description>
    </item>
    <item>
      <title>Negative change backup target while restoring backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</guid>
      <description>Related issues: #2560 [BUG] VM hanging on restoring state when backup-target disconnected suddenly Category: Category Verification Steps Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3 Create Image for VM creation Create VM vm1 Take Backup vm1b from vm1 Restore the backup vm1b to New/Existing VM When the VM still in restoring state, update backup-target settings to Use the default value then setup it back.</description>
    </item>
    <item>
      <title>Negative create backup on store that is full (NFS)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-full-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-full-backup-target/</guid>
      <description> Initiate a backup with existing VM where the NFS store is full Expected Results You should get an error </description>
    </item>
    <item>
      <title>Negative Create Backup Target</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-create-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-create-backup-target/</guid>
      <description> Open up Backup-target in settings Input Incorrect server info Save Expected Results You should get an error on saving </description>
    </item>
    <item>
      <title>Negative delete backup while restore is in progress</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-backup-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-backup-while-restoring/</guid>
      <description> Create a backup of VM which has data more than 10Gi. Add 2Gi data in the same VM. Initiate deletion of the backup. While deletion is in progress, create another backup Expected Results Creation of backup should be prevented as there is a deletion is in progress. Once the deletion is completed, the backup creation should take place </description>
    </item>
    <item>
      <title>Negative delete multiple backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-multiple-backups/</guid>
      <description> Disconnect Backup Target Select multiple Backups from Backups list Click Delete Expected Results You should get an error </description>
    </item>
    <item>
      <title>Negative delete single backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-single-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-single-backup/</guid>
      <description> Take down backup target either by account, or via network blocking Delete backup from backups list Expected Results You should get an error </description>
    </item>
    <item>
      <title>Negative delete Volume that is in use (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/negative-delete-volume-that-is-in-use/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/negative-delete-volume-that-is-in-use/</guid>
      <description> Navigate to Volumes page and check for a volume in use by a VM Try to delete volume Click delete on modal Expected Results Page should load You should get an error message on the delete modal </description>
    </item>
    <item>
      <title>Negative disrupt backup server while restore is in progress</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-disrupt-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-disrupt-backup-target-while-restoring/</guid>
      <description> Initiate a backup restore from NFS server. Disconnect network from NFS server for 5 secs Verify the restore status Expected Results The restore is not be interrupted and should complete. Data should be intact </description>
    </item>
    <item>
      <title>Negative edit backup read from file YAML</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-file/</guid>
      <description> Disconnect backup target Edit YAML for backup Read from File Show Diff Save Expected Results You should get an error on saving </description>
    </item>
    <item>
      <title>Negative edit backup YAML</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-yaml/</guid>
      <description> Disconnect backup target Edit YAML for backup Show Diff Save Expected Results You should get an error on saving </description>
    </item>
    <item>
      <title>Negative Harvester installer input same NIC IP and VIP</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</guid>
      <description> Related issues: #2229 [BUG] input nic ip and vip with same ip address in Harvester-Installer Related issues: #2377 [Backport v1.0.3] input nic ip and vip with same ip address in Harvester-Installer Category: Installation Verification Steps Boot into ISO installer Specify same IP for NIC and VIP Expected Results Error message is displayed </description>
    </item>
    <item>
      <title>Negative initiate a backup while system is taking another backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-while-taking-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-while-taking-backup/</guid>
      <description> Start a VM backup, bk-1 of a VM which has data d1 While the backup is in progress, write some more data d2 in the VM disk and initiate another backup bk-2. Verify the backup 1 and backup 2 Expected Results Backup bk-1 should have only d1 data backup bk-2 should have data d1 and d2 </description>
    </item>
    <item>
      <title>Negative migrate a turned on VM from one host to another</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-migrate-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-migrate-vm/</guid>
      <description> Migrate the VM from one host in the cluster to another Turn off/disconnect node while migrating Expected Results Migration should fail You should get an error message in the status </description>
    </item>
    <item>
      <title>Negative network comes back up after reboot external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</guid>
      <description> Start pinging the VM reboot the VM Expected Results The VM should respond The VM should reboot The pings should stop getting responses The pings should start getting responses again </description>
    </item>
    <item>
      <title>Negative network comes back up after reboot management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-management-after-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/negative-management-after-reboot/</guid>
      <description> Start pinging the VM from the management network reboot the VM Expected Results The VM should respond The VM should reboot The pings should stop getting responses The pings should start getting responses again </description>
    </item>
    <item>
      <title>Negative network disconnection for a longer time while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-network-disconnect-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-network-disconnect-while-migrating/</guid>
      <description> Initiate VM migration While migration is in progress, disconnect network for 100 sec on the node where the VM is scheduled Expected Results Migration should fail but volume data should be intact The VM should be accessible during the migration and should also be accessible once the migration fails </description>
    </item>
    <item>
      <title>Negative network disconnection for a short time while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-network-disruption-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-network-disruption-while-migrating/</guid>
      <description> Initiate VM migration. While migration is in progress, disconnect network for 5 sec on the node where the VM is scheduled Expected Results Migration should resume once the network is up again The VM should be accessible during and after the migration </description>
    </item>
    <item>
      <title>Negative node down while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-node-down-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-node-down-while-migrating/</guid>
      <description> Initiate VM migration. While migration is in progress, shut the node where the VM is scheduled. After failure, initiate the migration to another node Expected Results Migration should fail but volume data should be intact The VM should be accessible on older node The migration scheduled for another node should work fine The VM should be accessible during and after the migration </description>
    </item>
    <item>
      <title>Negative node un-schedulable during live migration</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-node-unschedulable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-node-unschedulable/</guid>
      <description>Prerequisite: Cluster is of 3 nodes. VM is running on Node-1 Node-2 and Node-3 don&amp;rsquo;t have space to migrate a VM to them. Steps: Create a vm on node-1 Migrate the VM. Expected Results Migration should not be started. Relevant error should be shown on the GUI. The existing VM should be accessible and the health check of the VM should be fine </description>
    </item>
    <item>
      <title>Negative Power down the node where the VM is getting replaced by the restore</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring-replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring-replace/</guid>
      <description>Related issues tests#1263 [ReleaseTesting] Negative Power down the node where the VM is getting replaced by the restore Verification Steps Setup a 3 nodes harvester&#xA;Create a VM w/ extra disk and some data&#xA;Backup and shutdown VM&#xA;Start to observe pod/virt-launcher-VMNAME to get the node VM restoring on for next step.&#xA;Initiate a restore with existing VM, get node info from pod/virt-launcher-VMNAME. While the restore is in progress and VM is starting on a node, shut down the node</description>
    </item>
    <item>
      <title>Negative power down the node where the VM is getting restored</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring/</guid>
      <description> Initiate a restore. While the restore is in progress and VM is starting on a node, shut down the node Expected Results The restore should fail </description>
    </item>
    <item>
      <title>Negative Restore a backup while VM is restoring</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</guid>
      <description>Related issues: #2559 [BUG] Backup unable to be restored and the VM can&amp;rsquo;t be deleted Category: Backup/Restore Verification Steps Install Harvester with any nodes Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3 Create Image for VM creation Create VM vm1 Take backup from vm1 as vm1b Take backup from vm1 as vm1b2 Click Edit YAML of vm1b, update field status.source.spec.spec.domain.cpu.cores, increase 1 Stop VM vm1 Restore backup vm1b2 with Replace Existing Restore backup vm1b with Replace Existing when the VM vm1 still in state restoring Expected Results You should get an error when trying to restore.</description>
    </item>
    <item>
      <title>Negative restore backup replace existing VM</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace/</guid>
      <description> On multi-node setup bring down node that is hosting VM Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Expected Results You should get an error on restoring </description>
    </item>
    <item>
      <title>Negative restore backup replace existing VM with backup from same VM that is turned on</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-deleting-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-deleting-backup/</guid>
      <description> Make sure VM is turned on Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Delete backup while restoring Expected Results You should get an error </description>
    </item>
    <item>
      <title>Negative restore backup replace existing VM with backup from same VM that is turned on (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-turned-on/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-turned-on/</guid>
      <description> Make sure VM is turned on Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Expected Results You get an error that you have to stop VM before restoring backup </description>
    </item>
    <item>
      <title>Negative vm clone tests</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-vm-clone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-vm-clone/</guid>
      <description>Case 1 Create a harvester cluster. Create a VM source-vm with 3 volumes: Image Volume Volume Container Volume After VM starts, run command echo &amp;quot;123&amp;quot; &amp;gt; test.txt &amp;amp;&amp;amp; sync. Click clone button on the source-vm and input new VM name target-vm. Delete source-vm while still cloning Expected Results target-vm should finish cloning After cloning run command cat ~/test.txt in the target-vm. The result should be 123. Case 2 Create a harvester cluster.</description>
    </item>
    <item>
      <title>network-attachment-definitions.k8s.cni.cncf.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/q-network-attachment-definitions.k8s.cni.cncf.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/webhooks/q-network-attachment-definitions.k8s.cni.cncf.io/</guid>
      <description>GUI Enable VLAN network in settings Create a network with VLAN 5 and assume its name is my-network. Create another network with VLAN 5: it should fails with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: VLAN ID 5 is already allocated Create a VM on VLAN 5, delete network my-network and it should fail with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: network my-network is still used by vm(s): vm-test in a modal Expected Results GUI Unsure of desired behavior.</description>
    </item>
    <item>
      <title>Networkconfigs function check</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2841-networkconfigs-function-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2841-networkconfigs-function-check/</guid>
      <description>Related issues: #2841 [FEATURE] Reorganize the networkconfigs UI Category: Network Verification Steps Go to Cluster Networks/Configs&#xA;Create a cluster network and provide the name Create a Network Config&#xA;Given the NICs that not been used by mgmt-bo (eg. ens1f1)&#xA;Use default active-backup mode&#xA;Check the cluster network config in Active status Go to Networks&#xA;Create a VLAN network&#xA;Given the name and vlan id&#xA;Select the cluster network from drop down list Check the vlan route activity Check the NIC ens1f1 can bind to the cnetwork-bo</description>
    </item>
    <item>
      <title>NIC ip and vip can&#39;t be the same in Harvester installer</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2449-nic-ip-vip-different-harvester-installer-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2449-nic-ip-vip-different-harvester-installer-copy/</guid>
      <description> Related issues: #2229 [BUG] input nic ip and vip with same ip address in Harvester-Installer Related issues: #2449 [backport v1.0] [BUG] input nic ip and vip with same ip address in Harvester-Installer Category: Harvester Installer Verification Steps Launch ISO install process Set static node IP and gateway Set the same node IP to the VIP field and press enter&#xA;Expected Results During Harvester ISO installer process, when we set static node IP address with the same one as the VIP IP address There will be an error message to prevent the installation process VIP must not be the same as Management NIC IP </description>
    </item>
    <item>
      <title>Node disk manager should prevent too many concurrent disk formatting occur within a short period</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1831_node_disk_manager_should_prevent_too_many_concurrent_disk_formatting_occur_within_a_short_period/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1831_node_disk_manager_should_prevent_too_many_concurrent_disk_formatting_occur_within_a_short_period/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1831&#xA;Criteria exceed the maximum, there should have requeue devices which equals the exceeds hit the maximum, there should not have requeue devices less than maximum, there should not have requeue devices Verify Steps: Install Harvester with any node having at least 6 additional disks Login to console and execute command to update log level to debug and max-concurrent-ops to 1 (On KVM environment, we have to set to 1 to make sure the requeuing will happen.</description>
    </item>
    <item>
      <title>Node join fails with self-signed certificate</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2736_node_join_fails_with_self-signed_certificate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2736_node_join_fails_with_self-signed_certificate/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2736&#xA;Verified this bug has been fixed.&#xA;Test Information Environment: qemu/KVM 2 nodes Harvester Version: master-032742f0-head ui-source Option: Auto Verify Steps: Follow Steps in https://github.com/harvester/harvester-installer/pull/335 </description>
    </item>
    <item>
      <title>Node Labeling for VM scheduling</title>
      <link>https://harvester.github.io/tests/manual/hosts/node_labeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/node_labeling/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1416&#xA;Verify Items Host labels can be assigned during installation via config-create / config-join YAML. Host labels can be managed post installation via the Harvester UI. Host label information can be accessed in Rancher Virtualization Management UI. Case: Label node when installing Install Harvester with config file and os.labels option Navigate to Host details then navigate to Labels in Config Check additional labels should be displayed Case: Label node after installed Install Harvester with at least 2 nodes Navigate to Host details then navigate to Labels in Config Use edit config to modify labels Reboot the Node and wait until its state become active Navigate to Host details then Navigate to Labels in Config Check modified labels should be displayed Case: Node&amp;rsquo;s Label availability Install Harvester with at least 2 nodes Navigate to Host details then navigate to Labels in Config Use edit config to modify labels Reboot the Node and wait until its state become active Navigate to Host details then Navigate to Labels in Config Check modified labels should be displayed Install Rancher with any nodes Navigate to Virtualization Management and import former created Harvester Wait Until state become Active Click Name field to visit dashboard repeat step 2-7, and both compare from Harvester&amp;rsquo;s dashboard (accessing via Harvester&amp;rsquo;s VIP) </description>
    </item>
    <item>
      <title>Node promotion for topology label</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2325-node-promotion-for-topology-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2325-node-promotion-for-topology-label/</guid>
      <description>Related issues: #2325 [FEATURE] Harvester control plane should spread across failure domains Category: Host Verification Steps Install first node, the role of this node should be Management Node Install second node, the role of this node should be Compute Node, the second node shouldn&amp;rsquo;t be promoted to Management Node Add label topology.kubernetes.io/zone=zone1 to the first node Install third node, the second node and third node shouldn&amp;rsquo;t be promoted Add label topology.</description>
    </item>
    <item>
      <title>Nodes with cordoned status should not be in VM migration list</title>
      <link>https://harvester.github.io/tests/manual/hosts/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</guid>
      <description>Related issues: #1501 Nodes with cordoned status should not be in the selection list for VM migration Category: Host Verification Steps Create multiple VMs on two of the nodes Set the idle node to cordoned state Edit any config of VM, click migrate Check the available node in the migration list Expected Results Node set in cordoned state will not show up in the available migration list</description>
    </item>
    <item>
      <title>PCI Devices Controller</title>
      <link>https://harvester.github.io/tests/manual/advanced/addons/pci-devices-controller/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/addons/pci-devices-controller/</guid>
      <description>Pre-requisite Enable PCI devices Create a harvester cluster in bare metal mode. Ensure one of the nodes has NIC separate from the management NIC Go to the management interface of the new cluster Go to Advanced -&amp;gt; PCI Devices Validate that the PCI devices aren&amp;rsquo;t enabled Click the link to enable PCI devices Enable PCI devices in the linked addon page Wait for the status to change to Deploy successful Navigate to the PCI devices page Validate that the PCI devices page is populated/populating with PCI devices Case 1 (PCI NIC passthrough) Create a harvester cluster in bare metal mode.</description>
    </item>
    <item>
      <title>Polish harvester machine config in Rancher</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2598-polish-harvester-machine-config-in-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2598-polish-harvester-machine-config-in-rancher/</guid>
      <description>Related issues: #2598 [BUG]Polish harvester machine config Category: Rancher integration Verification Steps Import Harvester from Rancher Create a standard user local in Rancher User &amp;amp; Authentication Open Cluster Management page Edit cluster config Expand Member Roles Add local user with Cluster Owner role Create cloud credential of Harvester Login with local user Open the provisioning RKE2 cluster page Select Advanced settings Add Pod Scheduling Select Pods in these namespaces Check the list of available pods with the namespaces options above Check can input Topology key value Access Harvester UI (Not from Rancher) Open project/namespace Create several namespaces Login local user to Rancher Open the the provisioning RKE2 cluster page Check the available Pods in these namespaces list have been updated Expected Results Checked the following test plan for RKE2 cluster are working as expected</description>
    </item>
    <item>
      <title>Power down a node out of three nodes available for the Cluster</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-power-off-one-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/negative-power-off-one-node-cluster/</guid>
      <description> Create a three nodes cluster for Harvester. Power down an added node. Expected Results On power down the node, the status of the node should become down. Harvester system system should be still up. </description>
    </item>
    <item>
      <title>Power down and power up the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</guid>
      <description>Create two vms on a cluster. Power down the node. Try to migrate a VM from the down node to active node. Leave the 2nd vm as it is. Power on the node Expected Results The 1st VM should be migrated to other node on manually doing it. The 2nd VM should be accessible once the node is up. Known bugs https://github.com/harvester/harvester/issues/982</description>
    </item>
    <item>
      <title>Power down the management node.</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-power-down-management-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/negative-power-down-management-node/</guid>
      <description> Create a three nodes cluster for Harvester. Power down the first node which was added to the cluster. Expected Results On power down the node, the status of the node should become down. Harvester system system should be still up. </description>
    </item>
    <item>
      <title>Power down the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</guid>
      <description> Create two vms on a cluster. Power down the node. Try to migrate a VM from the down node to active node. Leave the 2nd vm as it is. Expected Results The 1st VM should be migrated to other node on manually doing it. The 2nd VM should be recovered from the lost node </description>
    </item>
    <item>
      <title>Power node triggers VM reschedule</title>
      <link>https://harvester.github.io/tests/manual/hosts/vm_rescheduled_after_host_poweroff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/vm_rescheduled_after_host_poweroff/</guid>
      <description>Ref: N/A, legacy test case, VM is not migrated but rescheduled&#xA;Criteria VM should created and started successfully Node should be unavailable after shutdown VM should be restarted automatically Verify Steps: Install Harvester with at least 2 nodes Create a image for VM creation Create a VM vm1 and start it vm1 should started successfully Power off the node hosting vm1 the node should becomes unavailable on dashboard VM vm1 should be restarted automatically after vm-force-reset-policy seconds </description>
    </item>
    <item>
      <title>Press the Enter key in setting field shouldn&#39;t refresh page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2569-press-enter-settings-should-not-refresh-page-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2569-press-enter-settings-should-not-refresh-page-copy/</guid>
      <description>Related issues: #2569 [BUG] Press the Enter key, the page will be refreshed automatically Category: Settings Verification Steps Check every page have input filed in the Settings page Move cursor to any input field Click the Enter button Check the page will not be automatically loaded Expected Results On v1.0.3 backport, when we press the Enter key in the following page fields, it will not being refreshed automatically.&#xA;Also checked the following pages</description>
    </item>
    <item>
      <title>Prevent normal users create harvester-public namespace</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2485-prevent-normal-user-create-harvesterpublic-ns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2485-prevent-normal-user-create-harvesterpublic-ns/</guid>
      <description>Related issues: #2485 [FEATURE] [Harvester Node Driver v2] Prevent normal users from creating VMs in harvester-public namespace Category: Rancher integration Verification Steps Import Harvester from Rancher Create standard user in Rancher User &amp;amp; Authentication Edit Harvester in virtualization Management, assign Cluster Member role to user Login with user Create cloud credential Provision an RKE2 cluster Check the namespace dropdown list Expected Results Now the standard user with cluster member rights won&amp;rsquo;t display harvester-public while user node driver to provision the RKE2 cluster.</description>
    </item>
    <item>
      <title>Project owner role on customized project open Harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2394-2395-project-owner-customized-project-open-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2394-2395-project-owner-customized-project-open-harvester/</guid>
      <description> Related issues: #2394 [BUG] Standard rancher user with project owner role of customized project to access Harvester get &amp;ldquo;404 Not Found&amp;rdquo; error Related issues: #2395 [backport v1.0] [BUG] Standard rancher user with project owner role of customized project to access Harvester get &amp;ldquo;404 Not Found&amp;rdquo; error Category: Rancher integration Verification Steps Import Harvester from Rancher Access Harvester on virtualization management page Create a project test and namespace test under it Go to user authentication page Create a stand rancher user test Access Harvester in Rancher Set project owner role of test project to test user Login Rancher with test user Access the virtualization management page Expected Results Now the standard user with project owner role can access harvester in virtualization management page correctly </description>
    </item>
    <item>
      <title>Project owner should not see additional alert</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2288-2350-project-owner-should-not-see-alert-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2288-2350-project-owner-should-not-see-alert-copy/</guid>
      <description>Related issues: #2288 [BUG] The project-owner user will see an additional alert Related issues: #2350 [Backport v1.0] The project-owner user will see an additional alert Category: Rancher integration Verification Steps Importing a harvester cluster in a rancher cluster enter the imported harvester cluster from the Virtualization Management page create a new Project (test), Create a test namespace in the test project. go to Network page, add vlan 1 create a vm， choose test namespace, choose vlan network, click save create a new user (test), choose Standard User go to the project page, edit test Project, set test user to Project Owner。 login again with test user go to the vm page Expected Results Use rancher standard user test with project owner permission to access Harvester.</description>
    </item>
    <item>
      <title>Promote remaining host when delete one</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2191-promote-remaining-host-when-delete-one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2191-promote-remaining-host-when-delete-one/</guid>
      <description>Related issues: #2191 [BUG] Promote fail, cluster stays in Provisioning phase Category: Host Verification Steps Create a 4-node Harvester cluster. Wait for three nodes to become control plane nodes (role is control-plane,etcd,master). Delete one of the control plane nodes. The remaining worker node should be promoted to a control plane node (role is control-plane,etcd,master). Expected Results Four nodes Harvester cluster status, before delete one of the control-plane node&#xA;n1-221021:/etc # kubectl get nodes NAME STATUS ROLES AGE VERSION n1-221021 Ready control-plane,etcd,master 17h v1.</description>
    </item>
    <item>
      <title>Provision RKE2 cluster with resource quota configured</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/provision-rke2-cluster-with-resource-quota-configured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/provision-rke2-cluster-with-resource-quota-configured/</guid>
      <description>Related issues: #1455 Node driver provisioning fails when resource quota configured in project&#xA;Related issues: #1449 Incorrect naming of project resource configuration&#xA;Category: Rancher Integration Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Test Scenarios Scenario 1:&#xA;Project with resource quota: CPU Limit / CPU Reservation: 6000 / 6144 Memory Limit / Memory Reservation: 6000 / 6144 Scenario 2:</description>
    </item>
    <item>
      <title>PXE instll without iso_url field</title>
      <link>https://harvester.github.io/tests/manual/deployment/1439-pxe-install-without-iso-url-field/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/1439-pxe-install-without-iso-url-field/</guid>
      <description> Related issues: #1439 PXE boot installation doesn&amp;rsquo;t give an error if iso_url field is missing Environment setup This is easiest to test with the vagrant setup at https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester&#xA;edit https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27 to be blank Verification Steps Run the vagrant ./setup.sh from the vagrant repo Expected Results You should get an error in the console for the VM when installing </description>
    </item>
    <item>
      <title>PXE instll without iso_url field</title>
      <link>https://harvester.github.io/tests/manual/hosts/1439-pxe-install-without-iso-url-field/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1439-pxe-install-without-iso-url-field/</guid>
      <description> Related issues: #1439 PXE boot installation doesn&amp;rsquo;t give an error if iso_url field is missing Environment setup This is easiest to test with the vagrant setup at https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester&#xA;edit https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27 to be blank Verification Steps Run the vagrant ./setup.sh from the vagrant repo Expected Results You should get an error in the console for the VM when installing </description>
    </item>
    <item>
      <title>Rancher import harvester enhancement</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/rancher-import-harvester-enhacement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/rancher-import-harvester-enhacement/</guid>
      <description>Related issues: #1330 Http proxy setting download image Category: Rancher Integration Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Installed a 3 nodes harvester cluster Import harvester to rancher in virtualization management Enable node driver and create cloud credential Provision a RKE2 cluster in rancher Confirm RKE2 cluster is fully operated, can explore it Shutdown all 3 nodes server machine Wait for 10 minutes Power on all harvester nodes server machines Confirm harvester is fully operated Confirm RKE2 vm is back to running Check the RKE2 cluster status in rancher Expected Results The RKE2 cluster in rancher should turn back to Running with no error after harvester server node machine is fully power off and power on.</description>
    </item>
    <item>
      <title>Rancher Resource quota management</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/resource_quota/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/resource_quota/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1450&#xA;Verify Items Project&amp;rsquo;s Resource quotas can be updated correctly Namespace Default Limit should be assigned as the Project configured Namespace moving between projects should work correctly Case: Create Namespace with Resource quotas Install Harvester with any nodes Install Rancher Login to Rancher, import Harvester from Virtualization Management Access Harvester dashboard via Virtualization Management Navigate to Project/Namespaces, Create Project A with Resource quotas Create Namespace N1 based on Project A The Default value of Resource Quotas should be the same as Namespace Default Limit assigned in Project A Modifying resource limit should work correctly (when increasing/decreasing, the value should increased/decreased) After N1 Created, Click Edit Config on N1 resource limit should be the same as we assigned Increase/decrease resource limit then Save Click Edit Config on N1, resource limit should be the same as we assigned Click Edit Config on N1, then increase resource limit exceeds Project A&amp;rsquo;s Limit Click Save Button, error message should shown.</description>
    </item>
    <item>
      <title>rancher-monitoring status when hosting NODE down</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2243-rancher-monitoring-status-when-hosting-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2243-rancher-monitoring-status-when-hosting-node-down/</guid>
      <description>Related issues: #2243 [BUG] rancher-monitoring is unusable when hosting NODE is (accidently) down Category: Monitoring Verification Steps Install a two nodes harvester cluster Check the Initial state of the 2 nodes Harvester cluster harv-node1-0719:~ # kubectl get nodes NAME STATUS ROLES AGE VERSION harv-node1-0719 Ready control-plane,etcd,master 36m v1.21.11+rke2r1 harv-node2-0719 Ready &amp;lt;none&amp;gt; harv-node1-0719:~ # kubectl get pods -A | grep monitoring cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 33m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ckbjc 3/3 Running 0 33m harv-node1-0719:~ # kubectl get pods prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system -o yaml | grep nodeName nodeName: harv-node1-0719 Power off both nodes</description>
    </item>
    <item>
      <title>RBAC Cluster Owner</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</guid>
      <description> Related issues: #2626 [BUG] Access Harvester project/namespace page hangs with no response timeout with local owner role from Rancher Category: Authentication Verification Steps Import Harvester from Rancher Create a standard user local in Rancher User &amp;amp; Authentication Open Cluster Management page Edit cluster config Expand Member Roles Add local user with Cluster Owner role Logout Admin Login with local user Access Harvester from virtualization management Click the Project/Namespace page Expected Results Local owner role user can access and display Harvester project/namespace place correctly without hanging to timeout </description>
    </item>
    <item>
      <title>RBAC Create VM with restricted admin user</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</guid>
      <description>Related issues: #2587 [BUG] namespace on create VM is wrong when going through Rancher #2116 [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester Category: Authentication Verification Steps Verification Steps Import Harvester into Rancher Create a restricted admin Navigate to Volumes page Verify you only see associated Volumes Log out of admin and log in to restricted admin Navigate to Harvester UI via virtualization management Open virtual machines tab Click create Verified that namespace was default.</description>
    </item>
    <item>
      <title>Reboot a cluster and check VIP</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1669-reboot-cluster-check-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1669-reboot-cluster-check-vip/</guid>
      <description> Related issues: #1669 Unable to access harvester VIP nor node IP after reboot or fully power cycle node machines (Intermittent) Verification Steps Enable VLAN with NIC harvester-mgmt Create VLAN 1 Disable VLAN Enable VLAN again shutdown node 3, 2, 1 server machine Wait for 15 minutes Power on node 1 server machine, wait for 20 seconds Power on node 2 server machine, wait for 20 seconds Power on node 3 server machine Check if you can access VIP and each node IP Expected Results VIP should load the page and show on every node in the terminal </description>
    </item>
    <item>
      <title>Reboot a cluster and check VIP</title>
      <link>https://harvester.github.io/tests/manual/hosts/1669-reboot-cluster-check-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1669-reboot-cluster-check-vip/</guid>
      <description> Related issues: #1669 Unable to access harvester VIP nor node IP after reboot or fully power cycle node machines (Intermittent) Verification Steps Enable VLAN with NIC harvester-mgmt Create VLAN 1 Disable VLAN Enable VLAN again shutdown node 3, 2, 1 server machine Wait for 15 minutes Power on node 1 server machine, wait for 20 seconds Power on node 2 server machine, wait for 20 seconds Power on node 3 server machine Check if you can access VIP and each node IP Expected Results VIP should load the page and show on every node in the terminal </description>
    </item>
    <item>
      <title>Reboot host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</guid>
      <description>For Host that is in maintenance mode and turned on Reboot host Expected Results Host should reboot Maintenance mode label in hosts list should go from yellow to red to yellow Known Bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    <item>
      <title>Reboot host trigger VM migration</title>
      <link>https://harvester.github.io/tests/manual/hosts/vm_migrated_after_host_reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/vm_migrated_after_host_reboot/</guid>
      <description>Ref: N/A, legacy test case&#xA;Criteria VM should created and started successfully Node should be unavailable while rebooting VM should be migrated to ohter node Verify Steps: Install Harvester with at least 2 nodes Create a image for VM creation Create a VM vm1 and start it vm1 should started successfully Reboot the node hosting vm1 the node should becomes unavailable on dashboard vm1 should be automatically migrated to another node </description>
    </item>
    <item>
      <title>Reboot node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</guid>
      <description> Create a vm on the cluster. Reboot the node where the vm exists. Reboot the node where there is no vm Expected Results On rebooting the node, once the node is back up and Harvester is started, the host should become available on the cluster. </description>
    </item>
    <item>
      <title>Reboot the management node/added node.</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-reboot-management-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/negative-reboot-management-node/</guid>
      <description> Create a three nodes cluster for Harvester. Reboot the management node/added node. Expected Results Once the node is up after reboot, the node should become available in the cluster. </description>
    </item>
    <item>
      <title>Recover cordon and maintenace node after harvester node machine reboot</title>
      <link>https://harvester.github.io/tests/manual/hosts/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</guid>
      <description>Related issues: #1493 When hosts are stuck in maintenance mode and the cluster is unstable you can&amp;rsquo;t access the UI Category: Host Verification Steps Create 3 virtual machine on 3 harvester nodes Cordon 1st and 2nd node, Enable maintenance mode on 1st and 2nd node We can&amp;rsquo;t cordon and enable maintenance node on the remaining node Reboot 1st and 2nd node bare machine Wait for harvester machine back to service Login dashboard Disable maintenance mode on 1st and 2nd node Expected Results Cordon node and enter maintenance mode, after machine reboot, user can login harvester dashboard.</description>
    </item>
    <item>
      <title>Reinstall agent node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2665-2892-reinstall-agent-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2665-2892-reinstall-agent-node/</guid>
      <description>Related issues: #2665 [BUG] reinstall 1st node&#xA;Related issues: #2892 [BUG] rancher-system-agent keeps showing error on a new node in an upgraded cluster&#xA;Category: Host Verification Steps Test Plan 1: Reinstall management node and agent node in a upgraded cluster Create a 4-node v1.0.3 cluster.&#xA;Upgrade the master branch:&#xA;Check the spec content in provisioning.cattle.io/v1/clusters -&amp;gt; fleet-local Check the iface content in helm.cattle.io/v1/helmchartconfigs -&amp;gt; rke2-canal spec: │ │ valuesContent: |- │ │ flannel: │ │ iface: &amp;#34;&amp;#34; Remove the agent node and 1 management node.</description>
    </item>
    <item>
      <title>Rejoin node machine after Harvester upgrade</title>
      <link>https://harvester.github.io/tests/manual/upgrade/rejoin-node-after-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/rejoin-node-after-upgrade/</guid>
      <description>Related issues: #2655 [BUG] reinstall 1st node Category: Upgrade Harvester Environment requirement Network environment has available VLAN id setup on DHCP server DHCP server has setup the IP range can allocate to above VLAN id Harvester node can route to DHCP server through VLAN id to retrieve IP address Network has at least two NICs Suggest not to use SMR type HDD disk Verification Steps Create a 3 nodes v1.</description>
    </item>
    <item>
      <title>Remove a management node from a 3 nodes cluster and add it back to the cluster by reinstalling it</title>
      <link>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</guid>
      <description> From a HA cluster with 3 nodes Delete one of the nodes after the node promotion(all 3 nodes are management nodes) Reinstall the removed node with the same node name and IP The rejoined node will be promoted to master automatically Expected Results The removed node should be able to rejoin the cluster without issues Comments Purpose is to cover this scenario: https://github.com/harvester/harvester/issues/1040 Check the job promotion with the command kubectl get jobs -n harvester-system If a node is stuck in the removing status, you likely face to this issue, execute this command as workaround: kubectl get node -o name &amp;lt;nodename&amp;gt; | xargs -i kubectl patch {} -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;finalizers&amp;quot;:[]}}&#39; --type=merge </description>
    </item>
    <item>
      <title>Remove a node from the existing cluster</title>
      <link>https://harvester.github.io/tests/manual/deployment/remove-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/remove-node-cluster/</guid>
      <description>Remove node from the Harvester cluster using the Harvester UI Expected Results The components of Harvester should get cleaned up from the node.</description>
    </item>
    <item>
      <title>Remove Pod Scheduling from harvester rke2 and rke1</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</guid>
      <description>Related issues: #2642 [BUG] Remove Pod Scheduling from harvester rke2 and rke1 Category: Rancher Test Information Test Environment: 1 node harvester on local kvm machine Harvester version: v1.0-44fb5f1a-head (08/10) Rancher version: v2.6.7-rc7&#xA;Environment Setup Prepare Harvester master node Prepare Rancher v2.6.7-rc7 Import Harvester to Rancher Set ui-offline-preferred: Remote Go to Harvester Support page Download Kubeconfig Copy the content of Kubeconfig Verification Steps RKE2 Verification Steps Open Harvester Host page then edit host config Add the following key value in the labels page: topology.</description>
    </item>
    <item>
      <title>Remove unavailable node with VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</guid>
      <description>Create VMs on a host. Turn off Host Remove Host from hosts list Expected Results VMs should migrate to new host Known Bugs https://github.com/harvester/harvester/issues/983</description>
    </item>
    <item>
      <title>Restart Button Web VNC window</title>
      <link>https://harvester.github.io/tests/manual/_incoming/379-restart-button-web-vnc-window/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/379-restart-button-web-vnc-window/</guid>
      <description> Related issues: #379 [Question] Restart Button Web VNC window Category: VM Verification Steps Create a new VM with Ubuntu desktop 20.04 Prepare two volume Complete the installation process Open a web browser on Ubuntu desktop Check the shortcut keys combination Expected Results The soft reboot keys can display and reboot correctly on Linux OS VM (Ubuntu desktop 20.04) </description>
    </item>
    <item>
      <title>Restart/Stop VM with in progress Backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</guid>
      <description> Related issues: #1702 Don&amp;rsquo;t allow restart/stop vm when backup is in progress Verification Steps Create a VM. Create a VMBackup for it. Before VMBackup is done, stop/restart the VM. Verify VM can&amp;rsquo;t be stopped/restarted. </description>
    </item>
    <item>
      <title>Restore backup create new vm (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm/</guid>
      <description> Create a new file before restoring the backup and add some data Stop the VM where the backup was taken Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine Expected Results Backup should restore VM should update to previous backup File should no longer be present </description>
    </item>
    <item>
      <title>Restore backup create new vm in another namespace</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm-in-another-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm-in-another-namespace/</guid>
      <description> Create a VM vm in namespace default. Create a file ~/test.txt with content test. Create a VMBackup default-vm-backup for it. Create a new namepsace new-ns. Create a VMRestore restore-default-vm-backup-to-new-ns in new-ns namespace based on the VMBackup default-vm-backup to create a new VM. Expected Results A new VM in new-ns namespace should be created. It should have the file ~/test.txt with content test. </description>
    </item>
    <item>
      <title>Restore Backup for VM that was live migrated (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-for-vm-live-migrated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-for-vm-live-migrated/</guid>
      <description> Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine Expected Results Backup should restore VM should update to previous backup File should no longer be present </description>
    </item>
    <item>
      <title>Restore backup replace existing VM with backup from same VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-replace-existing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-replace-existing/</guid>
      <description> Create a new file before restoring the backup and add some data Stop the VM Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine Expected Results Backup should restore VM should update to previous backup File should no longer be present </description>
    </item>
    <item>
      <title>Restore First backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-first-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-first-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 1 Validate that md5sum -c file1.</description>
    </item>
    <item>
      <title>Restore last backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-last-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-last-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 3 Validate that md5sum -c file1-2.</description>
    </item>
    <item>
      <title>Restore middle backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-middle-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-middle-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 2 Validate that md5sum -c file1-2.</description>
    </item>
    <item>
      <title>restored VM can not be cloned</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2968_restored_vm_can_not_be_cloned/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2968_restored_vm_can_not_be_cloned/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2968&#xA;Test Information Environment: qemu/KVM 3 nodes Harvester Version: master-f96827b2-head ui-source Option: Auto Verify Steps: Follow Steps to reproduce in https://github.com/harvester/harvester/issues/2968#issue-1413026149 Additional regression test cases listed in https://github.com/harvester/tests/issues/568#issue-1414534000 </description>
    </item>
    <item>
      <title>Restored VM name does not support uppercases</title>
      <link>https://harvester.github.io/tests/manual/_incoming/4544_restored_vm_name_does_not_support_uppercases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/4544_restored_vm_name_does_not_support_uppercases/</guid>
      <description>Related issues: #4544 [BUG] Unable to restore backup into new VM when the name starts with upper case Category: Backup/Restore Verification Steps Setup backup-target in &amp;lsquo;Advanced&amp;rsquo; -&amp;gt; &amp;lsquo;Settings&amp;rsquo; Create an image for VM creation Create a VM vm1 Take a VM backup vm1b Go to &amp;lsquo;Backup &amp;amp; Snapshot&amp;rsquo;, restore vm1b to new VM Positive Cases Single lower Lowers Lowers contains &amp;lsquo;.&amp;rsquo; Lowers contains &amp;lsquo;-&amp;rsquo; Lowers contains &amp;lsquo;.&amp;rsquo; and &amp;lsquo;-&amp;rsquo; Negtive Cases Upper Upper infront of valid Upper append to valid Upper in the middle of valid Expected Results VM name should comply with following rules:</description>
    </item>
    <item>
      <title>Restricted admin should not see cattle-monitoring-system volumes</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2116-2351-restricted-admin-no-cattle-monitoring-system-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2116-2351-restricted-admin-no-cattle-monitoring-system-volumes/</guid>
      <description>Related issues: #2116 [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester Related issues: #2351 [Backport v1.0] You can see cattle-monitoring-system volumes as restricted admin in Harvester Category: Rancher integration Verification Steps Import Harvester to Rancher Create restricted admin in Rancher Log out of rancher Log in as restricted admin Navigate to Harvester ui in virtualization management Navigate to volumes page Expected Results Login Rancher with restricted admin and access Harvester volume page.</description>
    </item>
    <item>
      <title>Run multiple instances of the console</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/run-multiple-instances-console/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/run-multiple-instances-console/</guid>
      <description> Open up the console on two browsers to simulate multiple connections Login with both browsers create a new file on both instances Edit the file from the other instance and save Verify that you can see the changes from the other instance Expected Results You should be able to login from multiple browsers File should create File should update You should be able to see changes from all instances </description>
    </item>
    <item>
      <title>Set backup target S3	(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/advanced/set-s3-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/set-s3-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose S3 Set valid S3 target Save Expected Results login should complete Settings should save You should not get an error message </description>
    </item>
    <item>
      <title>Set backup-target NFS	(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/advanced/set-nfs-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/set-nfs-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose NFS Set valid NFS target Save Expected Results login should complete Settings should save You should not get an error message </description>
    </item>
    <item>
      <title>Set backup-target NFS invalid target</title>
      <link>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-nfs-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-nfs-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose NFS Set invalid NFS target Save Expected Results login should complete Settings should save You should get an error message </description>
    </item>
    <item>
      <title>Set backup-target S3 invalid target</title>
      <link>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-s3-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-s3-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose S3 Set invalid S3 target Save Expected Results login should complete Settings should save You should get an error message </description>
    </item>
    <item>
      <title>Set maintenance mode on the last available node shouldn&#39;t be allowed</title>
      <link>https://harvester.github.io/tests/manual/hosts/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</guid>
      <description>Related issues: #1014 Trying to set maintenance mode on the last available node shouldn&amp;rsquo;t be allowed Category: Host Verification Steps Create 3 vms located on node2 and node3 Open host page&#xA;Set node 3 into maintenance mode&#xA;Wait for virtual machine migrate to node 2&#xA;Set node 2 into maintenance mode&#xA;wait for virtual machine migrate to node 1&#xA;Set node 2 into maintenance mode&#xA;Expected Results Within 3 nodes and 3 virtual machines testing environment.</description>
    </item>
    <item>
      <title>Setup and test local Harvester upgrade responder</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1849-setup-test-local-upgrade-responder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1849-setup-test-local-upgrade-responder/</guid>
      <description>Related issues: #1849 [Task] Improve Harvester upgrade responder Category: Upgrade Verification Steps Follow the steps in https://github.com/harvester/harvester/issues/1849#issuecomment-1180346017&#xA;Clone longhorn/upgrade-responder and checkout to v0.1.4. Edit response.json content in config folder { &amp;#34;Versions&amp;#34;: [ { &amp;#34;Name&amp;#34;: &amp;#34;v1.0.2-master-head&amp;#34;, &amp;#34;ReleaseDate&amp;#34;: &amp;#34;2022-06-15T00:00:00Z&amp;#34;, &amp;#34;Tags&amp;#34;: [ &amp;#34;latest&amp;#34;, &amp;#34;test&amp;#34;, &amp;#34;dev&amp;#34; ] } ] } Install InfluxDB Run longhorn/upgrade-responder with the command: go run main.go --debug start --upgrade-response-config config/response.json --influxdb-url http://localhost:8086 --geodb geodb/GeoLite2-City.mmdb --application-name harvester Check the local upgrade responder is running curl -X POST http://localhost:8314/v1/checkupgrade \ -d &amp;#39;{ &amp;#34;appVersion&amp;#34;: &amp;#34;v1.</description>
    </item>
    <item>
      <title>Shut down host in maintenance mode and verify label change</title>
      <link>https://harvester.github.io/tests/manual/hosts/1272-shutdown-host-in-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1272-shutdown-host-in-maintenance-mode/</guid>
      <description> Related issues: #1272 Shut down a node with maintenance mode should show red label Verification Steps Open host page Set a node to maintenance mode Turn off host vm of the node Check node status Turn on host Check node status Expected Results The node should go into maintenance mode The node label should go red When turned on the node status should go back to yellow </description>
    </item>
    <item>
      <title>Shut down host then delete hosted VM</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete_vm_after_host_shutdown/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/delete_vm_after_host_shutdown/</guid>
      <description>Ref: N/A, legacy test case&#xA;Criteria VM should created and started successfully Node should be unavailable after shutdown VM should able to be deleted Verify Steps: Install Harvester with at least 2 nodes Create a image for VM creation Create a VM vm1 and start it vm1 should started successfully Power off the node hosting vm1 the node should becomes unavailable on dashboard Delete vm1, vm1 should be deleted successfully </description>
    </item>
    <item>
      <title>SSL Certificate</title>
      <link>https://harvester.github.io/tests/manual/advanced/ssl-certificate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/ssl-certificate/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/761&#xA;Verify Items generated kubeconfig is able to access kubenetes API new node able to join the cluster using the configured Domain Name create node with ssl-certificates settings is working as expected. Case: Kubeconfig Install Harvester with at least 2 nodes Generate self-signed TLS certificates from https://www.selfsignedcertificate.com/ with specific name Navigate to advanced settings, edit ssl-certificates settings Update generated .cert file to CA and Public Certificate, .key file to Private Key Relogin with domain name Navigate to Support page, then Click Download KubeConfig, file should named local.</description>
    </item>
    <item>
      <title>Start Host in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</guid>
      <description>For Host that is in maintenance mode and turned off Start host Expected Results Host should turn on Maintenance mode label in hosts list should go from red to yellow Known bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    <item>
      <title>Start VM and stop node Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm-and-stop-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm-and-stop-node/</guid>
      <description> Start the VM In a multi-node setup disconnect/shutdown the node where the VM is running Expected Results You should not be able to start the VM </description>
    </item>
    <item>
      <title>Start VM Negative (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Start the VM Expected Results You should not be able to start the VM </description>
    </item>
    <item>
      <title>Stop VM Negative (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-stop-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-stop-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Stop the VM Expected Results The VM list should quickly update to not running, or some other error state </description>
    </item>
    <item>
      <title>Support configuring a VLAN at the management interface in installer config</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1390_support_configuring_a_vlan_at_the_management_interface_in_installer_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1390_support_configuring_a_vlan_at_the_management_interface_in_installer_config/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1390, https://github.com/harvester/harvester/issues/1647&#xA;Verify Steps: Install Harvester with any nodes from PXE Boot with configurd vlan with vlan_id Harvester should installed successfully Login to console, execute ip a s dev mgmt-br.&amp;lt;vlan_id&amp;gt; should have IP and accessible Dashboard should be accessible </description>
    </item>
    <item>
      <title>Support multiple VLAN physical interfaces</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2259-multiple-vlan-physical-interfaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2259-multiple-vlan-physical-interfaces/</guid>
      <description> Related issues: #2259 [FEATURE] Support multiple VLAN physical interfaces Category: Network Verification Steps Create cluster network cn1 Create a vlanconfig config-n1 on cn1 which applied to node 1 only Select an available NIC on the Uplink Create a vlan, the cluster network cn1 vlanconfig and provide valid vlan id 91 Create cluster network cn2 Create a vlanconfig config-n2 on cn2 which applied to node 2 only Select an available NIC on the Uplink Create a vlan, the cluster network cn2 vlanconfig and provide valid vlan id 92 Create cluster network cn3 Create a vlanconfig config-n3 on cn3 which applied to node 3 only Select an available NIC on the Uplink Create a vlan, select the cluster network cn3 vlanconfig and provide valid vlan id 93 Create a VM, use the vlan id 1 and specific at any node Create a VM, use the vlan id 91 and specified at node1 Create another VM, use the vlan id 92 Expected Results Can create different vlan on each cluster network Can create VM using vlan id 91 and retrieve IP address correctly Can create VM using vlan id 92 and retrieve IP address correctly Can create VM using vlan id 1 and retrieve IP address correctly </description>
    </item>
    <item>
      <title>Support private registry for Rancher agent image in Air-gap</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2176-airgap-private-registry-rancher-agent-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2176-airgap-private-registry-rancher-agent-image/</guid>
      <description>Related issues: #2176 [Enhancement] Air-gap operation: Support using a private registry for Rancher agent image Category: Rancher Integration Verification Steps Environment Setup Use vagrant-pxe-harvester to create a harvester cluster. Create another VM myregistry and set it in the same virtual network. In myregistry VM: Install docker. Run following commands: mkdir auth docker run \ --entrypoint htpasswd \ httpd:2 -Bbn testuser testpassword &amp;gt; auth/htpasswd mkdir -p certs openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.</description>
    </item>
    <item>
      <title>Support Volume Clone</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2293_support_volume_clone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2293_support_volume_clone/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2293&#xA;Verify Steps: Install Harvester with any nodes Create an Image for VM creation Create vm1 with the image and an additional data volume disk-1 Navigate to Volumes, clone disk-0 and disk-1 which attached to vm1 by clicking Clone Volume Create vm2 with cloned disk-0 and disk-1 vm2 should started successfully Login to vm1, execute following commands: fdisk /dev/vdb with new and primary partition mkfs.ext4 /dev/vdb1 mkdir vdb &amp;amp;&amp;amp; mount -t ext4 /dev/vdb1 vdb ping 127.</description>
    </item>
    <item>
      <title>Support volume hot plug live migrate</title>
      <link>https://harvester.github.io/tests/manual/live-migration/support-volume-hot-unplug-live-migrate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/support-volume-hot-unplug-live-migrate/</guid>
      <description> Related issues: #1401 Support volume hot-unplug Category: Storage Environment setup Setup an airgapped harvester&#xA;Create an 3 nodes harvester cluster with large size disks Verification Steps Scenario2: Live migrate VM not have hot-plugged volume before, do hot-plugged the unplugged. Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click Detach volume Add volume again Migrate VM from one node to another Detach volume Add unplugged volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    <item>
      <title>Support Volume Hot Unplug (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/</guid>
      <description> Related issues: #1401 Support volume hot-unplug Category: Storage Environment setup Setup an airgapped harvester&#xA;Create an 3 nodes harvester cluster with large size disks Scenario1: Live migrate VM already have hot-plugged volume to new node, then detach (hot-unplug) it Verification Steps Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click de-attach volume Add volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    <item>
      <title>Support Volume Snapshot</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2294_support_volume_snapshot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2294_support_volume_snapshot/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2294&#xA;Verify Steps: Install Harvester with any nodes Create an Image for VM creation Create vm1 with the image and an additional data volume disk-1 Login to vm1, execute following commands: fdisk /dev/vdb with new and primary partition mkfs.ext4 /dev/vdb1 mkdir vdb &amp;amp;&amp;amp; mount -t ext4 /dev/vdb1 vdb ping 127.0.0.1 | tee -a vdb/test Navigate to Volumes, then click Take Snapshot button on disk-1 of vm1 into vm1-disk-2 Navigate to Virtual Machines, then update vm1 to add existing volume vm1-disk-2 Login to vm1 then mount /dev/vdb1(disk-1) and /dev/vdc1(disk-2) into vdb and vdc test file should be appeared in both folders of vdb and vdc test file should not be empty in both folders of vdb and vdc </description>
    </item>
    <item>
      <title>Switch the vlan interface of harvester node</title>
      <link>https://harvester.github.io/tests/manual/network/switch-the-vlan-interface-of-harvester-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/switch-the-vlan-interface-of-harvester-node/</guid>
      <description>Related issues: #1464 VM pods turn to the terminating state after switching the VLAN interface Category: Network Verification Steps User ipxe-example to build up 3 nodes harvester Login harvester dashboard -&amp;gt; Access Settings Enable vlan network with harvester-mgmt NIC interface Create a VM using harvester-mgmt Disable vlan network Enable vlan network and select bond0 interface Check host and vm is working Directly switch network interface from bond0 to harvester-mgmt without disable it.</description>
    </item>
    <item>
      <title>Sync harvester node&#39;s topology labels to rke2 guest-cluster&#39;s node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1418-sync-topology-labels-to-rke2-guest-cluster-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1418-sync-topology-labels-to-rke2-guest-cluster-node/</guid>
      <description>Related issues: #1418 Support topology aware scheduling of guest cluster workloads Verification Steps Add topology labels(topology.kubernetes.io/region, topology.kubernetes.io/zone) to the Harvester node:&#xA;In Harvester UI, select Hosts page. Click hosts&amp;rsquo; Edit Config. Select Labels page, click Add Labels. Fill in, eg, Key: topology.kubernetes.io/zone, Value: zone1. Create harvester guest-cluster from rancher-UI.&#xA;Wait for the guest-cluster to be created successfully and check if the guest-cluster node labels are consistent with the harvester nodes.</description>
    </item>
    <item>
      <title>Sync image display name to image labels</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2630-sync-image-display-name-to-image-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2630-sync-image-display-name-to-image-labels/</guid>
      <description>Related issues: #2630 [FEATURE] Sync image display_name to image labels Category: Image Verification Steps Login harvester dashboard Access the Preference page Enable developer tool Create an ubuntu focal image from url https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img View API of the created image Check can found the display name in the image API content Create the same ubuntu focal image from previous url again which would bring the same display name Check would be denied with error message Create a different ubuntu focal image with the same display name Expected Results In image API content, label harvesterhci.</description>
    </item>
    <item>
      <title>Take host out of maintenance mode that has been rebooted	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</guid>
      <description> For host in maintenance mode that has been rebooted take host out of maintenance mode Expected Results Host should go to Active Label shbould go green </description>
    </item>
    <item>
      <title>Take host out of maintenance mode that has not been rebooted (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</guid>
      <description> For host in maintenance mode that has not been rebooted take host out of maintenance mode Expected Results Host should go to Active Label shbould go green </description>
    </item>
    <item>
      <title>Target Harvester by setting the variable kubeconfig with your kubeconfig file in the provider.tf file (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-variasble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-variasble/</guid>
      <description> Define the kubeconfig variable in the provider.tf file terraform { required_providers { harvester = { source = &amp;#34;registry.terraform.io/harvester/harvester&amp;#34; version = &amp;#34;~&amp;gt; 0.1.0&amp;#34; } } } provider &amp;#34;harvester&amp;#34; { kubeconfig = &amp;#34;/path/of/my/kubeconfig&amp;#34; } Check if you can interact with the Harvester by creating resource like a SSH key Execute the terraform apply command Expected Results The resource should be created Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Check if you can see your resource in the Harvester WebUI </description>
    </item>
    <item>
      <title>Target Harvester with the default kubeconfig located in $HOME/.kube/config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-home/</guid>
      <description> Make sure the kubeconfig is defined in the file $HOME/.kube/config Check if you can interact with the Harvester by creating resource like a SSH key Execute the terraform apply command Expected Results The resource should be created Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Check if you can see your resource in the Harvester WebUI </description>
    </item>
    <item>
      <title>template with EFI (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</guid>
      <description>Related issues: #2577 [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected. Category Template Verification Steps Go to Template, create a VM template with Boot in EFI mode selected. Go to Virtual Machines, click Create, select Multiple instance, type in a random name prefix, and select the VM template we just created. Go to Advanced Options, for now this EFI checkbox should be checked without any issue.</description>
    </item>
    <item>
      <title>Temporary network disruption</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</guid>
      <description> Create a vms on the cluster. Disable network of a node for sometime. e.g. 5 sec, 5 mins Expected Results VM should be accessible after the network is up. </description>
    </item>
    <item>
      <title>Terraform import VLAN</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</guid>
      <description>Related issues: #2261 [FEATURE] enhance terraform network to not pruge route_cidr and route_gateway Category: Terraform Verification Steps Install Harvester with any nodes Install terraform-harvester-provider (using master-head for testing) Execute terraform init Create the file network.tf as following snippets, then execute terraform import harvester_clusternetwork.vlan vlan to import default vlan settings resource &amp;#34;harvester_clusternetwork&amp;#34; &amp;#34;vlan&amp;#34; { name = &amp;#34;vlan&amp;#34; enable = true default_physical_nic = &amp;#34;harvester-mgmt&amp;#34; } resource &amp;#34;harvester_network&amp;#34; &amp;#34;vlan1&amp;#34; { name = &amp;#34;vlan1&amp;#34; namespace = &amp;#34;harvester-public&amp;#34; vlan_id = 1 route_mode = &amp;#34;auto&amp;#34; } execute terraform apply Login to dashboard then navigate to Advanced/Networks, make sure the Route Connectivity becomes Active Execute terraform apply again and many more times Expected Results Resources should not be changed or added or destroyed.</description>
    </item>
    <item>
      <title>Terraform Rancher2 Provider Testing</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher2-terraform-integration/terraform_rancher2_provider_testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher2-terraform-integration/terraform_rancher2_provider_testing/</guid>
      <description>Ref: https://github.com/rancher/terraform-provider-rancher2/issues/1009&#xA;Test Information Environment Rancher: v2.7.X Environment for Harvester: bare-metal or qemu Harvester Version: v1.1.X ui-source Option: Auto Rancher2 Terraform Provider Plugin: v3.0.X rancher2 Test Setup Rancher2 Terraform Provider: make sure terraform is installed at version equal or greater than 1.3.9, ie: sudo apt install terraform utilize the setup-provider.sh script from the rancher2 terraform provider repo if testing an rc it would look something like ./setup-provider.sh rancher2 v3.0.0-rc1 ensure the provider is installed, can cross check the directory structures under ~/.</description>
    </item>
    <item>
      <title>Terraformer import KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</guid>
      <description>Related issues: #2604 [BUG] Terraformer imported VLAN always be 0 Category: Terraformer Verification Steps Install Harvester with any nodes Login to dashboard, navigate to: Advanced/Settings -&amp;gt; then enabledvlan` Navigate to Advanced/Networks and Create a Network which Vlan ID is not 0 Navigate to Support Page and Download KubeConfig file Initialize a terraform environment, download Harvester Terraformer Execute command terraformer import harvester -r network to generate terraform configuration from the cluster Generated file generated/harvester/network/network.</description>
    </item>
    <item>
      <title>Test a deployment with ALL resources at the same time (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/deployment-all-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/deployment-all-resources/</guid>
      <description>Re-use the previous generated TF files and group them all either in one directory or in the same file Generates a speculative execution plan with terraform plan command Create the resources with terraform apply command Check that all resources are correctly created/running on the Harvester cluster Destroy the resources with the command terraform destroy Expected Results Refer to the harvester_ssh_key resource expected results</description>
    </item>
    <item>
      <title>Test aborting live migration</title>
      <link>https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</guid>
      <description> On a VM that is turned on select migrate Start the migration Abort the migration Expected Results You should see the status move to migrating You should see the status move to aborting migration You should see the status move to running The VM should pass health checks </description>
    </item>
    <item>
      <title>Test NTP server timesync</title>
      <link>https://harvester.github.io/tests/manual/hosts/1535-test-ntp-timesync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1535-test-ntp-timesync/</guid>
      <description> Related issues: #1535 NTP daemon in host OS Environment setup This should be on at least a 3 node setup that has been running for several hours that had NTP servers setup during install&#xA;Verification Steps SSH into nodes and verify times are close Verify NTP is active with sudo timedatectl status Expected Results Times should be within a minute of each other NTP should show as active </description>
    </item>
    <item>
      <title>Test NTP server timesync</title>
      <link>https://harvester.github.io/tests/manual/misc/1535-test-ntp-timesync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/1535-test-ntp-timesync/</guid>
      <description> Related issues: #1535 NTP daemon in host OS Environment setup This should be on at least a 3 node setup that has been running for several hours that had NTP servers setup during install&#xA;Verification Steps SSH into nodes and verify times are close Verify NTP is active with sudo timedatectl status Expected Results Times should be within a minute of each other NTP should show as active </description>
    </item>
    <item>
      <title>Test the harvester_clusternetwork resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-clusternetwork-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-clusternetwork-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    <item>
      <title>Test the harvester_image resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-image-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-image-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    <item>
      <title>Test the harvester_network resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-network-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-network-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    <item>
      <title>Test the harvester_ssh_key resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-ssh-key-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-ssh-key-resource/</guid>
      <description>These following steps must be done for every resources, for avoiding repetitions, look at the detailed instructions at the beginning of the page.&#xA;Import a resource Generates a speculative execution plan with terraform plan command Create the resource with terraform apply command Use terraform plan again Use terraform apply again Destroy the resource with the command terraform destroy Expected Results The resource is well imported in the terraform.tfstate file and you can print it with the terraform show command The command should display the difference between the actual status and the configured status Plan: 1 to add, 0 to change, 0 to destroy.</description>
    </item>
    <item>
      <title>Test the harvester_virtualmachine resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-virtualmachine-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-virtualmachine-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    <item>
      <title>Test the harvester_volume resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-volume-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-volume-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    <item>
      <title>Test zero downtime for live migration download test</title>
      <link>https://harvester.github.io/tests/manual/live-migration/zero-downtime-download-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/zero-downtime-download-test/</guid>
      <description> Connect to VM via console Start a large file download Live migrate VM to new host Verify that file download does not fail Expected Results Console should open VM should start to migrate File download should </description>
    </item>
    <item>
      <title>Test zero downtime for live migration ping test</title>
      <link>https://harvester.github.io/tests/manual/live-migration/zero-downtime-ping-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/live-migration/zero-downtime-ping-test/</guid>
      <description> Continually ping VM Verify that ping is getting a response Live migrate VM to new host Verify that ping continues Expected Results Ping should get response VM should start to migrate Ping should not get any dropped packets </description>
    </item>
    <item>
      <title>Testing Harvester Storage Tiering</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2147-testing-storage-tiering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2147-testing-storage-tiering/</guid>
      <description>Related issues: #2147 [[FEATURE] Storage Tiering Category: Images Volumes VirtualMachines Test Setup Steps Have a Harvester Node with 3 Disks in total (one main disk, two additional disks), ideally the two additional disks should be roughly 20/30Gi for testing Add the additional disks to the harvester node (you may first need to be on the node itself and do a sudo gdisk /dev/sda and then w and y to write the disk identifier so that Harvester can recogonize the disk, note you shouldn&amp;rsquo;t need to build partitions) Add the disks to the Harvester node via: Hosts -&amp;gt; Edit Config -&amp;gt; Storage -&amp;gt; &amp;ldquo;Add Disk&amp;rdquo; (call-to-action), they should auto populate with available disks that you can add Save Navigate back to Hosts -&amp;gt; Host -&amp;gt; Edit Config -&amp;gt; Storage, then add a Host Tag, and a unique disk tag for every disk (including the main disk/default-disk) Verification Steps with Checks Navigate to Advanced -&amp;gt; Storage Classes -&amp;gt; Create (Call-To-Action), create a storageClass &amp;ldquo;sc-a&amp;rdquo;, specify nodeSelector (choose host), diskSelector (choose one of the unique disk tags), number of replicas (1-12) Also create a storageClass &amp;ldquo;sc-b&amp;rdquo;, specify nodeSelector (choose host), diskSelector (choose one of the unique disk tags), number of replicas (1-12) Create a new image img-a, specify storageClassName to sc-a Create a new vm vm1 use the image img-a Check the replicas number and location of rootdisk volume in longhorn UI Create a new volume volume-a by choose source=image img-a Add the volume volume-a to vm vm1 Check the replicas number and location of volume volume-a in longhorn UI: volume-a, should also be seen in kubectl get pv --all-namespaces (where &amp;ldquo;Claim&amp;rdquo; is volume-a) with the appropriate storage class also with something like kubectl describe pv/pvc-your-uuid-from-get-pv-call-with-volume-a --all-namespaces: can audit volume attributes like: VolumeAttributes: diskSelector=second migratable=true nodeSelector=node-2 numberOfReplicas=1 share=true staleReplicaTimeout=30 storage.</description>
    </item>
    <item>
      <title>The count of volume snapshots should not include VM&#39;s snapshots</title>
      <link>https://harvester.github.io/tests/manual/_incoming/3004-volume-snaphost-not-include-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/3004-volume-snaphost-not-include-vm/</guid>
      <description> Related issues: #3004 [BUG] The count of volume snapshots should not include VM&amp;rsquo;s snapshots Category: Volume Verification Steps Create a VM vm1 Take a VM snapshot Check the volume snapshot page Check the VM snapshot page Expected Results When one VM is created Only VM snap are created The count of volume snapshots should not include VM&amp;rsquo;s snapshots. </description>
    </item>
    <item>
      <title>Timeout option for support bundle</title>
      <link>https://harvester.github.io/tests/manual/advanced/support_bundle_timeout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/support_bundle_timeout/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1585&#xA;Verify Items An Timeout Option can be configured for support bundle Error message will display when reach timeout Case: Generate support bundle but hit timeout Install Harvester with at least 2 nodes Navigate to Advanced Settings, modify support-bundle-timeout to 2 Navigate to Support, Click Generate Support Bundle, and force shut down one of the node in the mean time. 2 mins later, the function will failed with an Error message pop up as the snapshot </description>
    </item>
    <item>
      <title>toggle harvester node driver with the harvester global flag</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/toggle-harvester-node-driver-with-harvester-global-flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/toggle-harvester-node-driver-with-harvester-global-flag/</guid>
      <description>Related issue: #1465 toggle harvester node driver with the harvester global flag Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Environment preparation as above steps Open global setting -&amp;gt; feature flag in rancher Check harvester feature flag Open cluster management -&amp;gt; Driver page Check harvester node driver Deactivate harvester feature flag Activate harvester feature flag Deactivate harvester node driver Activate harvester node driver Deactivate both harvester flag and node driver Activate harvester feature flag Expected Results Harvester feature flag will be enabled by default and turned on harvester node driver accordingly If the feature flag was turned off, nothing will change to the Harvester node driver.</description>
    </item>
    <item>
      <title>Topology aware scheduling of guest cluster workloads</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1418-2383-topology-scheduling-of-guest-cluster-workloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1418-2383-topology-scheduling-of-guest-cluster-workloads/</guid>
      <description> Related issues: #1418 [FEATURE] Support topology aware scheduling of guest cluster workloads Related issues: #2383 [backport v1.0.3] [FEATURE] Support topology aware scheduling of guest cluster workloads Category: Rancher integration Verification Steps Environment preparation as above steps Access Harvester node config page Add the following node labels with values topology.kubernetes.io/zone topology.kubernetes.io/region Provision an RKE2 cluster Wait for the provisioning complete Access RKE2 guest cluster Access the RKE2 cluster in Cluster Management page Click + to add another node Access the RKE2 cluster node page Wait until the second node created Edit yaml of the second node Check the harvester node label have propagated to the guest cluster node Expected Results The topology encoded in the Harvester cluster node labels Can be correctly propagated to the additional node of the RKE2 guest cluster </description>
    </item>
    <item>
      <title>Try to add a network with no name (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-add-network-no-name/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/negative-add-network-no-name/</guid>
      <description> Navigate to the networks page in harvester Click Create Don&amp;rsquo;t add a name Add a VLAN ID Click Create Expected Results You should get an error that says you need to add a name </description>
    </item>
    <item>
      <title>Turn off host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</guid>
      <description>Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Shut down Host Expected Results Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode host should shut down Maintenance mode label in hosts list should go red Known bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    <item>
      <title>UI enables option to display password on login page</title>
      <link>https://harvester.github.io/tests/manual/authentication/ui_password_show_btn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/ui_password_show_btn/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1550&#xA;Verify Items Password field in login page can be toggle show/hide Case: Toggle of Password field install harvester with any nodes setup password logout then login with password toggled </description>
    </item>
    <item>
      <title>Unable to stop VM which in starting state</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2263_unable_to_stop_vm_which_in_starting_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2263_unable_to_stop_vm_which_in_starting_state/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2263&#xA;Verify Steps: Install Harvester with any nodes Create an Windows iso image for VM creation Create the Windows VM by using the iso image When the VM in Starting state, Stop button should able to click and work as expected </description>
    </item>
    <item>
      <title>Update image labels after deleting source VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1602-update-labels-on-image-after-vm-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1602-update-labels-on-image-after-vm-delete/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; update image &amp;ldquo;img-1&amp;rdquo; labels Expected Results image &amp;ldquo;img-1&amp;rdquo; will be updated </description>
    </item>
    <item>
      <title>Update image labels after deleting source VM(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/1602-update-labels-on-image-after-vm-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/1602-update-labels-on-image-after-vm-delete/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; update image &amp;ldquo;img-1&amp;rdquo; labels Expected Results image &amp;ldquo;img-1&amp;rdquo; will be updated </description>
    </item>
    <item>
      <title>Upgrade guest cluster kubernetes version can also update the cloud provider chart version</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2546-upgrade-guest-k8s-version-upgrade-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2546-upgrade-guest-k8s-version-upgrade-cloud-provider/</guid>
      <description>Related issues: #2546 [BUG] Harvester Cloud Provider is not able to deploy upgraded container after upgrading the cluster Category: Rancher integration Verification Steps Prepare the previous stable Rancher rc version and Harvester Update rke-metadata-config to {&amp;quot;refresh-interval-minutes&amp;quot;:&amp;quot;1440&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;https://yufa-dev.s3.ap-east-1.amazonaws.com/data.json&amp;quot;} in global settings Update the ui-dashboard-index to https://releases.rancher.com/dashboard/latest/index.html Set ui-offline-preferred to Remote Refresh web page (ctrl + r) Open Create RKE2 cluster page Check the show deprecated kubernetes patched versions Select v1.23.8+rke2r1 Finish the RKE2 cluster provision Check the current cloud provider version in workload page Edit RKE2 cluster, upgrade the kubernetes version to 1.</description>
    </item>
    <item>
      <title>Upgrade Harvester from new cluster network design (after v1.1.0)</title>
      <link>https://harvester.github.io/tests/manual/upgrade/upgrade-from-new-network-design/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/upgrade-from-new-network-design/</guid>
      <description>Category: Upgrade Harvester Environment requirement Network environment has available VLAN id setup on DHCP server DHCP server has setup the IP range can allocate to above VLAN id Harvester node can route to DHCP server through VLAN id to retrieve IP address Network has at least two NICs Suggest not to use SMR type HDD disk We can select VM or Bare machine network setup according to available resource&#xA;Virtual Machine environment setup Clone ipxe-example https://github.</description>
    </item>
    <item>
      <title>Upgrade Harvester from traditonal cluster network design (before v1.1.0)</title>
      <link>https://harvester.github.io/tests/manual/upgrade/upgrade-from-traditional-network-design/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/upgrade-from-traditional-network-design/</guid>
      <description>Category: Upgrade Harvester Environment requirement Network environment has available VLAN id setup on DHCP server DHCP server has setup the IP range can allocate to above VLAN id Harvester node can route to DHCP server through VLAN id to retrieve IP address Network has at least two NICs Network has at least two NICs Suggest not to use SMR type HDD disk We can select VM or Bare machine network setup according to available resource</description>
    </item>
    <item>
      <title>Upgrade Harvester in Fully Airgapped Environment</title>
      <link>https://harvester.github.io/tests/manual/upgrade/fully-airgapped-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/fully-airgapped-upgrade/</guid>
      <description>Category: Upgrade Harvester Environment requirement Airgapped Network without internet connectivity Network environment has available VLAN id setup on DHCP server DHCP server has setup the IP range can allocate to above VLAN id Harvester node can route to DHCP server through VLAN id to retrieve IP address Network has at least two NICs Suggest not to use SMR type HDD disk We can select VM or Bare machine network setup according to your available resource Virtual Machine environment setup Clone ipxe-example https://github.</description>
    </item>
    <item>
      <title>Upgrade Harvester on node that has bonded NICs for management interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/3045-upgrade-with-bonded-nic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/3045-upgrade-with-bonded-nic/</guid>
      <description>Related issues: #3045 [BUG] Harvester Upgrade 1.0.3 to 1.1.0 does not handle multiple SLAVE in BOND for management interface Category: Upgrade Environment Setup This is to be done on a Harvester cluster where the NICs were configured to be bonded on install for the management interface. This can be done in one of two ways.&#xA;Single node virtualized environment Bare metal environment with at least two NICs (this should really be done on 10gig NICs, but can be done on gigabit) Both NICs should be on the same VLAN/network with the same subnet</description>
    </item>
    <item>
      <title>Upgrade Harvester with bonded NICs on network</title>
      <link>https://harvester.github.io/tests/manual/upgrade/bonded-nics-traditional-network-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/bonded-nics-traditional-network-upgrade/</guid>
      <description>Related issues: #3047 [BUG] migrate_harv_mgmt_to_mgmt_br.sh should remove ClusterNetwork resource Category: Upgrade Harvester Environment setup from v1.0.3 upgrade to v1.1.1 Clone ipxe-example and switch to v1.0 branch Add three additional Network interface in Vagrantfile harvester_node.vm.network &amp;#39;private_network&amp;#39;, libvirt__network_name: &amp;#39;harvester&amp;#39;, mac: @settings[&amp;#39;harvester_network_config&amp;#39;][&amp;#39;cluster&amp;#39;][node_number][&amp;#39;mac&amp;#39;] harvester_node.vm.network &amp;#39;private_network&amp;#39;, libvirt__network_name: &amp;#39;harvester&amp;#39; harvester_node.vm.network &amp;#39;private_network&amp;#39;, libvirt__network_name: &amp;#39;harvester&amp;#39; harvester_node.vm.network &amp;#39;private_network&amp;#39;, libvirt__network_name: &amp;#39;harvester&amp;#39; Edit the config-create.yaml.j2 and config-join.yaml.j2 in /ansible/role/harvester/template/ Add the cluster_network and defaultPysicalNIC to harvester-mgmt cluster_networks: vlan: enable: true description: &amp;#34;some description about this cluster network&amp;#34; config: defaultPhysicalNIC: harvester-mgmt Bond multiple NICs on harvester-mgmt and harvester-vlan networks networks: harvester-mgmt: interfaces: - name: {{ settings[&amp;#39;harvester_network_config&amp;#39;][&amp;#39;cluster&amp;#39;][0][&amp;#39;mgmt_interface&amp;#39;] }} # The management interface name - name: ens9 method: dhcp bond0: interfaces: - name: {{ settings[&amp;#39;harvester_network_config&amp;#39;][&amp;#39;cluster&amp;#39;][0][&amp;#39;vagrant_interface&amp;#39;] }} method: dhcp harvester-vlan: interfaces: - name: ens7 - name: ens8 method: none Verification Steps Provision previous version of Harvester cluster</description>
    </item>
    <item>
      <title>Upgrade Harvester with HDD Disks</title>
      <link>https://harvester.github.io/tests/manual/upgrade/upgrade-with-hdd-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/upgrade-with-hdd-disk/</guid>
      <description>Category: Upgrade Harvester Environment requirement Network environment has available VLAN id setup on DHCP server DHCP server has setup the IP range can allocate to above VLAN id Harvester node can route to DHCP server through VLAN id to retrieve IP address Network has at least two NICs Use HDD disk with SMR type or slow I/O speed n1-103:~ # smartctl -a /dev/sda smartctl 7.2 2021-09-14 r5237... === START OF INFORMATION SECTION === Model Family: Seagate BarraCuda 3.</description>
    </item>
    <item>
      <title>Upgrade Harvester with IPv6 DHCP</title>
      <link>https://harvester.github.io/tests/manual/upgrade/ipv6-dhcp-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/upgrade/ipv6-dhcp-upgrade/</guid>
      <description>Related issues: #2962 [BUG] Host IP is inconsistent Category: Upgrade Harvester Environment setup Open the virtual machine manager&#xA;Open the Connection Details -&amp;gt; Virtual Networks&#xA;Create a new virtual network workload&#xA;Add the following XML content&#xA;&amp;lt;network&amp;gt; &amp;lt;name&amp;gt;workload&amp;lt;/name&amp;gt; &amp;lt;uuid&amp;gt;ac62e6bf-6869-41a9-a2b7-25c06c7601c9&amp;lt;/uuid&amp;gt; &amp;lt;forward mode=&amp;#34;nat&amp;#34;&amp;gt; &amp;lt;nat&amp;gt; &amp;lt;port start=&amp;#34;1024&amp;#34; end=&amp;#34;65535&amp;#34;/&amp;gt; &amp;lt;/nat&amp;gt; &amp;lt;/forward&amp;gt; &amp;lt;bridge name=&amp;#34;virbr5&amp;#34; stp=&amp;#34;on&amp;#34; delay=&amp;#34;0&amp;#34;/&amp;gt; &amp;lt;mac address=&amp;#34;52:54:00:7b:ed:99&amp;#34;/&amp;gt; &amp;lt;domain name=&amp;#34;workload&amp;#34;/&amp;gt; &amp;lt;ip address=&amp;#34;192.168.101.1&amp;#34; netmask=&amp;#34;255.255.255.0&amp;#34;&amp;gt; &amp;lt;dhcp&amp;gt; &amp;lt;range start=&amp;#34;192.168.101.128&amp;#34; end=&amp;#34;192.168.101.254&amp;#34;/&amp;gt; &amp;lt;/dhcp&amp;gt; &amp;lt;/ip&amp;gt; &amp;lt;ip family=&amp;#34;ipv6&amp;#34; address=&amp;#34;fd7d:844d:3e17:f3ae::1&amp;#34; prefix=&amp;#34;64&amp;#34;&amp;gt; &amp;lt;dhcp&amp;gt; &amp;lt;range start=&amp;#34;fd7d:844d:3e17:f3ae::100&amp;#34; end=&amp;#34;fd7d:844d:3e17:f3ae::1ff&amp;#34;/&amp;gt; &amp;lt;/dhcp&amp;gt; &amp;lt;/ip&amp;gt; &amp;lt;/network&amp;gt; Change the bridge name to a new one</description>
    </item>
    <item>
      <title>Upgrade support of audit and event log</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2750-support-audit-event-log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2750-support-audit-event-log/</guid>
      <description>Related issues: #2750 [FEATURE] Upgrade support of audit and event log Category: Logging Audit Verification Steps Prepare v1.0.3 cluster, single-node and multi-node need to be tested separately Upgrade to v1.1.0-rc2 / master-head The upgrade should be successful, if not, check log and POD errors After upgrade, check following PODs and files, there should be no error Expected Results Check both Single and Multi nodes upgrade of the following:&#xA;Check the following files and pods have no error</description>
    </item>
    <item>
      <title>Upload Cloud Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/upload-cloud-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/upload-cloud-image/</guid>
      <description> Upload image to images page Create new vm with image using appropriate template Run VM health checks Expected Results Image should upload Health checks should pass </description>
    </item>
    <item>
      <title>Upload image that is invalid</title>
      <link>https://harvester.github.io/tests/manual/images/negative-upload-invalid-image-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/negative-upload-invalid-image-file/</guid>
      <description>steTry to upload invalid image file to images page Something like dmg, or tar.gzps Expected Results You should get an error Known Bugs https://github.com/harvester/harvester/issues/1425</description>
    </item>
    <item>
      <title>Upload ISO Image(e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/images/upload-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/upload-iso-image/</guid>
      <description> Upload image to images page Create new vm with image using appropriate template Run VM health checks Expected Results Image should upload Health checks should pass </description>
    </item>
    <item>
      <title>Use a non-admin user</title>
      <link>https://harvester.github.io/tests/manual/node-driver/non-admin-user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/non-admin-user/</guid>
      <description>create harvester user ltang, password ltang add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    <item>
      <title>Use template to create cluster through virtualization management</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/use-template-to-create-cluster-through-virtualization-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/use-template-to-create-cluster-through-virtualization-management/</guid>
      <description>Related issue: #1620 User is unable to use template to create cluster through virtualization management Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Import harvester from rancher through harvester settings Access harvester from rancher virtualization management page Open Virtual Machine page Click create Check Use VM Template Select one of the template Create VM according to the template Expected Results Access harvester from Rancher, on virtual machine page can load default three template to create VM.</description>
    </item>
    <item>
      <title>Validate network connectivity external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</guid>
      <description> Create a new VM Make sure that the network is set to the external VLAN with bridge as the type Ping VM Attempt to SSH to VM Expected Results VM should be created You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    <item>
      <title>Validate network connectivity invalid external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-network-connectivity-invalid-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/negative-network-connectivity-invalid-vlan/</guid>
      <description> Create a new VM Make sure that the network is set to the external VLAN with bridge as the type and a VLAN ID that isn&amp;rsquo;t valid for your network Ping VM Attempt to SSH to VM Expected Results VM should be created You should not be able to ping the VM from an external network You should not be able to SSH to VM </description>
    </item>
    <item>
      <title>Validate network connectivity management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/validate-network-management-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/validate-network-management-network/</guid>
      <description> Create a new VM Make sure that the network is set to the management network with masquerade as the type Ping VM Attempt to SSH to VM Expected Results VM should be created You should not be able to ping the VM from an external network You should not be able to SSH to VM </description>
    </item>
    <item>
      <title>Validate QEMU agent installation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1235-check-qemu-installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1235-check-qemu-installation/</guid>
      <description> Related issues: #1235 QEMU agent is not installed by default when creating VMs Verification Steps Creat openSUSE VM Start VM check for qemu-ga package Create Ubuntu VM Start VM Check for qemu-ga package Expected Results VMs should start Packages should be present </description>
    </item>
    <item>
      <title>Validate volume shows as in use when attached (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/validate-volume-shows-in-use-while-attached/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/validate-volume-shows-in-use-while-attached/</guid>
      <description> Navigate to Volumes and check for a volume in use by a VM Verify that the state says In Use Expected Results State should show correctly </description>
    </item>
    <item>
      <title>Verify &#34;Add Node Pool&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/verify-add-node-pool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/node-driver/verify-add-node-pool/</guid>
      <description> Create a cluster of 3 nodes, One node with etcd, Control Plane, Worker, the other two with Worker The cluster is created successfully, use the command kubectl get node to view the node roles Expected Results The status of the created cluster shows active show the 3 created node status running in harvester&amp;rsquo;s vm list the information displayed on rancher and harvester matches the template configuration Check that the node role is correct </description>
    </item>
    <item>
      <title>Verify and Configure Networking Connection (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-network-connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-network-connection/</guid>
      <description>Provide the hostName Select management NIC bond Select the IPv4 (Automatic and Static) Expected Results This value of hostname should be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn&amp;rsquo;t offer a hostname and this value is empty, a random hostname will be generated.</description>
    </item>
    <item>
      <title>Verify Configuring SSH keys</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-ssh/</guid>
      <description>Provide SSH keys while installing the Harvester. Verify user is able to login the node using that ssh key. Expected Results User should be able to login to the node using that ssh key.</description>
    </item>
    <item>
      <title>Verify Configuring via HTTP URL</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-http-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-http-config/</guid>
      <description>Provide the remote Harvester config, you can find an example of the config I&amp;rsquo;m using in the deployment test plan description Expected Results Check that all values are taking into account If you are using my config file, check: the node must be off after the installation the nvme and kvm modules are loaded the file /etc/test.txt exists with the correct rights the systcl values the env variable test_env should exist dns configured in /etc/resolv.</description>
    </item>
    <item>
      <title>Verify Enabling maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</guid>
      <description> Navigate to the Hosts page and select the node Click Maintenance Mode Expected Results The existing VM should get migrated to other nodes. Verify the CRDs to see the maintenance mode is enabled. Comments Needs other test cases to be added If VM migration fails How does live migration work What happens if there are no schedulable resources on nodes Check CRDs on hosts On going into maintenance mode kubectl get virtualmachines &amp;ndash;all-namespaces Kubectl get virtualmachines/name -o yaml On coming out of maintenance mode kubectl get virtualmachines &amp;ndash;all-namespaces Kubectl get virtualmachines/name -o yaml Check that maintenance mode host isn&amp;rsquo;t schedulable Fully provision all nodes and try to create a VM It should fail Migration with maintenance mode What if migration gets stuck, can you cancel VMs going to different hosts Canceling maintenance mode P1 Put in maintenance mode Check migration of VMs Check status of VMs modify filesystem on VMs Check status of host Take host out of maintenance mode Check status of host Migrate VMs back to host Check filesystem Create new VMs on host Check status of VMs </description>
    </item>
    <item>
      <title>Verify network data template</title>
      <link>https://harvester.github.io/tests/manual/misc/1634-terms-and-conditions-link/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/misc/1634-terms-and-conditions-link/</guid>
      <description> Related issues: #1634 Welcome screen asks to agree to T&amp;amp;Cs for using Rancher not Harvester Verification Steps Install Harvester Go to management page and see last line (before Continue button) Verify link to SUSE EULA https://www.suse.com/licensing/eula/ Expected Results Link should go to SUSE EULA </description>
    </item>
    <item>
      <title>Verify network data template</title>
      <link>https://harvester.github.io/tests/manual/templates/1655-network-data-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/templates/1655-network-data-template/</guid>
      <description> Related issues: #1655 When using a VM Template the Network Data in the template is not displayed Verification Steps Create new VM template with network data in advanced settings network: version: 1 config: - type: physical name: interface0 subnets: - type: static address: 10.84.99.0/24 gateway: 10.84.99.254 Create new VM and select template Verify that network data is in advanced network config Expected Results network data should show </description>
    </item>
    <item>
      <title>Verify operations like Stop, restart, pause, download YAML, generate template (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</guid>
      <description> Take an existing VM and Press the appropriate buttons for the associated operations Stop Restart Pause Download YAML Generate Template Expected Results All operations should complete successfully </description>
    </item>
    <item>
      <title>Verify SSH key was added from Github during install</title>
      <link>https://harvester.github.io/tests/manual/authentication/verify-github-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/authentication/verify-github-ssh/</guid>
      <description> Add ssh key from Github while installing the Harvester. Login Harvester with github. Expected Results User should be able to logout/login successfully. </description>
    </item>
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/advanced/1661-vm-force-reset-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/1661-vm-force-reset-policy/</guid>
      <description> Related issues: #1661 vm-force-deletion-policy for vm-force-reset-policy Environment setup Setup an airgapped harvester&#xA;Create a 3 node harvester cluster Verification Steps Navigate to advanced settings and edit vm-force-reset-policy Set reset policy to 60 Create VM Run health checks Shut down node that is running VM Check for when it starts to migrate to new Host Expected Results It should migrate after 60 seconds </description>
    </item>
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1660-volume-unit-vm-details/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1660-volume-unit-vm-details/</guid>
      <description> Related issues: #1660 The volume unit on the vm details page is incorrect Verification Steps Create new .1G volume Create new VM Create with raw-image template Add opensuse base image Add .1G Volume Verify size in VM details on volume tab Expected Results Size should show as .1G </description>
    </item>
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/volumes/1660-volume-unit-vm-details/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/1660-volume-unit-vm-details/</guid>
      <description> Related issues: #1660 The volume unit on the vm details page is incorrect Verification Steps Create new .1G volume Create new VM Create with raw-image template Add opensuse base image Add .1G Volume Verify size in VM details on volume tab Expected Results Size should show as .1G </description>
    </item>
    <item>
      <title>Verify that VMs stay up when disks are evicted</title>
      <link>https://harvester.github.io/tests/manual/volumes/1334-evict-disks-check-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/volumes/1334-evict-disks-check-vms/</guid>
      <description>Related issues #1334 Volumes fail with Scheduling Failure after evicting disc on multi-disc node #5307 Replicas should be evicted and rescheduled to other disks before removing extra disk Verification Steps Created 3 nodes Harvester.&#xA;Added formatted disk (called disk A) to node0 VM in the harvester node page.&#xA;Added disk tag test on following disk in the longhorn page.&#xA;disk A of node0 root disk of node1 root disk of node2 Created storage class with disk tag test and replica 3.</description>
    </item>
    <item>
      <title>Verify the external link at the bottom of the page</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-bottom-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/ui/verify-bottom-links/</guid>
      <description> Click all the external links available at the bottom of the page - Docs, Forums, Slack, File an issue. Click the Generate support bundle at the bottom of the page Expected Results The external links should take user to correct URL in new tab in the browser. The support bundle should be generated once the Generate support bundle. The progress should be shown while the bundle is getting generated. The Generated bundle should have all components logs and Yaml </description>
    </item>
    <item>
      <title>Verify the Filter on the Host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</guid>
      <description> Enter name of a host and verify the nodes get filtered out. Expected Results The edited name should be reflected on the host. </description>
    </item>
    <item>
      <title>Verify the Harvester UI URL (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/ui/verify-url/</guid>
      <description> Navigate to the Harvester UI and verify the URL. Verify the Harvester icon on the left top corner Expected Results The URL should be the management ip + /dashboard redirect to login page if not login redirect to dashboard page if already login </description>
    </item>
    <item>
      <title>Verify the info of the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-node-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/verify-node-info/</guid>
      <description> Navigate to the hosts tab and verify the following. State Name Host IP CPU Memory Storage Size Age Expected Results All the data/status shown on the page should be correct. </description>
    </item>
    <item>
      <title>Verify the installation confirmation screen</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-installation-confirmation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-installation-confirmation/</guid>
      <description>Verify all the details shown on the screen Expected Results The info should reflect all the user filled data.</description>
    </item>
    <item>
      <title>Verify the Installer Options</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-installer-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-installer-options/</guid>
      <description> Verify the following options available while installing the Harvester is working Installation target Node IP Cluster token Password DNS Server VIP HTTP Proxy NTP Address Expected Results Should show all the disks available. Verify the min and max length acceptable for cluster token. Verify the password rule </description>
    </item>
    <item>
      <title>Verify the left side menu (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-left-menu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/ui/verify-left-menu/</guid>
      <description> Check all the menu at the left side of the screen. Verify the preference and logout option is available at the right top of the screen Expected Results The menu should have Dashboard, Hosts, Virtual machines, Volumes, Images and Advance. The Advance menu should have sub menu Templates, backups, network, SSH keys, Users, Cloud config templates, Settings. Clicking on the menu should take user to the respective pages </description>
    </item>
    <item>
      <title>Verify the links which navigate to the internal pages</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-internal-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/ui/verify-internal-links/</guid>
      <description> Click the links available on the pages like on dashboard - host, virtual machines etc Verify the events and resources tabs presents in the pages e.g. - Dashboard, Virtual machines Expected Results The internal link should take user to the correct page in the same tab opened in the browser </description>
    </item>
    <item>
      <title>Verify the options available for image</title>
      <link>https://harvester.github.io/tests/manual/images/verify-options-available-for-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/images/verify-options-available-for-image/</guid>
      <description> Create vm with YAML using the menu option. Download Yaml Verify the downloaded Yaml file. Clone the Image Expected Results All user-specified fields must match what show on GUI: Namespace Name Description URL Labels </description>
    </item>
    <item>
      <title>Verify the Proxy configuration</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-proxy/</guid>
      <description>Provide a valid proxy address, verify it works after installation is complete. Provide empty proxy address. Expected Results For empty proxy address, by default DHCP should provide the management url and it should navigate to the Harvester UI.</description>
    </item>
    <item>
      <title>Verify the state for Powered down node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</guid>
      <description> Power down the node and check the state of the node in the cluster Expected Results The node state should show unavilable </description>
    </item>
    <item>
      <title>vGPU/SR-IOV GPU</title>
      <link>https://harvester.github.io/tests/manual/advanced/addons/2764-vgpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/advanced/addons/2764-vgpu/</guid>
      <description>Related issues: #1661 vGPU Support Pre-requisite Enable PCI devices Create a harvester cluster in bare metal mode. Ensure one of the nodes has NIC separate from the management NIC Go to the management interface of the new cluster Go to Advanced -&amp;gt; PCI Devices Validate that the PCI devices aren&amp;rsquo;t enabled Click the link to enable PCI devices Enable PCI devices in the linked addon page Wait for the status to change to Deploy Successful Navigate to the PCI devices page Validate that the PCI devices page is populated/populating with PCI devices Pre-requisite Enable vGPU This can only be ran on a bare metal Harvester cluster that has an Nvidia card that support vGPU.</description>
    </item>
    <item>
      <title>View log function on virtual machine</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/5266-view-log-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/5266-view-log-function/</guid>
      <description>Related issues: #5266 [BUG] Click View Logs option on virtual machine dashboard can&amp;rsquo;t display any log entry Category: Virtual Machines Verification Steps Create one virtual machines named vm1 in the Harvester virtual machine page Wait until the vm1 in running state Click the View Logs in the side option menu Check the log panel content of vm Click the Clear button Click the Download button Enter some query sting in the Filter field Click settings, change the Show the latest to different options Uncheck/Check the Wrap Lines Uncheck/Check the Show Timestamps Expected Results Should display the detailed log entries on the vm log panel including timestamp and content All existing logs would be cleaned up Ensure new logs will display on the panel Check can correctly download the log to the .</description>
    </item>
    <item>
      <title>VIP configured in a VLAN network should be reached</title>
      <link>https://harvester.github.io/tests/manual/network/vip-configured-on-vlan-network-should-be-reached/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/vip-configured-on-vlan-network-should-be-reached/</guid>
      <description> Related issue: #1424 VIP configured in a VLAN network can not be reached Category: Network Environment Setup The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan Verification Steps Enable virtual network with harvester-mgmt Open Network -&amp;gt; Create a virtual network Provide network name and correct vlan id Open Route, use the default auto setting Create a VM and use the created route SSH to harvester node Ping the IP of the created VM Create a virutal network Provide network name and correct vlan id Open Route, use the manual setting Provide the CIDR and Gateway value Repeat step 5 - 7 Expected Results Check the auto route vlan can be detected with running status Check the manual route vlan can be detected with running status Check the VM can get IP based on auto or manual vlan route Check can ping VM IP from harvester node </description>
    </item>
    <item>
      <title>VIP is accessibility with VLAN enabled on management port</title>
      <link>https://harvester.github.io/tests/manual/network/vip_vlan_mgmtport/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/network/vip_vlan_mgmtport/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1722&#xA;Verify Items VIP should be accessible when VLAN enabled on management port Case: Single Node enables VLAN on management port Install Harvester with single node Login to dashboard then navigate to Settings Edit vlan to enable VLAN on harvester-mgmt reboot the node after reboot, login to console Run the command should not contain any output sudo -s kubectl get pods -A --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep harvester-network-controller-manager | xargs kubectl logs -n harvester-system | grep &amp;quot;Failed to update lock&amp;quot; Repeat step 4-6 with 10 times, should not have any error </description>
    </item>
    <item>
      <title>VIP Load balancer verification (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-vip-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/deployment/verify-vip-load-balancer/</guid>
      <description>Case DHCP Install Harvester on one Node Install with VIP pulling from DHCP Verify that IP is assigned via DHCP Add at least one additional node Use VIP address as management address for adding node Finish install of additional nodes Create new VM Connect to VM via web console Case Static IP Install Harvester on one Node Install with VIP set statically Verify that IP is assigned correctly Add at least one additional node Use VIP address as management address for adding node Finish install of additional nodes Create new VM Connect to VM via web console Expected Results Install of all nodes should complete New nodes should show up in hosts list via web UI at VIP VMs should create Console should open </description>
    </item>
    <item>
      <title>virtualmachineimages.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachineimages.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachineimages.harvesterhci.io/</guid>
      <description>GUI Create an image from GUI Create another image with the same name. The operation should fail with admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: A resource with the same name exists kube-api Create an image from the manifest: $ cat image.yaml --- apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: generateName: image- namespace: default spec: sourceType: download displayName: cirros-0.5.1-x86_64-disk2.img url: http://192.168.2.106/cirros-0.5.1-x86_64-disk.img $ kubectl create -f image.yml virtualmachineimage.harvesterhci.io/image-8dkbq created Try to create an image with the same manifest: $ kubectl create -f image.</description>
    </item>
    <item>
      <title>virtualmachinerestores.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachinerestores.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachinerestores.harvesterhci.io/</guid>
      <description>GUI Setup a backup target Create a backup from a VM. Assume the VM name is vm-test Wait until backup is done Restore the backup to a VM, enter vm-test in the Virtual Machine Name field kube-api $ cat restore.yaml 1 --- apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineRestore metadata: name: restore-aaaa namespace: default spec: newVM: false target: apiGroup: kubevirt.io kind: VirtualMachine name: &amp;#34;&amp;#34; virtualMachineBackupName: test $ kubectl create -f restore.yaml Expected Results GUI The operation should fail with admission webhook &amp;ldquo;validator.</description>
    </item>
    <item>
      <title>virtualmachinetemplateversions.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachinetemplateversions.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachinetemplateversions.harvesterhci.io/</guid>
      <description>kube-api List default templates: $ kubectl get virtualmachinetemplateversions.harvesterhci.io -n harvester-public GUI Go to Advanced -&amp;gt; Templates page Create a new template and set it as the default version Try to delete the default version Expected Results kube-api Default templates should exist: NAME TEMPLATE_ID VERSION AGE iso-image-base-version 1 39m raw-image-base-version 1 39m windows-iso-image-base-version 1 39m GUI Creating a new template should succeed Deleting the default version of a template should fail with: admission webhook &amp;ldquo;validator.</description>
    </item>
    <item>
      <title>VLAN Upgrade Test</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2734-vlan-upgrade-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2734-vlan-upgrade-test/</guid>
      <description>Related issues: #2734 [FEATURE] VLAN enhancement upgrading Category: Upgrade Verification Steps Test plan 1: harvester-mgmt vlan1 Prepare a 3 nodes v1.0.3 Harvester cluster Enable network on harvester-mgmt Create vlan id 1 Create two VMs, one set to vlan 1 and another use harvester-mgmt Perform manual upgrade to v1.1.0 Test plan 2: enps0 NIC with valid vlan Prepare a 3 nodes v1.0.3 Harvester cluster Enable network on another NIC (eg. enp129s0) Create vlan id 91 on enp129s0 Create two VMs, one set to vlan 91 and another use harvester-mgmt Perform manual upgrade to v1.</description>
    </item>
    <item>
      <title>VM Backup with metadata</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/vm_backup_metadata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/vm_backup_metadata/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/988&#xA;Verify Items Metadata should be removed along with VM deleted Metadata should be synced after backup target switched Metadata can be used in new cluster Case: Metadata create and delete Install Harvester with any nodes Create an image for VM creation Setup NFS/S3 backup target Create a VM, then create a backup named backup1 File default-backup1.cfg should be exist in the backup target path &amp;lt;backup root&amp;gt;/harvester/vmbackups Delete the VM Backup backup1 File default-backup1.</description>
    </item>
    <item>
      <title>VM boot stress test</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2906-vm-boot-stress-test-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2906-vm-boot-stress-test-/</guid>
      <description> Related issues: #2906 [BUG] VM can’t boot due to filesystem corruption Category: Volume Verification Steps Create volume (Harvester, Longhorn storage class) Create volume from image Unmount volume from VM Delete volume in use and not in use Export volume to image Create VM from the exported image Edit volume to increase size Delete volume in use Clone volume Take volume snapshot Restore volume snapshot Utilize the E2E test in harvester/test repo to prepare a script to continues run step 1-11 at lease 100 runs Expected Results Pass more than 300 rounds of the I/O write test, Should Not encounter data corruption issue and VM is alive opensuse:~ # xfs_info /dev/vda3 meta-data=/dev/vda3 isize=512 agcount=13, agsize=653887 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=0, rmapbt=0 = reflink=0 data = bsize=4096 blocks=7858427, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 </description>
    </item>
    <item>
      <title>VM Import/Migration</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2274-vm-import/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2274-vm-import/</guid>
      <description>Related issues: #2274 [Feature] VM Import/Migration Category: Virtual Machine Test Information Test Environment:&#xA;1 node harvester on local kvm machine Harvester version: v1.1.0-rc1 Vsphere: 7.0 Openstack: Simulated using running devstack Download kubeconfig for harvester cluster Environment Setup Prepare Harvester master node Prepare vsphere setup (or use existing setup) Prepare a devstack cluster (Openstack 16.2) (stable/train) OpenStack Setup Prepare a baremetal or virtual machine to host the OpenStack service For automated installation on virtual machine, please refer to the cloud init user data in https://github.</description>
    </item>
    <item>
      <title>VM IP addresses should be labeled per network interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2032-2370-vm-ip-lableled-per-network-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2032-2370-vm-ip-lableled-per-network-interface/</guid>
      <description>Related issues: #2032 [BUG] VM IP addresses should be labeled per network interface Related issues: #2370 [backport v1.0.3] VM IP addresses should be labeled per network interface Category: Virtual Machine Verification Steps Enable network with magement-mgmt interface Create vlan network vlan1 with id 1 Check the IP address on the VM page Create a VM with harvester-mgmt network Import Harvester in Rancher Provision a RKE2 cluster from Rancher Check the IP address on the VM page Expected Results Now the VM list only show IP which related to user access.</description>
    </item>
    <item>
      <title>VM label names consistentency before and after the restore</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2662-vm-label-names-consistentency-after-the-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2662-vm-label-names-consistentency-after-the-restore/</guid>
      <description>Related issues: #2662 [BUG] VM label names should be consistent before and after the restore task is done Category: Network Verification Steps Create a VM named ubuntu Check the label name in virtual machine yaml content, label marked with harvesterhci.io/vmName Setup the S3 backup target Take a S3 backup with name After the backup task is done, delete the current VM Restore VM from the backup with the same name ubuntu (Create New) Check the yaml content after VM fully operated Expected Results The vm lable name is consistent to display harvesterhci.</description>
    </item>
    <item>
      <title>VM on error state</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/vm_on_error_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/vm_on_error_state/</guid>
      <description>Ref:&#xA;https://github.com/harvester/harvester/issues/1446 https://github.com/harvester/harvester/issues/982 Verify Items Error message should displayed when VM can&amp;rsquo;t be scheduled VM&amp;rsquo;s state should be changed when host is down Case: Create a VM that no Node can host it Install Harvester with any nodes download a image to create VM create a VM with over-commit (consider to over-provisioning feature, double or triple the host resource would be more reliable.) VM should shows Starting state, and an alart icon shows aside.</description>
    </item>
    <item>
      <title>VM scheduling on Specific node (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/vm_schedule_on_node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/vm_schedule_on_node/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1350&#xA;Verify Items Node which is not active should not be listed in Node Scheduling list Case: Schedule VM on the Node which is Enable Maintenance Mode Install Harvester with at least 2 nodes Login and Navigate to Virtual Machines Create VM and Select Run VM on specific node(s)... All Active nodes should in the list Navigate to Host and pick node(s) to Enable Maintenance Mode Make sure Node(s) state changed into Maintenance Mode Repeat step 2 and 3 Picked Node(s) should not in the list Revert picked Node(s) to back to state of Active Repeat step 2 to 4 </description>
    </item>
    <item>
      <title>VM Snapshot support</title>
      <link>https://harvester.github.io/tests/manual/_incoming/553_vm_snapshot_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/553_vm_snapshot_support/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/553&#xA;Verify Steps: Install Harvester with any nodes Create an Image for VM creation Create vm1 with the image and an additional data volume disk-1 Login to vm1, execute following commands: fdisk /dev/vdb with new and primary partition mkfs.ext4 /dev/vdb1 mkdir vdb &amp;amp;&amp;amp; mount -t ext4 /dev/vdb1 vdb ping 127.0.0.1 | tee -a test vdb/test Navigate to Virtual Machines page, click Take Snapshot button on vm1&amp;rsquo;s details, named vm1s1 Execute sync on vm1 and Take Snapshot named vm1s2 Interrupt ping.</description>
    </item>
    <item>
      <title>VM template is not working with Node scheduling</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2244_vm_template_is_not_working_with_node_scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2244_vm_template_is_not_working_with_node_scheduling/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2244&#xA;Verify Steps: Install Harvester with any nodes Create an Image for VM creation Create VM with Multiple Instance and Use VM Template, In Node Scheduling tab, select Run VM on specific node(s) Created VMs should be scheduled on the specific node </description>
    </item>
    <item>
      <title>VM&#39;s CPU maximum limitation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/vm_cpu_limits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/virtual-machines/vm_cpu_limits/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1565&#xA;Verify Items VM&amp;rsquo;s maximum CPU amount should not have limitation. Case: Create VM with large CPU amount Install harvester with any nodes Create image for VM creation Create a VM with vCPU over than 100 Start VM and verify lscpu shows the same amount </description>
    </item>
    <item>
      <title>VMIs created from VM Template don&#39;t have LiveMigrate evictionStrategy set</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2357_vmis_created_from_vm_template_do_nott_have_livemigrate_evictionstrategy_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2357_vmis_created_from_vm_template_do_nott_have_livemigrate_evictionstrategy_set/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2357&#xA;Verify Steps: Install Harvester with at least 2 nodes Create Image for VM Creation Navigate to Advanced/Templates and create a template t1 Create VM vm1 from template t1 Edit YAML of vm1, field spec.template.spec.evictionStrategy should be LiveMigrate Enable Maintenance Mode on the host which hosting vm1 vm1 should start migrating automatically Migration should success </description>
    </item>
    <item>
      <title>VMs can&#39;t start if a node contains more than ~60 VMs</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2722_vms_can_not_start_if_a_node_contains_more_than_60_vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2722_vms_can_not_start_if_a_node_contains_more_than_60_vms/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2722&#xA;Verify Steps: Install Harvester with any nodes Login to console, execute sysctl -a | grep aio, the value of fs.aio-max-nr should be 1048576 Update the value by executing: mkdir -p /usr/local/lib/sysctl.d/ cat &amp;gt; /usr/local/lib/sysctl.d/harvester.conf &amp;lt;&amp;lt;EOF fs.aio-max-nr = 61440 EOF sysctl --system Execute sysctl -a | grep aio, the value of fs.aio-max-nr should be 61440 Reboot the node then execute sysctl -a | grep aio, the value of fs.aio-max-nr should still be 61440 Create an image for VM creation Create 60 VMs and schedule on the node which updated fs.</description>
    </item>
    <item>
      <title>Volume size should be editable on derived template</title>
      <link>https://harvester.github.io/tests/manual/templates/derived_template_configure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/templates/derived_template_configure/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1711&#xA;Verify Items Volume size can be changed when creating a derived template Case: Update volume size on new template derived from exist template Install Harvester with any Nodes Login to Dashboard Create Image for Template Creation Create Template T1 with Image Volume and additional Volume Modify Template T1 with update Volume size Volume size should be editable Click Save, then edit new version of T1 Volume size should be updated as expected </description>
    </item>
    <item>
      <title>VolumeSnapshot Management</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2296_volumesnapshot_management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2296_volumesnapshot_management/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2296&#xA;Verify Steps: Install Harvester with any nodes Create an Image for VM creation Create vm vm1 and start it *Take Snapshot on vm1 named vm1s1 Navigate to Volumes, click disks of vm1 then move to Snapshots tab, volume of snapshot vm1s1 should not displayed Navigate to Advanced/Volume Snapshots, volumes of snapshot vm1s1 should not displayed Navigate to Advanced/VM Snapshots, snapshot vm1s1 should displayed </description>
    </item>
    <item>
      <title>Wrong mgmt bond MTU size during initial ISO installation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2437_wrong_mgmt_bond_mtu_size_during_initial_iso_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2437_wrong_mgmt_bond_mtu_size_during_initial_iso_installation/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/2437&#xA;Verify Steps: Install Harvester via ISO and configure IPv4 Method with static Inputbox MTU (Optional) should be available and optional Configured MTU should reflect to the port&amp;rsquo;s MTU after installation </description>
    </item>
    <item>
      <title>Zero downtime upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</guid>
      <description> Related issues: #1707 [BUG] Zero downtime upgrade stuck in &amp;ldquo;Waiting for VM live-migration or shutdown&amp;hellip;&amp;rdquo; Category: Upgrade Verification Steps Create a ubuntu image from URL Enable Network with management-mgmt Create a virtual network vlan1 with id 1 Setup backup target Create a VM backup Follow the guide to do upgrade test Expected Results Can upgrade correctly with all VMs remain in running </description>
    </item>
  </channel>
</rss>
